[
  {
    "objectID": "worksheets/ws-04.html",
    "href": "worksheets/ws-04.html",
    "title": "Worksheet 3: Landscape metrics part 2",
    "section": "",
    "text": "Please navigate to the landscapemetrics package.\nRun the following tutorials with the example data of this course:\n\n\n\n\n\n\n\nSample lsm\nIrregular Areas\nUtilities\n\n\n\n\nFor the irregular vector data download the OSM data e.g. from Geofabrik and use the category landuse. Note you need to merge Niedersachsen and Sachsen-Anhalt\n\n\n\n\n\n\n\n\n\nDownload the fragstats tutorial Analyzing a single grid.\nRun the tutorial but use landscapemetrics instead"
  },
  {
    "objectID": "worksheets/ws-04.html#exercises-ws-3",
    "href": "worksheets/ws-04.html#exercises-ws-3",
    "title": "Worksheet 3: Landscape metrics part 2",
    "section": "",
    "text": "Please navigate to the landscapemetrics package.\nRun the following tutorials with the example data of this course:\n\n\n\n\n\n\n\nSample lsm\nIrregular Areas\nUtilities\n\n\n\n\nFor the irregular vector data download the OSM data e.g. from Geofabrik and use the category landuse. Note you need to merge Niedersachsen and Sachsen-Anhalt\n\n\n\n\n\n\n\n\n\nDownload the fragstats tutorial Analyzing a single grid.\nRun the tutorial but use landscapemetrics instead"
  },
  {
    "objectID": "worksheets/ws-00.html",
    "href": "worksheets/ws-00.html",
    "title": "Worksheets HowTo",
    "section": "",
    "text": "The Working Sheets will provide weekly assignments related to the general task of reaching out for the course goals.\n\nMandatory assignments (Studienleistung) will be marked as \nGraded examinations (Prüfungsleistung) will be marked as"
  },
  {
    "objectID": "worksheets/ws-07.html",
    "href": "worksheets/ws-07.html",
    "title": "Worksheet 7: Reproducible Analysis Workflows",
    "section": "",
    "text": "Achieving reliable monitoring hinges on the reproducibility of both field data collection and subsequent analyses. In the context of spatial micro-scale analysis and modeling, the precise geo-object locations play a central role, presenting an additional challenge when considering the 3D structure. Incorporating the temporal aspect, such as accurately recording repeated flights, further elevates these demands. Whether utilizing low-cost or professional UAV systems, these factors must be acknowledged to enable meaningful analysis or modeling. While LiDAR systems are the preferred choice for acquiring forest structure, their high cost, both in acquisition and processing, poses challenges. A feasible alternative lies in off-the-shelf UAV systems with integrated image acquisition. These systems allow for the generation of not only orthorectified planar image data but also quasi-three-dimensional point clouds through photo reconstruction and image processing techniques, providing elevation information for each coordinate."
  },
  {
    "objectID": "worksheets/ws-07.html#aims-and-goals",
    "href": "worksheets/ws-07.html#aims-and-goals",
    "title": "Worksheet 7: Reproducible Analysis Workflows",
    "section": "Aims and Goals",
    "text": "Aims and Goals\nReproducibility in Data Acquisition and Analysis:\nAims: Establish a framework for reproducible and automated data acquisition and analysis, crucial for robust monitoring.\nGoals: Ensure that the 3D parameters and forest ecological metrics are calculated in a fully reproducible manner, enabling accurate spatial micro-scale modeling.\nIntegration of Temporal and Spatial Aspects:\nAims: Address the challenges posed by the 3D structure and temporal aspects in UAV-based monitoring. Goals: Achieve meaningful analysis and modeling by acknowledging and incorporating precise geo-object locations and accurate recording of repeated flights.\nCost-Effective Alternatives for Forest Structure Acquisition:\nAims: Explore alternatives to expensive LiDAR systems for acquiring forest structure. Goals: Utilize off-the-shelf UAV systems with integrated image acquisition to generate quasi-three-dimensional point clouds, providing elevation information in a cost-effective manner.\nAutomated and Reproducible Workflows:\nAims: Develop automated workflows that streamline data processing and analysis. Goals: Enable a robust and efficient classification method using machine learning object-based image analysis (OBIA), ensuring complex classification goals can be achieved with minimal technical expertise."
  },
  {
    "objectID": "worksheets/ws-07.html#project-repository",
    "href": "worksheets/ws-07.html#project-repository",
    "title": "Worksheet 7: Reproducible Analysis Workflows",
    "section": "Project Repository",
    "text": "Project Repository\n\nforenius-pp repository for additional functions"
  },
  {
    "objectID": "worksheets/ws-05.html",
    "href": "worksheets/ws-05.html",
    "title": "Worksheet 4: Pattern-based spatial analysis",
    "section": "",
    "text": "Read the study area polygon from the exdata/harz_borders.gpkg file using the read_sf() function from the sf package.\nRead the land cover raster data for Europe from the file exdata/lc_europe.tif using the function rast() from the package terra. Visualise both datasets.\nCrop and mask the raster to the polygon boundaries. Visualise the results.\nCompute a spatial signature for the study area. Can you understand its meaning?\nFind out which areas of the Europe raster are most similar to the study area (this may take a minute or so). Try different window sizes (e.g. 200 or 500).\n\n\n\n\n\n\n\n\n\n\nDownload Corine data from 2012 and 2018.\nDefine an appropriate scale for the assessment of forest structure changes at European level\nIdentify the areas with the smallest and largest changes\nProvide a short interpretation\nHow’s the Harz ranked?"
  },
  {
    "objectID": "worksheets/ws-05.html#exercises-ws-4",
    "href": "worksheets/ws-05.html#exercises-ws-4",
    "title": "Worksheet 4: Pattern-based spatial analysis",
    "section": "",
    "text": "Read the study area polygon from the exdata/harz_borders.gpkg file using the read_sf() function from the sf package.\nRead the land cover raster data for Europe from the file exdata/lc_europe.tif using the function rast() from the package terra. Visualise both datasets.\nCrop and mask the raster to the polygon boundaries. Visualise the results.\nCompute a spatial signature for the study area. Can you understand its meaning?\nFind out which areas of the Europe raster are most similar to the study area (this may take a minute or so). Try different window sizes (e.g. 200 or 500).\n\n\n\n\n\n\n\n\n\n\nDownload Corine data from 2012 and 2018.\nDefine an appropriate scale for the assessment of forest structure changes at European level\nIdentify the areas with the smallest and largest changes\nProvide a short interpretation\nHow’s the Harz ranked?"
  },
  {
    "objectID": "worksheets/ws-03.html",
    "href": "worksheets/ws-03.html",
    "title": "Worksheet 2: Landscape metrics part 1",
    "section": "",
    "text": "The marginal entropy and relative mutual information can be calculated using the landscapemetrics package’s functions: lsm_l_ent() and lsm_l_relmutinf(). Calculate both of these metrics for the exdata/lc_small.tif raster.\nRead the exdata/lc_europe.tif raster using rast() from the terra package and the exdata/polygons.gpkg vector data using the read_sf() function from the sf package. Calculate the marginal entropy and relative mutual information for each polygon using the sample_lsm() function.\nJoin the calculated values with the polygons (see https://r-spatialecology.github.io/landscapemetrics/articles/irregular_areas.html for more details).\nCalculate SHDI and AI for the polygons. Compare the values of SHDI and AI with the marginal entropy and relative mutual information (e.g., using a scatterplot or by calculating the correlation coefficient). Are the results similar?\n(Extra) Create your own polygonal grid using st_make_grid() function from the sf package for the area from the exdata/polygons.gpkg file. Calculate the marginal entropy and relative mutual information for each square using the sample_lsm() function. Visualize the results."
  },
  {
    "objectID": "worksheets/ws-03.html#exercises-ws-2",
    "href": "worksheets/ws-03.html#exercises-ws-2",
    "title": "Worksheet 2: Landscape metrics part 1",
    "section": "",
    "text": "The marginal entropy and relative mutual information can be calculated using the landscapemetrics package’s functions: lsm_l_ent() and lsm_l_relmutinf(). Calculate both of these metrics for the exdata/lc_small.tif raster.\nRead the exdata/lc_europe.tif raster using rast() from the terra package and the exdata/polygons.gpkg vector data using the read_sf() function from the sf package. Calculate the marginal entropy and relative mutual information for each polygon using the sample_lsm() function.\nJoin the calculated values with the polygons (see https://r-spatialecology.github.io/landscapemetrics/articles/irregular_areas.html for more details).\nCalculate SHDI and AI for the polygons. Compare the values of SHDI and AI with the marginal entropy and relative mutual information (e.g., using a scatterplot or by calculating the correlation coefficient). Are the results similar?\n(Extra) Create your own polygonal grid using st_make_grid() function from the sf package for the area from the exdata/polygons.gpkg file. Calculate the marginal entropy and relative mutual information for each square using the sample_lsm() function. Visualize the results."
  },
  {
    "objectID": "reader/mc_2025_microclimate_viewer_merged.html",
    "href": "reader/mc_2025_microclimate_viewer_merged.html",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "This document demonstrates and explains a six-stage spatial pipeline for Ecowitt temperature data:\n\nIngest & clean: load two loggers, harmonize names, and aggregate to 3-hourly.\nInterpolation preview: per-timestep KED (universal kriging) and a multi-panel plot.\nScale inference (L): fit a global variogram to get L50 / L95 (spatial correlation ranges).\nScale-matched predictors: build DEM-derived rasters (optionally slope/aspect/TRI) at the right scale.\nTune \\(R^*\\): select an optimal drift radius \\(R\\) with block-CV (U-curve), then benchmark methods.\nDiagnostics: export products, report scales, and compute an optional error budget.\n\nKey concept: We use two DEMs on purpose — DEM_scale at native/coarser resolution drives scales, tuning, folds, CV, and error budget. DEM_render is an upsampled/aggregated DEM for pretty maps and interpolation output. This separation prevents the tuner from collapsing to 10 m just because pixels are tiny.\n\n\n\n\nCode\n# Global setup + packages\nset.seed(42)\noptions(width = 100)\n\npkgs &lt;- c(\n  \"sf\",\"terra\",\"raster\",\"dplyr\",\"automap\",\"gstat\",\"mapview\",\"stars\",\n  \"readxl\",\"stringr\",\"tidyr\",\"purrr\",\"lubridate\",\"rprojroot\",\n  \"exactextractr\",\"zoo\",\"ggplot2\",\"viridis\",\"mgcv\",\"randomForest\",\"fields\",\"sp\",\"deldir\",\n  \"leaflet\",\"DT\",\"htmltools\",\"jsonlite\",\"shiny\" # viewer deps\n)\nneed &lt;- setdiff(pkgs, rownames(installed.packages()))\nif (length(need)) install.packages(need, dependencies = TRUE)\ninvisible(lapply(pkgs, function(p) suppressPackageStartupMessages(library(p, character.only = TRUE))))\n\n\n\n\nCode\n# Small utilities (kept identical to your working script where applicable)\n# SpatRaster sicher \"pinnen\" (Datei-basiert) und als gültiges Objekt zurückgeben\n.pin_rast &lt;- function(r, crs = NULL, dir = NULL, name = \"pinned\") {\n  stopifnot(inherits(r, \"SpatRaster\"))\n  if (is.null(dir)) dir &lt;- file.path(getwd(), \"run_cache\")\n  if (!dir.exists(dir)) dir.create(dir, recursive = TRUE)\n  f &lt;- file.path(dir, paste0(name, \".tif\"))\n  # immer schreiben → garantiert datei-gestützt und voll materialisiert\n  terra::writeRaster(r, f, overwrite = TRUE)\n  rp &lt;- terra::rast(f)\n  if (!is.null(crs)) {\n    # nur projezieren, wenn noch nicht identisch\n    same &lt;- try(terra::crs(rp, proj=TRUE) == as.character(crs), silent = TRUE)\n    if (!isTRUE(same)) rp &lt;- terra::project(rp, as.character(crs), method = \"near\")\n  }\n  # Sanity check\n  invisible(terra::nlyr(rp))\n  rp\n}\n\n# Prüfer: „lebt“ der Pointer?\n.is_alive_spatr &lt;- function(r) {\n  inherits(r, \"SpatRaster\") && !inherits(try(terra::nlyr(r), silent = TRUE), \"try-error\")\n}\n\n# Safe, URL-friendly file slug\nslug &lt;- function(x) { \n  x &lt;- gsub(\"[^0-9A-Za-z_-]+\",\"-\", x)\n  x &lt;- gsub(\"-+\",\"-\", x)\n  gsub(\"(^-|-$)\",\"\", x)\n}\n\n# Human-readable time labels from AYYYY... keys\npretty_time &lt;- function(x) {\n  vapply(x, function(s) {\n    if (grepl(\"^A\\\\d{14}$\", s)) {\n      ts &lt;- as.POSIXct(substr(s, 2, 15), format = \"%Y%m%d%H%M%S\", tz = \"UTC\")\n      format(ts, \"%Y-%m-%d %H:%M\")\n    } else if (grepl(\"^A\\\\d{8}(_D)?$\", s)) {\n      ts &lt;- as.Date(substr(s, 2, 9), format = \"%Y%m%d\")\n      format(ts, \"%Y-%m-%d\")\n    } else s\n  }, character(1))\n}\n\n# Pick the most data-dense time-slice (max number of finite observations)\npick_densest_index &lt;- function(sf_wide, var_names) {\n  nn &lt;- sapply(var_names, function(v) sum(is.finite(sf_wide[[v]])))\n  which.max(nn)\n}\n\n# Build figure descriptions (viewer)\nbuild_explanations &lt;- function(fig_dir, pick_ts) {\n  ts_label &lt;- slug(pretty_time(pick_ts))\n  files &lt;- c(\n    \"timeseries_panel_grid.png\",\n    \"timeseries_panel_grid.pdf\",\n    sprintf(\"u_curve_%s.png\", ts_label),\n    sprintf(\"u_curve_extras_%s.png\", ts_label),\n    sprintf(\"benchmark_%s.png\", ts_label),\n    sprintf(\"benchmark_extras_%s.png\", ts_label)\n  )\n  paths &lt;- file.path(fig_dir, files)\n  desc  &lt;- c(\n    \"Per-timestep KED previews; dots=stations; red=plot boundary.\",\n    \"Same as PNG, vector PDF.\",\n    \"U-curve for tuning R via block-CV (drift-only).\",\n    \"U-curve with extra predictors.\",\n    \"Method comparison at R* (lower RMSE is better).\",\n    \"Benchmark with extras at R*.\"\n  )\n  keep &lt;- file.exists(paths)\n  out &lt;- as.list(desc[keep]); names(out) &lt;- basename(paths[keep])\n  out\n}\n\n\n\nWhy this matters: using fine pixels for scale estimation will cap \\(R\\) at a few pixels, resulting in the “10 m fallback”. Keep DEM_scale at a realistic native/coarser resolution for L/R.\n\n\n\n\n\n\nCode\n# Robust project root finder and paths\nwd &lt;- rprojroot::find_rstudio_root_file()\n\nsource(file.path(wd, \"reader/all_functions_1.R\"))   # consolidated toolkit\n\n#fn_DTM        &lt;- file.path(wd, \"reader/data_2024/copernicus_DEM.tif\")\nfn_DTM        &lt;- file.path(wd, \"reader/data_2024/DEM.tif\")\nfn_stations   &lt;- file.path(wd, \"reader/data_2024/stations_prelim_modifiziert.gpkg\")\nfn_area       &lt;- file.path(wd, \"reader/data_2024/plot.shp\")\nfn_temp_FC29  &lt;- file.path(wd, \"reader/data_2024/all_GW1000A-WIFIFC29.xlsx\")\nfn_temp_DB2F  &lt;- file.path(wd, \"reader/data_2024/all_GW1000A-WIFIDB2F.xlsx\")\ncleandata_rds &lt;- file.path(wd, \"reader/data_2024/climdata.RDS\")\n\nout_dir    &lt;- file.path(wd, \"reader/interpolated\")\nfig_dir    &lt;- file.path(out_dir, \"fig\")\nmethod_dir &lt;- file.path(out_dir, \"methods_compare\")\nreport_dir &lt;- file.path(out_dir, \"report\")\nfor (d in c(out_dir, fig_dir, method_dir, report_dir)) if (!dir.exists(d)) dir.create(d, recursive = TRUE)\n\n# CRS\nepsg &lt;- \"EPSG:32633\"  # UTM zone 33N\nsf_crs_utm33 &lt;- sf::st_crs(epsg)\n\n\n\n\n\n\n\nCode\n# DEMs\nDEM_scale  &lt;- terra::rast(fn_DTM) |&gt; terra::project(epsg)\nDEM_scale  &lt;- terra::aggregate(DEM_scale, c(20, 20))  # coarsen ~20–25 m (as in your working code)\nnames(DEM_scale) &lt;- \"altitude\"\nDEM_render &lt;- DEM_scale |&gt; terra::aggregate(fact = c(10, 10))  # for rendering products\n\ncat(\"DEM_scale res (m): \", paste(terra::res(DEM_scale),  collapse=\" x \"), \"\\n\")\n\n\nDEM_scale res (m):  2.00223686801611 x 2.0022368680127 \n\n\nCode\ncat(\"DEM_render res (m):\", paste(terra::res(DEM_render), collapse=\" x \"), \"\\n\")\n\n\nDEM_render res (m): 20.0223686801606 x 20.022368680127 \n\n\nCode\n# Stations and plot boundary → same CRS\nstations_pos &lt;- sf::st_read(fn_stations, quiet = TRUE) |&gt; sf::st_transform(sf_crs_utm33)\nplot_area    &lt;- sf::st_read(fn_area, quiet = TRUE)     |&gt; sf::st_transform(sf_crs_utm33) |&gt; sf::st_make_valid()\n\n# Altitude from DEM_scale (not the upsampled one)\nstations_pos &lt;- stations_pos %&gt;%\n  dplyr::mutate(altitude = exactextractr::exact_extract(DEM_scale, sf::st_buffer(stations_pos, 1), \"mean\"))\n\n\n\n  |                                                                                                \n  |                                                                                          |   0%\n  |                                                                                                \n  |======                                                                                    |   7%\n  |                                                                                                \n  |=============                                                                             |  14%\n  |                                                                                                \n  |===================                                                                       |  21%\n  |                                                                                                \n  |==========================                                                                |  29%\n  |                                                                                                \n  |================================                                                          |  36%\n  |                                                                                                \n  |=======================================                                                   |  43%\n  |                                                                                                \n  |=============================================                                             |  50%\n  |                                                                                                \n  |===================================================                                       |  57%\n  |                                                                                                \n  |==========================================================                                |  64%\n  |                                                                                                \n  |================================================================                          |  71%\n  |                                                                                                \n  |=======================================================================                   |  79%\n  |                                                                                                \n  |=============================================================================             |  86%\n  |                                                                                                \n  |====================================================================================      |  93%\n  |                                                                                                \n  |==========================================================================================| 100%\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntemp_FC29 &lt;- extract_ecowitt_core_vars(fn_temp_FC29)\n\n\nNew names:\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...27`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n• `` -&gt; `...36`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...39`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n• `` -&gt; `...50`\n• `` -&gt; `...51`\n• `` -&gt; `...52`\n• `` -&gt; `...53`\n• `` -&gt; `...54`\n• `` -&gt; `...55`\n• `` -&gt; `...56`\n• `` -&gt; `...57`\n• `` -&gt; `...58`\n• `` -&gt; `...59`\n• `` -&gt; `...60`\n• `` -&gt; `...61`\n• `` -&gt; `...62`\n• `` -&gt; `...63`\n• `` -&gt; `...64`\n• `` -&gt; `...65`\n• `` -&gt; `...66`\n• `` -&gt; `...67`\n• `` -&gt; `...68`\n• `` -&gt; `...69`\n• `` -&gt; `...70`\n• `` -&gt; `...71`\n• `` -&gt; `...72`\n• `` -&gt; `...73`\n• `` -&gt; `...74`\n• `` -&gt; `...75`\n• `` -&gt; `...76`\n• `` -&gt; `...77`\n• `` -&gt; `...78`\n• `` -&gt; `...79`\n• `` -&gt; `...80`\n\n\nCode\ntemp_DB2F &lt;- extract_ecowitt_core_vars(fn_temp_DB2F)\n\n\nNew names:\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...27`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n• `` -&gt; `...36`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...39`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n• `` -&gt; `...50`\n• `` -&gt; `...51`\n• `` -&gt; `...52`\n• `` -&gt; `...53`\n• `` -&gt; `...54`\n• `` -&gt; `...55`\n• `` -&gt; `...56`\n• `` -&gt; `...57`\n\n\nCode\nt_rh_all  &lt;- merge_ecowitt_logger_vars(temp_FC29, temp_DB2F)\n\n# Clean display names and map to verbose station names\nfor (meas in c(\"temperature\",\"humidity\")) {\n  t_rh_all[[meas]] &lt;- t_rh_all[[meas]] %&gt;%\n    dplyr::rename_with(~ to_verbose(.x, ifelse(meas==\"temperature\",\"Temperature\",\"Humidity\")), -Time) %&gt;%\n    clean_names()\n}\n\n# Aggregate to 3-hour steps\ntemp_agg &lt;- t_rh_all$temperature %&gt;%\n  dplyr::mutate(time = lubridate::floor_date(Time, \"3 hours\")) %&gt;%\n  dplyr::group_by(time) %&gt;%\n  dplyr::summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)), .groups = \"drop\")\nnames(temp_agg) &lt;- clean_ids(names(temp_agg))\n\n# long → wide matrix: station rows, time columns\ntemp_matrix &lt;- temp_agg %&gt;%\n  tidyr::pivot_longer(cols = -time, names_to = \"stationid\", values_to = \"value\") %&gt;%\n  tidyr::pivot_wider(names_from = time, values_from = value)\n\n# Join to station geometry and altitude\nstations_pos &lt;- stations_pos %&gt;% dplyr::mutate(stationid = to_verbose(stationid))\nm &lt;- dplyr::left_join(stations_pos, temp_matrix, by = \"stationid\")\n\n# Hygiene\nstations_pos$stationid &lt;- gsub(\"\\\\(℃\\\\)|\\\\(％\\\\)|\\\\(\\\\%\\\\)\", \"\", stations_pos$stationid)\nm$stationid            &lt;- gsub(\"\\\\(℃\\\\)|\\\\(％\\\\)|\\\\(\\\\%\\\\)\", \"\", m$stationid)\nnames(m)               &lt;- fix_names(names(m))\nsaveRDS(m, cleandata_rds)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmin_pts &lt;- 5\nvars &lt;- as.list(grep(\"^A\\\\d{8,14}\", names(m), value = TRUE))\n\nkriged_list &lt;- lapply(vars, function(v) {\n  interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\")\n})\n\n\nInterpolating: A20230827150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230827150000\n\n\nInterpolating: A20230827180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230827180000\n\n\nInterpolating: A20230828150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230828150000\n\n\nInterpolating: A20230828180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230828180000\n\n\nInterpolating: A20230828210000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230828210000\n\n\nInterpolating: A20230829\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230829\n\n\nInterpolating: A20230829030000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230829030000\n\n\nInterpolating: A20230829060000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230829060000\n\n\nInterpolating: A20230829090000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230829090000_interpolated.tif\n\n\nInterpolating: A20230829120000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230829120000_interpolated.tif\n\n\nInterpolating: A20230829150000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230829150000_interpolated.tif\n\n\nInterpolating: A20230829180000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230829180000_interpolated.tif\n\n\nInterpolating: A20230829210000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230829210000_interpolated.tif\n\n\nInterpolating: A20230830\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230830_interpolated.tif\n\n\nInterpolating: A20230830030000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830030000\n\n\nInterpolating: A20230830060000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830060000\n\n\nInterpolating: A20230830090000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230830090000_interpolated.tif\n\n\nInterpolating: A20230830120000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230830120000_interpolated.tif\n\n\nInterpolating: A20230830150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830150000\n\n\nInterpolating: A20230830180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830180000\n\n\nInterpolating: A20230830210000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230830210000_interpolated.tif\n\n\nInterpolating: A20230831\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230831_interpolated.tif\n\n\nInterpolating: A20230831030000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831030000\n\n\nInterpolating: A20230831060000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831060000\n\n\nInterpolating: A20230831090000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230831090000_interpolated.tif\n\n\nInterpolating: A20230831120000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230831120000_interpolated.tif\n\n\nInterpolating: A20230831150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831150000\n\n\nInterpolating: A20230831180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831180000\n\n\nInterpolating: A20230831210000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230831210000_interpolated.tif\n\n\nInterpolating: A20230901\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230901\n\n\nCode\nnames(kriged_list) &lt;- vars\n\n\n\n\nCode\n# Shared color scale across all timestamps\npanel &lt;- timeseries_panel(\n  kriged_list        = kriged_list,\n  plot_area          = plot_area,\n  stations_pos       = stations_pos,\n  cells_target       = 150000,\n  max_cols           = 4,\n  label_pretty_time  = TRUE,\n  out_png            = file.path(fig_dir, \"timeseries_panel_grid.png\"),\n  out_pdf            = file.path(fig_dir, \"timeseries_panel_grid.pdf\"),\n  fill_label         = \"Temperature\"\n)\n\n\nCoordinate system already present. Adding new coordinate system, which will replace the existing\none.\n\n\nCode\npanel$plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Densest timestamp\npick_idx &lt;- pick_densest_index(m, vars)\npick_ts  &lt;- names(m)[pick_idx]\n\n# Extra predictors (kept as in your working code)\nextra_list &lt;- list(\n  slope  = terra::terrain(DEM_scale, v = \"slope\",  unit = \"degrees\"),\n  aspect = terra::terrain(DEM_scale, v = \"aspect\"),\n  tri    = terra::terrain(DEM_scale, v = \"TRI\")\n)\n\n# One timestamp (densest), with extras\nres_one &lt;- run_one(\n  v           = vars[[pick_idx]],\n  m           = m,\n  DEM_render  = DEM_render,\n  DEM_scale   = DEM_scale,\n  method_dir  = method_dir,\n  fig_dir     = fig_dir,\n  report_dir  = report_dir,\n  extra_preds = extra_list,\n  save_figs   = TRUE\n)\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\nR*: tuner failed; falling back to 56.8593 m.\n\n\nWarning in x@pntr$rastDistance(target, exclude, keepNA, tolower(unit), TRUE, : GDAL Message 1:\nPixels not square, distances will be inaccurate.\n\n\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n\n\nR*: tuner failed; falling back to 56.8593 m.\n\n\nWarning in x@pntr$rastDistance(target, exclude, keepNA, tolower(unit), TRUE, : GDAL Message 1:\nPixels not square, distances will be inaccurate.\n\n\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n\n\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n\n\n\n\n\n\n\nCode\ncompute_all &lt;- FALSE\n\nif (isTRUE(compute_all)) {\n  req &lt;- c(\"m\",\"DEM_render\",\"DEM_scale\",\"method_dir\",\"fig_dir\",\"report_dir\")\n  miss &lt;- req[!vapply(req, exists, logical(1), inherits = TRUE)]\n  if (length(miss)) stop(\"Fehlende Objekte im Environment: \", paste(miss, collapse = \", \"))\n\n  .best_from_bench &lt;- function(bench_obj) {\n    if (is.null(bench_obj) || !is.data.frame(bench_obj$table) || nrow(bench_obj$table) &lt; 1)\n      return(NULL)\n    b &lt;- bench_obj$table\n    b &lt;- b[is.finite(b$RMSE), , drop = FALSE]\n    if (!nrow(b)) return(NULL)\n    b &lt;- b[order(b$RMSE), , drop = FALSE]\n    b[1, c(\"method\",\"RMSE\"), drop = FALSE]\n  }\n\n  message(sprintf(\"Starte compute_all für %d Zeitschritte …\", length(vars)))\n\n  res_all &lt;- setNames(lapply(vars, function(vv) {\n    message(\"→ run_one: \", pretty_time(vv))\n    tryCatch(\n      run_one(\n        v           = vv,\n        m           = m,\n        DEM_render  = DEM_render,\n        DEM_scale   = DEM_scale,\n        method_dir  = method_dir,\n        fig_dir     = fig_dir,\n        report_dir  = report_dir,\n        extra_preds = extra_list,\n        save_figs   = TRUE,\n        save_tables = TRUE\n      ),\n      error = function(e) {\n        warning(\"run_one fehlgeschlagen für \", vv, \": \", conditionMessage(e))\n        NULL\n      }\n    )\n  }), vars)\n\n  saveRDS(res_all, file.path(report_dir, \"all_results.RDS\"))\n\n  summ &lt;- do.call(rbind, lapply(names(res_all), function(k) {\n    r &lt;- res_all[[k]]\n    if (is.null(r)) {\n      return(data.frame(\n        ts_key      = k, \n        stamp       = pretty_time(k),\n        R_star      = NA_real_,\n        best_source = NA_character_,  # \"no_extras\"/\"with_extras\"\n        best_method = NA_character_,\n        best_RMSE   = NA_real_\n      ))\n    }\n    rstar &lt;- suppressWarnings(as.numeric(r$tune$R_star))\n    if (!is.finite(rstar)) rstar &lt;- NA_real_\n\n    b0 &lt;- .best_from_bench(r$bench)\n    bE &lt;- .best_from_bench(r$bench_ex)\n\n    score0 &lt;- if (!is.null(b0) && isTRUE(is.finite(b0$RMSE))) b0$RMSE else Inf\n    scoreE &lt;- if (!is.null(bE) && isTRUE(is.finite(bE$RMSE))) bE$RMSE else Inf\n\n    if (is.infinite(score0) && is.infinite(scoreE)) {\n      src &lt;- NA_character_; bm &lt;- NA_character_; br &lt;- NA_real_\n    } else if (score0 &lt;= scoreE) {\n      src &lt;- \"no_extras\"; bm &lt;- b0$method; br &lt;- score0\n    } else {\n      src &lt;- \"with_extras\"; bm &lt;- bE$method; br &lt;- scoreE\n    }\n\n    data.frame(\n      ts_key      = k,\n      stamp       = pretty_time(k),\n      R_star      = rstar,\n      best_source = src,\n      best_method = bm,\n      best_RMSE   = br\n    )\n  }))\n\n  utils::write.csv(summ, file.path(report_dir, \"summary_Rstar_bestmethod.csv\"), row.names = FALSE)\n  message(\"✔ Fertig: summary_Rstar_bestmethod.csv geschrieben.\")\n}\n\n\n\n\n\n\n\nCode\nts_label &lt;- pretty_time(pick_ts)\nbench_base_csv &lt;- file.path(report_dir, sprintf(\"benchmark_%s.csv\",        slug(ts_label)))\nbench_ex_csv   &lt;- file.path(report_dir, sprintf(\"benchmark_extras_%s.csv\", slug(ts_label)))\neb_base_csv    &lt;- file.path(report_dir, sprintf(\"error_budget_%s.csv\",     slug(ts_label)))\neb_ex_csv      &lt;- file.path(report_dir, sprintf(\"error_budget_extras_%s.csv\", slug(ts_label)))\n\nif (is.list(res_one$bench)    && is.data.frame(res_one$bench$table))    write.csv(res_one$bench$table,    bench_base_csv, row.names = FALSE)\nif (is.list(res_one$bench_ex) && is.data.frame(res_one$bench_ex$table)) write.csv(res_one$bench_ex$table, bench_ex_csv,   row.names = FALSE)\nif (is.data.frame(res_one$errtab))    write.csv(res_one$errtab,    eb_base_csv, row.names = FALSE)\nif (is.data.frame(res_one$errtab_ex)) write.csv(res_one$errtab_ex, eb_ex_csv,   row.names = FALSE)\n\n\n\n\n\n\n\nCode\nn_stations &lt;- nrow(stations_pos)\nn_pts_ts   &lt;- sum(is.finite(m[[pick_ts]]))\nLs         &lt;- get_Ls(res_one$wf$L)\nLs_e       &lt;- if (!is.null(res_one$wf_ex)) get_Ls(res_one$wf_ex$L) else NULL\nRstar_base &lt;- suppressWarnings(as.numeric(res_one$tune$R_star))\nRstar_ex   &lt;- suppressWarnings(as.numeric(res_one$tune_ex$R_star))\n\ncat(sprintf(\"Chosen R (micro/local): %s / %s m\\n\",\n            ifelse(is.finite(res_one$wf$R['micro']), round(res_one$wf$R['micro']), \"NA\"),\n            ifelse(is.finite(res_one$wf$R['local']), round(res_one$wf$R['local']), \"NA\")\n))\n\n\nChosen R (micro/local): 19 / 57 m\n\n\n\n\n\n\nThe viewer launches a Shiny app; to not block Quarto rendering, the launch is wrapped in if (interactive()).\n\n\n\nCode\n# Minimal viewer injection: use your working run_mc_viewer() with robust file matching\nraster_path &lt;- function(method, ts) {\n  stopifnot(length(method) == 1, length(ts) == 1)\n  m &lt;- tolower(method)\n  .ts_tokens &lt;- function(ts_key) {\n    raw &lt;- tolower(as.character(ts_key))\n    pty &lt;- tolower(pretty_time(ts_key))\n    slug_pt &lt;- gsub(\"[^0-9A-Za-z_-]+\",\"-\", pty)\n    d14 &lt;- sub(\"^a\", \"\", raw)\n    ymd  &lt;- if (nchar(d14) &gt;= 8) substr(d14,1,8) else NA_character_\n    hhmm &lt;- if (nchar(d14) &gt;= 12) substr(d14,9,12) else NA_character_\n    comp1 &lt;- if (!is.na(ymd) && !is.na(hhmm))\n      paste0(substr(ymd,1,4),\"-\",substr(ymd,5,6),\"-\",substr(ymd,7,8),\"-\",\n             substr(hhmm,1,2),\"-\",substr(hhmm,3,4)) else NA_character_\n    comp2 &lt;- gsub(\"-\", \"\", comp1)\n    ymd_dash &lt;- if (!is.na(ymd)) paste0(substr(ymd,1,4),\"-\",substr(ymd,5,6),\"-\",substr(ymd,7,8)) else NA_character_\n    unique(na.omit(c(raw, slug_pt, comp1, comp2, ymd_dash, ymd)))\n  }\n  toks &lt;- .ts_tokens(ts)\n  tok_rx &lt;- gsub(\"[-_]\", \"[-_]\", toks)\n\n  all_files &lt;- list.files(method_dir, pattern = \"\\\\.tif$\", full.names = TRUE, ignore.case = TRUE)\n  if (!length(all_files)) return(NA_character_)\n  b &lt;- tolower(basename(all_files))\n\n  keep_pref &lt;- grepl(paste0(\"^\", m, \"_\"), b)\n  files_m &lt;- all_files[keep_pref]; b_m &lt;- b[keep_pref]\n  if (length(files_m)) {\n    score &lt;- vapply(seq_along(b_m), function(i) {\n      max(c(0, vapply(tok_rx, function(rx) if (grepl(rx, b_m[i], perl = TRUE)) nchar(rx) else 0L, integer(1))))\n    }, numeric(1))\n\n    if (any(score &gt; 0)) {\n      best &lt;- files_m[score == max(score)]\n      bbest &lt;- tolower(basename(best))\n      idxR &lt;- grep(\"_rstar\\\\.tif$\", bbest)\n      if (length(idxR)) return(best[idxR[1]])\n      idxL &lt;- grep(\"_l95\\\\.tif$\", bbest)\n      if (length(idxL)) return(best[idxL[1]])\n      return(best[1])\n    }\n  }\n\n  if (toupper(method) %in% c(\"KED\",\"PREVIEW\")) {\n    out_dir_local &lt;- dirname(method_dir)\n    prev &lt;- list.files(out_dir_local, pattern = \"\\\\.tif$\", full.names = TRUE, ignore.case = TRUE)\n    if (length(prev)) {\n      bp &lt;- tolower(basename(prev))\n      rx_prev &lt;- paste0(\"^(\", paste0(tok_rx, collapse = \"|\"), \")_interpolated(_wgs84)?\\\\.tif$\")\n      hit &lt;- grepl(rx_prev, bp, perl = TRUE)\n      if (any(hit)) return(prev[which(hit)[1]])\n    }\n  }\n  NA_character_\n}\n\n\n\n\nCode\n# Only launch interactively\nif (interactive()) {\n  explanations &lt;- build_explanations(fig_dir = fig_dir, pick_ts = vars[[pick_idx]])\n  run_mc_viewer(\n    vars         = vars,\n    method_dir   = method_dir,\n    fig_dir      = fig_dir,\n    stations_pos = stations_pos,\n    plot_area    = plot_area,\n    wf           = res_one$wf,\n    wf_ex        = res_one$wf_ex,\n    tune         = res_one$tune,\n    tune_ex      = res_one$tune_ex,\n    bench        = res_one$bench,\n    bench_ex     = res_one$bench_ex,\n    tab_err      = res_one$errtab,\n    tab_err_ex   = res_one$errtab_ex,\n    explanations = explanations\n  )\n}",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "reader/mc_2025_microclimate_viewer_merged.html#requirements-helpers",
    "href": "reader/mc_2025_microclimate_viewer_merged.html#requirements-helpers",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "Code\n# Global setup + packages\nset.seed(42)\noptions(width = 100)\n\npkgs &lt;- c(\n  \"sf\",\"terra\",\"raster\",\"dplyr\",\"automap\",\"gstat\",\"mapview\",\"stars\",\n  \"readxl\",\"stringr\",\"tidyr\",\"purrr\",\"lubridate\",\"rprojroot\",\n  \"exactextractr\",\"zoo\",\"ggplot2\",\"viridis\",\"mgcv\",\"randomForest\",\"fields\",\"sp\",\"deldir\",\n  \"leaflet\",\"DT\",\"htmltools\",\"jsonlite\",\"shiny\" # viewer deps\n)\nneed &lt;- setdiff(pkgs, rownames(installed.packages()))\nif (length(need)) install.packages(need, dependencies = TRUE)\ninvisible(lapply(pkgs, function(p) suppressPackageStartupMessages(library(p, character.only = TRUE))))\n\n\n\n\nCode\n# Small utilities (kept identical to your working script where applicable)\n# SpatRaster sicher \"pinnen\" (Datei-basiert) und als gültiges Objekt zurückgeben\n.pin_rast &lt;- function(r, crs = NULL, dir = NULL, name = \"pinned\") {\n  stopifnot(inherits(r, \"SpatRaster\"))\n  if (is.null(dir)) dir &lt;- file.path(getwd(), \"run_cache\")\n  if (!dir.exists(dir)) dir.create(dir, recursive = TRUE)\n  f &lt;- file.path(dir, paste0(name, \".tif\"))\n  # immer schreiben → garantiert datei-gestützt und voll materialisiert\n  terra::writeRaster(r, f, overwrite = TRUE)\n  rp &lt;- terra::rast(f)\n  if (!is.null(crs)) {\n    # nur projezieren, wenn noch nicht identisch\n    same &lt;- try(terra::crs(rp, proj=TRUE) == as.character(crs), silent = TRUE)\n    if (!isTRUE(same)) rp &lt;- terra::project(rp, as.character(crs), method = \"near\")\n  }\n  # Sanity check\n  invisible(terra::nlyr(rp))\n  rp\n}\n\n# Prüfer: „lebt“ der Pointer?\n.is_alive_spatr &lt;- function(r) {\n  inherits(r, \"SpatRaster\") && !inherits(try(terra::nlyr(r), silent = TRUE), \"try-error\")\n}\n\n# Safe, URL-friendly file slug\nslug &lt;- function(x) { \n  x &lt;- gsub(\"[^0-9A-Za-z_-]+\",\"-\", x)\n  x &lt;- gsub(\"-+\",\"-\", x)\n  gsub(\"(^-|-$)\",\"\", x)\n}\n\n# Human-readable time labels from AYYYY... keys\npretty_time &lt;- function(x) {\n  vapply(x, function(s) {\n    if (grepl(\"^A\\\\d{14}$\", s)) {\n      ts &lt;- as.POSIXct(substr(s, 2, 15), format = \"%Y%m%d%H%M%S\", tz = \"UTC\")\n      format(ts, \"%Y-%m-%d %H:%M\")\n    } else if (grepl(\"^A\\\\d{8}(_D)?$\", s)) {\n      ts &lt;- as.Date(substr(s, 2, 9), format = \"%Y%m%d\")\n      format(ts, \"%Y-%m-%d\")\n    } else s\n  }, character(1))\n}\n\n# Pick the most data-dense time-slice (max number of finite observations)\npick_densest_index &lt;- function(sf_wide, var_names) {\n  nn &lt;- sapply(var_names, function(v) sum(is.finite(sf_wide[[v]])))\n  which.max(nn)\n}\n\n# Build figure descriptions (viewer)\nbuild_explanations &lt;- function(fig_dir, pick_ts) {\n  ts_label &lt;- slug(pretty_time(pick_ts))\n  files &lt;- c(\n    \"timeseries_panel_grid.png\",\n    \"timeseries_panel_grid.pdf\",\n    sprintf(\"u_curve_%s.png\", ts_label),\n    sprintf(\"u_curve_extras_%s.png\", ts_label),\n    sprintf(\"benchmark_%s.png\", ts_label),\n    sprintf(\"benchmark_extras_%s.png\", ts_label)\n  )\n  paths &lt;- file.path(fig_dir, files)\n  desc  &lt;- c(\n    \"Per-timestep KED previews; dots=stations; red=plot boundary.\",\n    \"Same as PNG, vector PDF.\",\n    \"U-curve for tuning R via block-CV (drift-only).\",\n    \"U-curve with extra predictors.\",\n    \"Method comparison at R* (lower RMSE is better).\",\n    \"Benchmark with extras at R*.\"\n  )\n  keep &lt;- file.exists(paths)\n  out &lt;- as.list(desc[keep]); names(out) &lt;- basename(paths[keep])\n  out\n}\n\n\n\nWhy this matters: using fine pixels for scale estimation will cap \\(R\\) at a few pixels, resulting in the “10 m fallback”. Keep DEM_scale at a realistic native/coarser resolution for L/R.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "reader/mc_2025_microclimate_viewer_merged.html#project-paths",
    "href": "reader/mc_2025_microclimate_viewer_merged.html#project-paths",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "Code\n# Robust project root finder and paths\nwd &lt;- rprojroot::find_rstudio_root_file()\n\nsource(file.path(wd, \"reader/all_functions_1.R\"))   # consolidated toolkit\n\n#fn_DTM        &lt;- file.path(wd, \"reader/data_2024/copernicus_DEM.tif\")\nfn_DTM        &lt;- file.path(wd, \"reader/data_2024/DEM.tif\")\nfn_stations   &lt;- file.path(wd, \"reader/data_2024/stations_prelim_modifiziert.gpkg\")\nfn_area       &lt;- file.path(wd, \"reader/data_2024/plot.shp\")\nfn_temp_FC29  &lt;- file.path(wd, \"reader/data_2024/all_GW1000A-WIFIFC29.xlsx\")\nfn_temp_DB2F  &lt;- file.path(wd, \"reader/data_2024/all_GW1000A-WIFIDB2F.xlsx\")\ncleandata_rds &lt;- file.path(wd, \"reader/data_2024/climdata.RDS\")\n\nout_dir    &lt;- file.path(wd, \"reader/interpolated\")\nfig_dir    &lt;- file.path(out_dir, \"fig\")\nmethod_dir &lt;- file.path(out_dir, \"methods_compare\")\nreport_dir &lt;- file.path(out_dir, \"report\")\nfor (d in c(out_dir, fig_dir, method_dir, report_dir)) if (!dir.exists(d)) dir.create(d, recursive = TRUE)\n\n# CRS\nepsg &lt;- \"EPSG:32633\"  # UTM zone 33N\nsf_crs_utm33 &lt;- sf::st_crs(epsg)",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "reader/mc_2025_microclimate_viewer_merged.html#base-data-dem-strategy",
    "href": "reader/mc_2025_microclimate_viewer_merged.html#base-data-dem-strategy",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "Code\n# DEMs\nDEM_scale  &lt;- terra::rast(fn_DTM) |&gt; terra::project(epsg)\nDEM_scale  &lt;- terra::aggregate(DEM_scale, c(20, 20))  # coarsen ~20–25 m (as in your working code)\nnames(DEM_scale) &lt;- \"altitude\"\nDEM_render &lt;- DEM_scale |&gt; terra::aggregate(fact = c(10, 10))  # for rendering products\n\ncat(\"DEM_scale res (m): \", paste(terra::res(DEM_scale),  collapse=\" x \"), \"\\n\")\n\n\nDEM_scale res (m):  2.00223686801611 x 2.0022368680127 \n\n\nCode\ncat(\"DEM_render res (m):\", paste(terra::res(DEM_render), collapse=\" x \"), \"\\n\")\n\n\nDEM_render res (m): 20.0223686801606 x 20.022368680127 \n\n\nCode\n# Stations and plot boundary → same CRS\nstations_pos &lt;- sf::st_read(fn_stations, quiet = TRUE) |&gt; sf::st_transform(sf_crs_utm33)\nplot_area    &lt;- sf::st_read(fn_area, quiet = TRUE)     |&gt; sf::st_transform(sf_crs_utm33) |&gt; sf::st_make_valid()\n\n# Altitude from DEM_scale (not the upsampled one)\nstations_pos &lt;- stations_pos %&gt;%\n  dplyr::mutate(altitude = exactextractr::exact_extract(DEM_scale, sf::st_buffer(stations_pos, 1), \"mean\"))\n\n\n\n  |                                                                                                \n  |                                                                                          |   0%\n  |                                                                                                \n  |======                                                                                    |   7%\n  |                                                                                                \n  |=============                                                                             |  14%\n  |                                                                                                \n  |===================                                                                       |  21%\n  |                                                                                                \n  |==========================                                                                |  29%\n  |                                                                                                \n  |================================                                                          |  36%\n  |                                                                                                \n  |=======================================                                                   |  43%\n  |                                                                                                \n  |=============================================                                             |  50%\n  |                                                                                                \n  |===================================================                                       |  57%\n  |                                                                                                \n  |==========================================================                                |  64%\n  |                                                                                                \n  |================================================================                          |  71%\n  |                                                                                                \n  |=======================================================================                   |  79%\n  |                                                                                                \n  |=============================================================================             |  86%\n  |                                                                                                \n  |====================================================================================      |  93%\n  |                                                                                                \n  |==========================================================================================| 100%",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "reader/mc_2025_microclimate_viewer_merged.html#ecowitt-ingestion-cleaning-aggregation",
    "href": "reader/mc_2025_microclimate_viewer_merged.html#ecowitt-ingestion-cleaning-aggregation",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "Code\ntemp_FC29 &lt;- extract_ecowitt_core_vars(fn_temp_FC29)\n\n\nNew names:\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...27`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n• `` -&gt; `...36`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...39`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n• `` -&gt; `...50`\n• `` -&gt; `...51`\n• `` -&gt; `...52`\n• `` -&gt; `...53`\n• `` -&gt; `...54`\n• `` -&gt; `...55`\n• `` -&gt; `...56`\n• `` -&gt; `...57`\n• `` -&gt; `...58`\n• `` -&gt; `...59`\n• `` -&gt; `...60`\n• `` -&gt; `...61`\n• `` -&gt; `...62`\n• `` -&gt; `...63`\n• `` -&gt; `...64`\n• `` -&gt; `...65`\n• `` -&gt; `...66`\n• `` -&gt; `...67`\n• `` -&gt; `...68`\n• `` -&gt; `...69`\n• `` -&gt; `...70`\n• `` -&gt; `...71`\n• `` -&gt; `...72`\n• `` -&gt; `...73`\n• `` -&gt; `...74`\n• `` -&gt; `...75`\n• `` -&gt; `...76`\n• `` -&gt; `...77`\n• `` -&gt; `...78`\n• `` -&gt; `...79`\n• `` -&gt; `...80`\n\n\nCode\ntemp_DB2F &lt;- extract_ecowitt_core_vars(fn_temp_DB2F)\n\n\nNew names:\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...27`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n• `` -&gt; `...36`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...39`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n• `` -&gt; `...50`\n• `` -&gt; `...51`\n• `` -&gt; `...52`\n• `` -&gt; `...53`\n• `` -&gt; `...54`\n• `` -&gt; `...55`\n• `` -&gt; `...56`\n• `` -&gt; `...57`\n\n\nCode\nt_rh_all  &lt;- merge_ecowitt_logger_vars(temp_FC29, temp_DB2F)\n\n# Clean display names and map to verbose station names\nfor (meas in c(\"temperature\",\"humidity\")) {\n  t_rh_all[[meas]] &lt;- t_rh_all[[meas]] %&gt;%\n    dplyr::rename_with(~ to_verbose(.x, ifelse(meas==\"temperature\",\"Temperature\",\"Humidity\")), -Time) %&gt;%\n    clean_names()\n}\n\n# Aggregate to 3-hour steps\ntemp_agg &lt;- t_rh_all$temperature %&gt;%\n  dplyr::mutate(time = lubridate::floor_date(Time, \"3 hours\")) %&gt;%\n  dplyr::group_by(time) %&gt;%\n  dplyr::summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)), .groups = \"drop\")\nnames(temp_agg) &lt;- clean_ids(names(temp_agg))\n\n# long → wide matrix: station rows, time columns\ntemp_matrix &lt;- temp_agg %&gt;%\n  tidyr::pivot_longer(cols = -time, names_to = \"stationid\", values_to = \"value\") %&gt;%\n  tidyr::pivot_wider(names_from = time, values_from = value)\n\n# Join to station geometry and altitude\nstations_pos &lt;- stations_pos %&gt;% dplyr::mutate(stationid = to_verbose(stationid))\nm &lt;- dplyr::left_join(stations_pos, temp_matrix, by = \"stationid\")\n\n# Hygiene\nstations_pos$stationid &lt;- gsub(\"\\\\(℃\\\\)|\\\\(％\\\\)|\\\\(\\\\%\\\\)\", \"\", stations_pos$stationid)\nm$stationid            &lt;- gsub(\"\\\\(℃\\\\)|\\\\(％\\\\)|\\\\(\\\\%\\\\)\", \"\", m$stationid)\nnames(m)               &lt;- fix_names(names(m))\nsaveRDS(m, cleandata_rds)",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "reader/mc_2025_microclimate_viewer_merged.html#interpolation-preview-per-timestep-ked",
    "href": "reader/mc_2025_microclimate_viewer_merged.html#interpolation-preview-per-timestep-ked",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "Code\nmin_pts &lt;- 5\nvars &lt;- as.list(grep(\"^A\\\\d{8,14}\", names(m), value = TRUE))\n\nkriged_list &lt;- lapply(vars, function(v) {\n  interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\")\n})\n\n\nInterpolating: A20230827150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230827150000\n\n\nInterpolating: A20230827180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230827180000\n\n\nInterpolating: A20230828150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230828150000\n\n\nInterpolating: A20230828180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230828180000\n\n\nInterpolating: A20230828210000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230828210000\n\n\nInterpolating: A20230829\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230829\n\n\nInterpolating: A20230829030000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230829030000\n\n\nInterpolating: A20230829060000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230829060000\n\n\nInterpolating: A20230829090000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230829090000_interpolated.tif\n\n\nInterpolating: A20230829120000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230829120000_interpolated.tif\n\n\nInterpolating: A20230829150000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230829150000_interpolated.tif\n\n\nInterpolating: A20230829180000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230829180000_interpolated.tif\n\n\nInterpolating: A20230829210000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230829210000_interpolated.tif\n\n\nInterpolating: A20230830\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230830_interpolated.tif\n\n\nInterpolating: A20230830030000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830030000\n\n\nInterpolating: A20230830060000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830060000\n\n\nInterpolating: A20230830090000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230830090000_interpolated.tif\n\n\nInterpolating: A20230830120000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230830120000_interpolated.tif\n\n\nInterpolating: A20230830150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830150000\n\n\nInterpolating: A20230830180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830180000\n\n\nInterpolating: A20230830210000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230830210000_interpolated.tif\n\n\nInterpolating: A20230831\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230831_interpolated.tif\n\n\nInterpolating: A20230831030000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831030000\n\n\nInterpolating: A20230831060000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831060000\n\n\nInterpolating: A20230831090000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230831090000_interpolated.tif\n\n\nInterpolating: A20230831120000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230831120000_interpolated.tif\n\n\nInterpolating: A20230831150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831150000\n\n\nInterpolating: A20230831180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831180000\n\n\nInterpolating: A20230831210000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/LV-19-d19-006-25/reader/interpolated/A20230831210000_interpolated.tif\n\n\nInterpolating: A20230901\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230901\n\n\nCode\nnames(kriged_list) &lt;- vars\n\n\n\n\nCode\n# Shared color scale across all timestamps\npanel &lt;- timeseries_panel(\n  kriged_list        = kriged_list,\n  plot_area          = plot_area,\n  stations_pos       = stations_pos,\n  cells_target       = 150000,\n  max_cols           = 4,\n  label_pretty_time  = TRUE,\n  out_png            = file.path(fig_dir, \"timeseries_panel_grid.png\"),\n  out_pdf            = file.path(fig_dir, \"timeseries_panel_grid.pdf\"),\n  fill_label         = \"Temperature\"\n)\n\n\nCoordinate system already present. Adding new coordinate system, which will replace the existing\none.\n\n\nCode\npanel$plot",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "reader/mc_2025_microclimate_viewer_merged.html#method-comparison-for-densest-timestamp-working-code",
    "href": "reader/mc_2025_microclimate_viewer_merged.html#method-comparison-for-densest-timestamp-working-code",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "Code\n# Densest timestamp\npick_idx &lt;- pick_densest_index(m, vars)\npick_ts  &lt;- names(m)[pick_idx]\n\n# Extra predictors (kept as in your working code)\nextra_list &lt;- list(\n  slope  = terra::terrain(DEM_scale, v = \"slope\",  unit = \"degrees\"),\n  aspect = terra::terrain(DEM_scale, v = \"aspect\"),\n  tri    = terra::terrain(DEM_scale, v = \"TRI\")\n)\n\n# One timestamp (densest), with extras\nres_one &lt;- run_one(\n  v           = vars[[pick_idx]],\n  m           = m,\n  DEM_render  = DEM_render,\n  DEM_scale   = DEM_scale,\n  method_dir  = method_dir,\n  fig_dir     = fig_dir,\n  report_dir  = report_dir,\n  extra_preds = extra_list,\n  save_figs   = TRUE\n)\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\nR*: tuner failed; falling back to 56.8593 m.\n\n\nWarning in x@pntr$rastDistance(target, exclude, keepNA, tolower(unit), TRUE, : GDAL Message 1:\nPixels not square, distances will be inaccurate.\n\n\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n\n\nR*: tuner failed; falling back to 56.8593 m.\n\n\nWarning in x@pntr$rastDistance(target, exclude, keepNA, tolower(unit), TRUE, : GDAL Message 1:\nPixels not square, distances will be inaccurate.\n\n\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n\n\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "reader/mc_2025_microclimate_viewer_merged.html#optional-compute-all-time-steps",
    "href": "reader/mc_2025_microclimate_viewer_merged.html#optional-compute-all-time-steps",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "Code\ncompute_all &lt;- FALSE\n\nif (isTRUE(compute_all)) {\n  req &lt;- c(\"m\",\"DEM_render\",\"DEM_scale\",\"method_dir\",\"fig_dir\",\"report_dir\")\n  miss &lt;- req[!vapply(req, exists, logical(1), inherits = TRUE)]\n  if (length(miss)) stop(\"Fehlende Objekte im Environment: \", paste(miss, collapse = \", \"))\n\n  .best_from_bench &lt;- function(bench_obj) {\n    if (is.null(bench_obj) || !is.data.frame(bench_obj$table) || nrow(bench_obj$table) &lt; 1)\n      return(NULL)\n    b &lt;- bench_obj$table\n    b &lt;- b[is.finite(b$RMSE), , drop = FALSE]\n    if (!nrow(b)) return(NULL)\n    b &lt;- b[order(b$RMSE), , drop = FALSE]\n    b[1, c(\"method\",\"RMSE\"), drop = FALSE]\n  }\n\n  message(sprintf(\"Starte compute_all für %d Zeitschritte …\", length(vars)))\n\n  res_all &lt;- setNames(lapply(vars, function(vv) {\n    message(\"→ run_one: \", pretty_time(vv))\n    tryCatch(\n      run_one(\n        v           = vv,\n        m           = m,\n        DEM_render  = DEM_render,\n        DEM_scale   = DEM_scale,\n        method_dir  = method_dir,\n        fig_dir     = fig_dir,\n        report_dir  = report_dir,\n        extra_preds = extra_list,\n        save_figs   = TRUE,\n        save_tables = TRUE\n      ),\n      error = function(e) {\n        warning(\"run_one fehlgeschlagen für \", vv, \": \", conditionMessage(e))\n        NULL\n      }\n    )\n  }), vars)\n\n  saveRDS(res_all, file.path(report_dir, \"all_results.RDS\"))\n\n  summ &lt;- do.call(rbind, lapply(names(res_all), function(k) {\n    r &lt;- res_all[[k]]\n    if (is.null(r)) {\n      return(data.frame(\n        ts_key      = k, \n        stamp       = pretty_time(k),\n        R_star      = NA_real_,\n        best_source = NA_character_,  # \"no_extras\"/\"with_extras\"\n        best_method = NA_character_,\n        best_RMSE   = NA_real_\n      ))\n    }\n    rstar &lt;- suppressWarnings(as.numeric(r$tune$R_star))\n    if (!is.finite(rstar)) rstar &lt;- NA_real_\n\n    b0 &lt;- .best_from_bench(r$bench)\n    bE &lt;- .best_from_bench(r$bench_ex)\n\n    score0 &lt;- if (!is.null(b0) && isTRUE(is.finite(b0$RMSE))) b0$RMSE else Inf\n    scoreE &lt;- if (!is.null(bE) && isTRUE(is.finite(bE$RMSE))) bE$RMSE else Inf\n\n    if (is.infinite(score0) && is.infinite(scoreE)) {\n      src &lt;- NA_character_; bm &lt;- NA_character_; br &lt;- NA_real_\n    } else if (score0 &lt;= scoreE) {\n      src &lt;- \"no_extras\"; bm &lt;- b0$method; br &lt;- score0\n    } else {\n      src &lt;- \"with_extras\"; bm &lt;- bE$method; br &lt;- scoreE\n    }\n\n    data.frame(\n      ts_key      = k,\n      stamp       = pretty_time(k),\n      R_star      = rstar,\n      best_source = src,\n      best_method = bm,\n      best_RMSE   = br\n    )\n  }))\n\n  utils::write.csv(summ, file.path(report_dir, \"summary_Rstar_bestmethod.csv\"), row.names = FALSE)\n  message(\"✔ Fertig: summary_Rstar_bestmethod.csv geschrieben.\")\n}",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "reader/mc_2025_microclimate_viewer_merged.html#save-csvs-for-the-one-timestamp",
    "href": "reader/mc_2025_microclimate_viewer_merged.html#save-csvs-for-the-one-timestamp",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "Code\nts_label &lt;- pretty_time(pick_ts)\nbench_base_csv &lt;- file.path(report_dir, sprintf(\"benchmark_%s.csv\",        slug(ts_label)))\nbench_ex_csv   &lt;- file.path(report_dir, sprintf(\"benchmark_extras_%s.csv\", slug(ts_label)))\neb_base_csv    &lt;- file.path(report_dir, sprintf(\"error_budget_%s.csv\",     slug(ts_label)))\neb_ex_csv      &lt;- file.path(report_dir, sprintf(\"error_budget_extras_%s.csv\", slug(ts_label)))\n\nif (is.list(res_one$bench)    && is.data.frame(res_one$bench$table))    write.csv(res_one$bench$table,    bench_base_csv, row.names = FALSE)\nif (is.list(res_one$bench_ex) && is.data.frame(res_one$bench_ex$table)) write.csv(res_one$bench_ex$table, bench_ex_csv,   row.names = FALSE)\nif (is.data.frame(res_one$errtab))    write.csv(res_one$errtab,    eb_base_csv, row.names = FALSE)\nif (is.data.frame(res_one$errtab_ex)) write.csv(res_one$errtab_ex, eb_ex_csv,   row.names = FALSE)",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "reader/mc_2025_microclimate_viewer_merged.html#console-summary",
    "href": "reader/mc_2025_microclimate_viewer_merged.html#console-summary",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "Code\nn_stations &lt;- nrow(stations_pos)\nn_pts_ts   &lt;- sum(is.finite(m[[pick_ts]]))\nLs         &lt;- get_Ls(res_one$wf$L)\nLs_e       &lt;- if (!is.null(res_one$wf_ex)) get_Ls(res_one$wf_ex$L) else NULL\nRstar_base &lt;- suppressWarnings(as.numeric(res_one$tune$R_star))\nRstar_ex   &lt;- suppressWarnings(as.numeric(res_one$tune_ex$R_star))\n\ncat(sprintf(\"Chosen R (micro/local): %s / %s m\\n\",\n            ifelse(is.finite(res_one$wf$R['micro']), round(res_one$wf$R['micro']), \"NA\"),\n            ifelse(is.finite(res_one$wf$R['local']), round(res_one$wf$R['local']), \"NA\")\n))\n\n\nChosen R (micro/local): 19 / 57 m",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "reader/mc_2025_microclimate_viewer_merged.html#shiny-viewer-optional-uses-existing-files",
    "href": "reader/mc_2025_microclimate_viewer_merged.html#shiny-viewer-optional-uses-existing-files",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "The viewer launches a Shiny app; to not block Quarto rendering, the launch is wrapped in if (interactive()).\n\n\n\nCode\n# Minimal viewer injection: use your working run_mc_viewer() with robust file matching\nraster_path &lt;- function(method, ts) {\n  stopifnot(length(method) == 1, length(ts) == 1)\n  m &lt;- tolower(method)\n  .ts_tokens &lt;- function(ts_key) {\n    raw &lt;- tolower(as.character(ts_key))\n    pty &lt;- tolower(pretty_time(ts_key))\n    slug_pt &lt;- gsub(\"[^0-9A-Za-z_-]+\",\"-\", pty)\n    d14 &lt;- sub(\"^a\", \"\", raw)\n    ymd  &lt;- if (nchar(d14) &gt;= 8) substr(d14,1,8) else NA_character_\n    hhmm &lt;- if (nchar(d14) &gt;= 12) substr(d14,9,12) else NA_character_\n    comp1 &lt;- if (!is.na(ymd) && !is.na(hhmm))\n      paste0(substr(ymd,1,4),\"-\",substr(ymd,5,6),\"-\",substr(ymd,7,8),\"-\",\n             substr(hhmm,1,2),\"-\",substr(hhmm,3,4)) else NA_character_\n    comp2 &lt;- gsub(\"-\", \"\", comp1)\n    ymd_dash &lt;- if (!is.na(ymd)) paste0(substr(ymd,1,4),\"-\",substr(ymd,5,6),\"-\",substr(ymd,7,8)) else NA_character_\n    unique(na.omit(c(raw, slug_pt, comp1, comp2, ymd_dash, ymd)))\n  }\n  toks &lt;- .ts_tokens(ts)\n  tok_rx &lt;- gsub(\"[-_]\", \"[-_]\", toks)\n\n  all_files &lt;- list.files(method_dir, pattern = \"\\\\.tif$\", full.names = TRUE, ignore.case = TRUE)\n  if (!length(all_files)) return(NA_character_)\n  b &lt;- tolower(basename(all_files))\n\n  keep_pref &lt;- grepl(paste0(\"^\", m, \"_\"), b)\n  files_m &lt;- all_files[keep_pref]; b_m &lt;- b[keep_pref]\n  if (length(files_m)) {\n    score &lt;- vapply(seq_along(b_m), function(i) {\n      max(c(0, vapply(tok_rx, function(rx) if (grepl(rx, b_m[i], perl = TRUE)) nchar(rx) else 0L, integer(1))))\n    }, numeric(1))\n\n    if (any(score &gt; 0)) {\n      best &lt;- files_m[score == max(score)]\n      bbest &lt;- tolower(basename(best))\n      idxR &lt;- grep(\"_rstar\\\\.tif$\", bbest)\n      if (length(idxR)) return(best[idxR[1]])\n      idxL &lt;- grep(\"_l95\\\\.tif$\", bbest)\n      if (length(idxL)) return(best[idxL[1]])\n      return(best[1])\n    }\n  }\n\n  if (toupper(method) %in% c(\"KED\",\"PREVIEW\")) {\n    out_dir_local &lt;- dirname(method_dir)\n    prev &lt;- list.files(out_dir_local, pattern = \"\\\\.tif$\", full.names = TRUE, ignore.case = TRUE)\n    if (length(prev)) {\n      bp &lt;- tolower(basename(prev))\n      rx_prev &lt;- paste0(\"^(\", paste0(tok_rx, collapse = \"|\"), \")_interpolated(_wgs84)?\\\\.tif$\")\n      hit &lt;- grepl(rx_prev, bp, perl = TRUE)\n      if (any(hit)) return(prev[which(hit)[1]])\n    }\n  }\n  NA_character_\n}\n\n\n\n\nCode\n# Only launch interactively\nif (interactive()) {\n  explanations &lt;- build_explanations(fig_dir = fig_dir, pick_ts = vars[[pick_idx]])\n  run_mc_viewer(\n    vars         = vars,\n    method_dir   = method_dir,\n    fig_dir      = fig_dir,\n    stations_pos = stations_pos,\n    plot_area    = plot_area,\n    wf           = res_one$wf,\n    wf_ex        = res_one$wf_ex,\n    tune         = res_one$tune,\n    tune_ex      = res_one$tune_ex,\n    bench        = res_one$bench,\n    bench_ex     = res_one$bench_ex,\n    tab_err      = res_one$errtab,\n    tab_err_ex   = res_one$errtab_ex,\n    explanations = explanations\n  )\n}",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "reader/mc1.html",
    "href": "reader/mc1.html",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "",
    "text": "froggit shop [DE]\n  \n  \n    \n     ecowitt shop [US]\n  \n  \n    \n     Fine Offset\n  \n\n      \nThe sensors from Fine Offset are re-branded and partly modified by the resellers. This article deals with sensors from the german re-seller froggit and the US re-seller ecowitt. More precise the DP-/GW SmartHubs WiFi Gateway with temperature, humidity & Pressure which is developed by fine offset. The unique selling point of the LoRa-Wifi gateway is the extraordinarily extensive possibility of connecting radio-bound sensors."
  },
  {
    "objectID": "reader/mc1.html#calibration-concept",
    "href": "reader/mc1.html#calibration-concept",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Calibration Concept",
    "text": "Calibration Concept\nThe low budget sensors are usually lacking of a stable measurement quality. To obtain reliable micro climate data a two step calibration process is suggested.\n\nThe measurements of all sensors (preferably in a climate chamber) will be statistically analysed to identify sensor which produce systematic and significant outliers.\nThe sensors are calibrated against an operational running high price reference station in the field.\n\n\n\n\n\n\n\nFuture Calibration Plans\n\n\n\n\n\nFor the future a machine learning approach including the radiation, azimuth, temperature and humidity as predictors for the calibrated temperature as the response variable will be used as an rolling calibration tool."
  },
  {
    "objectID": "reader/mc1.html#switching-scheme",
    "href": "reader/mc1.html#switching-scheme",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Switching scheme",
    "text": "Switching scheme\nThe battery box has a very simple design. Besides the cabling, it contains a solar charge regulator, a fuse panel for the protection of the consumers and an AGM 120aH battery.\n ## Components * Sealable, durable Wham Bam Heavy Duty Box, 62 L, 59,5 x 40 x 37 cm, PP Recycling Plastic Wham Bam Box. The “Wham Bam Box” made of recycled PP plastic was chosen for its extreme mechanical strength and almost complete biochemical resistance. The bad temperature spectrum for thermal stability is from approx. -10 -140 °C., it is acid and alkali resistant and waterproof. By additionally equipping the box with a fire protection mat, the almost airtight closure offers a virtually complete reduction of fire load inside and outside the box. * 12V deep-cycle battery BSA Audio Solar Technologie 120 Ah 12V C100 * 3 x Neutrik powerCON TRUE1 NAC3FPX outlets and Neutrik SCNAC-FPX sealing cover. * Fuse Box for car fuses up to max. 15A per fuse, maximum 30A per fuse box, With sealed cover, splash-proof, Material: PA6.6, 12 connections on the side * Nominal voltage: 32 V/DC * Nominal current (per output): 15 A * Temperature range: -20 - +85 °C * Connections: Flat plug 8x 6,3 x 0,8 mm lateral * Solar charge controller, 20A (ALLPOWERS, available from various brands) Specification ALLPOWERS"
  },
  {
    "objectID": "reader/mc1.html#wiring",
    "href": "reader/mc1.html#wiring",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Wiring",
    "text": "Wiring\n\nBattery to solar charger:\n\nPole terminal connectors (+ and -)\n6 mm2 cables (red and black)\n2 x Crimp cable shoes\n\nSolar panel to solar charger\n\nMC4 photovoltaic connectors (+ and -) Weidemüller\n6 mm2 cables (red and black)\n2x Crimp cable shoes\n\nSolar charger fuse box outlets\n\n6 x 1,5 mm2 cables, red\n6 x 1/4’’ FASTON terminals Fuse Box\n3 x 1,5 mm2 cables, black\n2 x Crimp cable shoes (holding 3 wires)\n6 x 6,35mm / 1/4’’ crimp FASTON terminals\n\n\nPlease note the following points: * Silicone cables, solar cables, plugs and fuse box fulfills industry standards. All cable lugs are crimped and checked. * The cable lugs are not screwed to the charging cables with cable lugs but through the crimp connection with the end sleeve. * A main fuse (e.g. 40A automatic circuit breaker) must be installed\nSee also the figure below."
  },
  {
    "objectID": "reader/mc1.html#mounting",
    "href": "reader/mc1.html#mounting",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Mounting",
    "text": "Mounting\n\nOutlets: 6x M3 screw (12mm), washers and nuts\nSolar connectors: 2 x waterproof cable glands\nSolar charger and fuse box:\n\nWooden plate, glued to the box\n4 screws for Solar Charge Controller\n4 screws for fuse box Cable lugs and plugs are covered with self-vulcanizing tape and additionally insulated."
  },
  {
    "objectID": "reader/mc1.html#station-setup-in-the-field",
    "href": "reader/mc1.html#station-setup-in-the-field",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Station setup in the field",
    "text": "Station setup in the field\nFor safe operation, the following points must be taken into account when setting up the box:\n1.) The box must be placed horizontally. Preferable at on a clearing to reduce impacts of falling branches or similar.  2.) One square meter around the box must be cleared of any vegetation and the A-horizon (depending on the slope, even more).\n 3.) Around this area a further strip with a diameter of at least 1 meter must also be cleared of organic material, especially leaves. Dig up the A-horizon and exclude roots and organic stuff. Note that the wiring sections must also be cleared of combustible organic material.\n 4.) Check cables and screws for proper seating and integrity.\n\n5.) Check proper installation of the solar panel. Mount the panel on a simple wooden slat attached to the frame to avoid damage to the protective foil on the back. Such damage will destroy the panel.\n\n6.) Attach the solar connectors to the panel. This avoids ground contact and provides good weather protection. This can be done very easily by threading cable ties through the plugs and the junction box. {% include figure image_path=“../images/battery_box/07_solar_plugs.jpg” alt=“Attach the solar connectors to the panel.” %}  7.) Finally, the box should be secured against unauthorized or accidental opening. For this purpose there is a steel cable with a number lock, which is to be attached in the way it is placed there."
  },
  {
    "objectID": "reader/mc1.html#final-check",
    "href": "reader/mc1.html#final-check",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Final check",
    "text": "Final check\n\nAll contacts and cables must be checked for proper seating and integrity. Especially the charging cables on the battery must be screwed tightly.\nAll cables are to be laid without tension.\nThe solar cables are to be laid separately to avoid a short circuit, so that an animal crossing etc. does not cause them to come into contact.\nThe box is secured and tight."
  },
  {
    "objectID": "reader/mc1.html#risk-assessment",
    "href": "reader/mc1.html#risk-assessment",
    "title": "Microclimate Sensors & Power supply Units",
    "section": "Risk Assessment",
    "text": "Risk Assessment\nHere you find the preliminary risk assesment for the installation and operation of 12 V solar power based energy supply units and measuring sensor systems."
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html",
    "href": "reader/mc_2025_pipemodel.html",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "",
    "text": "The PipeModel is a deliberately idealized yet physically plausible valley scenario. It distills terrain to the essentials (parabolic cross-valley profile) and optional features (left-side hill, right-side pond or hollow), so that dominant microclimate drivers become visible and testable:\n\nRadiation via terrain exposure cos(i) from slope & aspect\nElevation: daytime negative lapse; pre-dawn weak inversion\nCold-air pooling along the valley axis (Gaussian trough)\nSurface type / land-cover (grass / forest / water / bare soil / maize) alters heating, shading, roughness and nocturnal behaviour\n\nYou can sample synthetic stations, train interpolators (IDW, Kriging variants, RF, GAM), and assess them with spatial LBO-CV.\n\n🔧 This document keeps the previous behaviour but extends the physics with a modular land-cover layer that feeds into both daytime and night fields.\n\n\n\n\nCode\n# Crisp figures\n# EN: Crisp figures\nknitr::opts_chunk$set(fig.width = 9, fig.height = 6, dpi = 150)\n# Alle Chunk-Meldungen global weg\n# EN: Silence messages/warnings in all chunks\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n\n# Hilfsfunktion: gstat & Co. ruhigstellen\n# EN: Helper: silence gstat & friends\nquiet &lt;- function(expr) suppressWarnings(suppressMessages(force(expr)))\n\n\n\n\n\n\n\n\n\n\n\nFunction\nRole\n\n\n\n\nbuild_topography()\nCreates elevation (E), optional hill & pond footprints (+ slope/aspect).\n\n\nbuild_landcover()\nBuilds categorical land-cover raster (grass/forest/water/bare/maize).\n\n\nbuild_physics_fields()\nComputes T14 & T05 from topo + land-cover + sun + noise.\n\n\nbuild_scenario()\nOne-stop wrapper returning all rasters (E, R14, R05, lc, etc.).\n\n\nmake_blocks_and_assign()\nBuilds grid blocks and assigns station points for LBO-CV.\n\n\npred_*()\nPoint-wise predictors: Voronoi, IDW, OK, KED, RF, GAM.\n\n\nrun_lbo_cv()\nLeave-Block-Out cross-validation driver (per-block holdout).\n\n\npredict_maps()\nGrid predictions for each model; returns df + ready-made maps.\n\n\nbuild_panels_with_errors()\nTruth | predictions / error panels with CV residuals overlay.\n\n\nmake_obs_pred_scatter()\nObserved vs predicted scatter per model.\n\n\nblock_metrics_long()\nPer-block RMSE/MAE long table for box/scatter plots.\n\n\nmake_block_metric_box()\nBoxplots of block-wise RMSE/MAE per model.\n\n\nmake_abs_error_box()\nBoxplots of per-station absolute error per model.\n\n\nmake_residual_density()\nResidual density (per model) quick diagnostic.\n\n\nrun_for_time()\nSmall wrapper to run CV + maps + panel for one time slot."
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html#helper-function-cheat-sheet",
    "href": "reader/mc_2025_pipemodel.html#helper-function-cheat-sheet",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "",
    "text": "Function\nRole\n\n\n\n\nbuild_topography()\nCreates elevation (E), optional hill & pond footprints (+ slope/aspect).\n\n\nbuild_landcover()\nBuilds categorical land-cover raster (grass/forest/water/bare/maize).\n\n\nbuild_physics_fields()\nComputes T14 & T05 from topo + land-cover + sun + noise.\n\n\nbuild_scenario()\nOne-stop wrapper returning all rasters (E, R14, R05, lc, etc.).\n\n\nmake_blocks_and_assign()\nBuilds grid blocks and assigns station points for LBO-CV.\n\n\npred_*()\nPoint-wise predictors: Voronoi, IDW, OK, KED, RF, GAM.\n\n\nrun_lbo_cv()\nLeave-Block-Out cross-validation driver (per-block holdout).\n\n\npredict_maps()\nGrid predictions for each model; returns df + ready-made maps.\n\n\nbuild_panels_with_errors()\nTruth | predictions / error panels with CV residuals overlay.\n\n\nmake_obs_pred_scatter()\nObserved vs predicted scatter per model.\n\n\nblock_metrics_long()\nPer-block RMSE/MAE long table for box/scatter plots.\n\n\nmake_block_metric_box()\nBoxplots of block-wise RMSE/MAE per model.\n\n\nmake_abs_error_box()\nBoxplots of per-station absolute error per model.\n\n\nmake_residual_density()\nResidual density (per model) quick diagnostic.\n\n\nrun_for_time()\nSmall wrapper to run CV + maps + panel for one time slot."
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html#d.1-generated-rasters-derived-fields",
    "href": "reader/mc_2025_pipemodel.html#d.1-generated-rasters-derived-fields",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "5.1 D.1 Generated rasters & derived fields",
    "text": "5.1 D.1 Generated rasters & derived fields\n\n\n\n\n\n\n\n\n\nName\nUnit\nWhat it is\nHow it’s built\n\n\n\n\nE (elev)\nm\nGround elevation\nParabolic “half-pipe” across y; + optional hill; − optional pond/hollow\n\n\nslp, asp\nrad\nSlope, aspect\nterra::terrain(E, \"slope\"/\"aspect\", \"radians\")\n\n\nI14, I05\n–\nCosine solar incidence at 14/05 UTC\ncosi_fun(alt, az, slp, asp), clamped to [0,1]\n\n\nlc\ncat\nLand-cover class\n{Forest, Water, Bare Soil, Maize}; rules from hill/slope/water masks\n\n\nhillW\n0–1\nHill weight (1 inside footprint)\nDisk/Gaussian on left third; combines main + optional micro-hills\n\n\nlake\n0/1\nWater mask\n1 only when lake_choice == \"water\" (disk on right third)\n\n\nI14_eff\n–\nShaded incidence (day)\nI14 * shade_fac_by_lc[lc]\n\n\nαI(lc)\n–\nDaytime solar sensitivity by LC\nLook-up from alpha_I_by_lc\n\n\ndawn_bias(lc)\n°C\nAdditive pre-dawn bias by LC\nLook-up from dawn_bias_by_lc\n\n\npool_fac(lc)\n–\nPooling multiplier by LC\nLook-up from pool_fac_by_lc\n\n\nR14 (T14)\n°C\nDaytime “truth” temperature field\nEq. (below)\n\n\nR05 (T05)\n°C\nPre-dawn “truth” temperature field\nEq. (below)"
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html#d.2-governing-equations",
    "href": "reader/mc_2025_pipemodel.html#d.2-governing-equations",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "5.2 D.2 Governing equations",
    "text": "5.2 D.2 Governing equations\nLet $$ be the domain-mean elevation. Define the cross-valley cold-pool kernel\n\\[\n\\texttt{pool\\_base} \\;=\\; A \\exp\\!\\left[-(d_y/w)^2\\right],\\quad d_y=|y-y_0|,\n\\]\nblocked over the hill by (1 − pool_block_gain * hillW).\nDay (14 UTC)\n\\[\nT_{14} \\;=\\; T0_{14} \\;+\\; \\texttt{lapse\\_14}\\,(E-\\overline{E})\n\\;+\\; \\alpha_I(\\texttt{lc})\\, I_{14}^{\\text{eff}}\n\\;+\\; \\varepsilon_{14},\n\\quad\nI_{14}^{\\text{eff}} = I_{14}\\cdot \\texttt{shade\\_fac}(\\texttt{lc}).\n\\]\nPre-dawn (05 UTC)\n\\[\nT_{05} \\;=\\; T0_{05} \\;+\\; \\texttt{inv\\_05}\\,(E-\\overline{E})\n\\;+\\; \\eta_{\\text{slope}}\\;\\texttt{slp}\n\\;-\\; \\texttt{pool\\_base}\\cdot(1-\\texttt{pool\\_block\\_gain}\\cdot\\texttt{hillW})\\cdot \\texttt{pool\\_fac}(\\texttt{lc})\n\\;+\\; \\texttt{dawn\\_bias}(\\texttt{lc})\n\\;+\\; \\varepsilon_{05}.\n\\]\nNoise $_{14},_{05} (0,,0.3^2)$ i.i.d.\n\nNote vs. predecessor: the former warm_bias_water_dawn * lake term is now folded into dawn_bias(lc) (class “Water”); daytime α_map became αI(lc) * I14_eff with explicit canopy shading."
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html#d.3-dials-what-you-can-tweak",
    "href": "reader/mc_2025_pipemodel.html#d.3-dials-what-you-can-tweak",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "5.3 D.3 Dials (what you can tweak)",
    "text": "5.3 D.3 Dials (what you can tweak)\n\n5.3.1 Global scalars\n\n\n\n\n\n\n\n\n\n\nParameter\nDefault\nSensible range\nAffects\nVisual signature (+)\n\n\n\n\nT0_14\n26.0 °C\n20–35\nT14 baseline\nUniform warming\n\n\nlapse_14\n−0.0065 °C/m\n−0.01…−0.002\nT14 vs elevation\nCooler rims, warmer floor\n\n\nT0_05\n8.5 °C\n3–15\nT05 baseline\nUniform warming\n\n\ninv_05\n+0.003 °C/m\n0–0.008\nT05 vs elevation\nRims warmer vs floor\n\n\nη_slope\n0.6\n0–1.5\nT05 slope flow proxy\nSteeper slopes a bit warmer at dawn\n\n\npool_base amplitude\n4.0 K\n1–8\nT05 pooling depth\nStronger blue band on valley axis\n\n\nw_pool\n70 m\n40–150\nT05 pooling width\nNarrower/broader cold band\n\n\npool_block_gain\n0.4\n0–1\nHill blocking\nWarm “tongue” over hill at dawn\n\n\nnoise σ\n0.3 K\n0–1\nBoth\nFine speckle/random texture\n\n\n\n\n\n5.3.2 Land-cover coefficients (by class)\nDefaults used in the code:\n\n\n\n\n\n\n\n\n\n\nLC class\nalpha_I_by_lc\nshade_fac_by_lc\ndawn_bias_by_lc (°C)\npool_fac_by_lc\n\n\n\n\nForest\n3.5\n0.6\n+0.3\n0.7\n\n\nWater\n1.5\n1.0\n+1.2\n0.8\n\n\nBare Soil\n6.0\n1.0\n−0.5\n1.1\n\n\nMaize\n4.5\n0.9\n+0.1\n1.0\n\n\n\nInterpretation: Bare Soil heats most by day and enhances pooling (factor &gt; 1) and cool bias at dawn; Forest damps radiation by day (shading) and reduces pooling (factor &lt; 1); Water heats little by day, gets a positive dawn bias and reduced pooling; Maize sits between grass and forest.\n\n\n5.3.3 Geometry/toggles\n\n\n\n\n\n\n\n\n\nParameter\nDefault\nOptions / range\nEffect\n\n\n\n\nlake_choice\n\"water\"\n\"none\", \"water\", \"hollow\"\nControls depression; only \"water\" sets LC=Water (thermal effects).\n\n\nhill_choice\n\"bump\"\n\"none\", \"bump\"\nAdds blocking & relief.\n\n\nlake_diam_m\n80\n40–150\nSize of pond/hollow.\n\n\nlake_depth_m\n10\n5–30\nDepression depth.\n\n\nhill_diam_m\n80\n40–150\nHill footprint.\n\n\nhill_height_m\n50\n10–120\nHill relief.\n\n\nsmooth_edges\nFALSE\nbool\nSoft pond rim if TRUE.\n\n\nhill_smooth\nFALSE\nbool\nGaussian hill if TRUE.\n\n\n(optional) micro-hills\noff\nrandom_hills, micro_*\nAdds sub-footprint relief; included in hillW."
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html#d.4-quick-recipes",
    "href": "reader/mc_2025_pipemodel.html#d.4-quick-recipes",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "5.4 D.4 Quick “recipes”",
    "text": "5.4 D.4 Quick “recipes”\n\nCloud/haze day → ↓ alpha_I_by_lc (all classes, esp. Bare/Maize) → daytime LC contrasts fade; models lean on elevation/smoothness.\nHotter afternoon → ↑ T0_14 (+1…+3 K) → uniform bias shift; rankings unchanged.\nStronger pooling → ↑ pool_base and/or ↓ w_pool → sharper, deeper trough; drift-aware models gain.\nWater vs hollow → \"water\" sets LC=Water → ↓ daytime heating, ↑ dawn warm bias, ↓ pooling; \"hollow\" keeps only geometry (no water thermals).\nHill blocking → ↑ pool_block_gain → warm dawn tongue over hill; harder CV across blocks.\nCover swaps (what if): set a patch to Bare Soil → warmer day, colder dawn & stronger pooling; to Forest → cooler day, weaker pooling & slight dawn warm-up."
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html#d.5-geometry-at-a-glance",
    "href": "reader/mc_2025_pipemodel.html#d.5-geometry-at-a-glance",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "5.5 D.5 Geometry at a glance",
    "text": "5.5 D.5 Geometry at a glance\n\nValley: $E (y-y_0)^2$ — U-shape across y, uniform along x.\nHill (left third): disk/Gaussian of hill_height_m, diameter hill_diam_m; contributes to hillW.\nPond/Hollow (right third): disk depression of lake_depth_m; LC becomes Water only if lake_choice == \"water\"."
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html#d.6-what-each-term-looks-like-on-maps",
    "href": "reader/mc_2025_pipemodel.html#d.6-what-each-term-looks-like-on-maps",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "5.6 D.6 What each term looks like on maps",
    "text": "5.6 D.6 What each term looks like on maps\n\n\n\n\n\n\n\nTerm\nMap signature\n\n\n\n\nlapse_14 * (E-Ȇ)\nSubtle cool rims / warm floor (day)\n\n\nαI(lc) * I14_eff\nWarm sun-facing slopes; damped under forest/water\n\n\ninv_05 * (E-Ȇ)\nRims warmer vs pooled floor (dawn inversion)\n\n\nη_slope * slp\nSlight dawn warm bias on steeper slopes\n\n\n− pool_base * (1−gain*hillW) * pool_fac(lc)\nBlue band on axis; weaker over forest/water, stronger bare\n\n\n+ dawn_bias(lc)\nLocal dawn warm spots (water/forest), cool bias (bare)"
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html#d.8-settings-of-the-current-example",
    "href": "reader/mc_2025_pipemodel.html#d.8-settings-of-the-current-example",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "5.7 D.8 Settings of the current example",
    "text": "5.7 D.8 Settings of the current example\n\nalpha_I_by_lc = c(Forest=3.5, Water=1.5, Bare=6.0, Maize=4.5), shade_fac_by_lc = c(0.6,1.0,1.0,0.9), dawn_bias_by_lc = c(+0.3,+1.2,−0.5,+0.1), pool_fac_by_lc = c(0.7,0.8,1.1,1.0)."
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html#truth-predictions-error-panels",
    "href": "reader/mc_2025_pipemodel.html#truth-predictions-error-panels",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "8.1 Truth, predictions & error panels",
    "text": "8.1 Truth, predictions & error panels\n\n\nCode\n# --- Variogram/Scale utilities -----------------------------------------------\n# ------------------------------------------------------------------------------\n\n# ------------------------------------------------------------------------------\n# plot_variogram_with_scales(vg, L50, L95, sill, title)\n# Purpose:\n#   Quick ggplot helper to display the empirical variogram with dotted sill and\n#   dashed vertical markers at L50 and L95 for interpretation.\n# Returns: a ggplot object.\n# ------------------------------------------------------------------------------\nplot_variogram_with_scales &lt;- function(vg, L50, L95, sill, title = \"Empirical variogram\") {\n  df &lt;- as.data.frame(vg)\n  ggplot2::ggplot(df, ggplot2::aes(dist, gamma)) +\n    ggplot2::geom_point(size = 1.4) +\n    ggplot2::geom_line(alpha = 0.5) +\n    ggplot2::geom_hline(yintercept = sill, linetype = \"dotted\", linewidth = 0.4) +\n    ggplot2::geom_vline(xintercept = L50, colour = \"#2b8cbe\", linetype = \"dashed\") +\n    ggplot2::geom_vline(xintercept = L95, colour = \"#de2d26\", linetype = \"dashed\") +\n    ggplot2::annotate(\"text\", x = L50, y = 0, vjust = -0.5,\n                      label = sprintf(\"L50 = %.0f m\", L50)) +\n    ggplot2::annotate(\"text\", x = L95, y = 0, vjust = -0.5,\n                      label = sprintf(\"L95 = %.0f m\", L95), colour = \"#de2d26\") +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(title = title, x = \"Distance (m)\", y = \"Semivariance\")\n}\n\n# --- DEM smoothing + sun geometry --------------------------------------------\n# ------------------------------------------------------------------------------\n# gaussian_focal(r, radius_m, sigma_m = NULL)\n# Purpose:\n#   Build a separable, normalized 2D Gaussian kernel (in pixels) from a target\n#   radius in meters, then return the kernel matrix to be used with terra::focal.\n# Inputs:\n#   - r: reference raster to read pixel size from.\n#   - radius_m: target smoothing radius (meters).\n#   - sigma_m: optional sigma; by default half the radius.\n# Returns:\n#   A normalized kernel matrix suitable for terra::focal() smoothing.\n# ------------------------------------------------------------------------------\ngaussian_focal &lt;- function(r, radius_m, sigma_m = NULL) {\n  resx &lt;- terra::res(r)[1]\n  if (is.null(sigma_m)) sigma_m &lt;- radius_m / 2\n  rad_px   &lt;- max(1L, round(radius_m / resx))\n  sigma_px &lt;- max(0.5, sigma_m / resx)\n  xs &lt;- -rad_px:rad_px\n  k1 &lt;- exp(-0.5 * (xs / sigma_px)^2); k1 &lt;- k1 / sum(k1)\n  K  &lt;- outer(k1, k1); K / sum(K)\n}\n\n# ------------------------------------------------------------------------------\n# smooth_dem_and_derive(E, alt, az, radius_m)\n# Purpose:\n#   Smooth the DEM at a given metric radius and recompute slope and\n#   cosine-of-incidence for a specified sun position (alt/az).\n# Returns:\n#   list(Es = smoothed DEM, slp = slope, cosi = cosine-of-incidence)\n# ------------------------------------------------------------------------------\nsmooth_dem_and_derive &lt;- function(E, alt, az, radius_m) {\n  K   &lt;- gaussian_focal(E, radius_m)\n  Es  &lt;- terra::focal(E, w = K, fun = mean, na.policy = \"omit\", pad = TRUE)\n  slp &lt;- terra::terrain(Es, v = \"slope\",  unit = \"radians\")\n  asp &lt;- terra::terrain(Es, v = \"aspect\", unit = \"radians\")\n  zen &lt;- (pi/2 - alt)\n  ci  &lt;- cos(slp)*cos(zen) + sin(slp)*sin(zen)*cos(az - asp)\n  ci  &lt;- terra::ifel(ci &lt; 0, 0, ci)\n  list(Es = Es, slp = slp, cosi = ci)\n}\n\n# --- helpers to cap k by available info --------------------------------\n.k_for_xy &lt;- function(n, n_xy) max(3, min(60, n_xy - 1L, floor(n * 0.8)))\n.kcap_unique &lt;- function(x, kmax) {\n  ux &lt;- unique(x[is.finite(x)])\n  nu &lt;- length(ux)\n  if (nu &lt;= 3) return(0L)                # treat as constant/near-constant\n  max(4L, min(kmax, nu - 1L))\n}\n\n# --- CV of GAM with R-smoothed predictors (robust k) -------------------\n# ------------------------------------------------------------------------------\n# cv_gam_with_R(stn_sf, E, alt, az, R, block_size_m)\n# Purpose:\n#   Leave-block-out CV of a GAM whose predictors are computed from a DEM\n#   smoothed at radius R (meters). This aligns the drift scale to the process\n#   scale before fitting, then evaluates predictive skill via blocked holdouts.\n# Notes:\n#   - Contains guards for low sample size and dynamic k to avoid mgcv errors.\n# Returns:\n#   list(cv = per-point CV table, RMSE = numeric)\n# ------------------------------------------------------------------------------\ncv_gam_with_R &lt;- function(stn_sf, E, alt = NULL, az = NULL, R, block_size_m = NULL) {\n  \n  # ---- 0) Blockgröße sauber auflösen (tuning-fähig)\n  bs &lt;- suppressWarnings(as.numeric(block_size_m)[1])             # bevorzugt: Tuning\n  if (!is.finite(bs) || bs &lt;= 0) {\n    bs &lt;- suppressWarnings(as.numeric(get0(\"block_size\",          # Fallback: global\n                                           ifnotfound = NA_real_)))\n  }\n  if (!is.finite(bs) || bs &lt;= 0)\n    stop(\"cv_gam_with_R(): keine gültige Blockgröße gefunden (Tuning oder global).\")\n  \n  # ---- 1) R-gesmoothete Raster bauen (wie bei dir)\n  zR   &lt;- smooth_mean_R(E, R)\n  slpR &lt;- terra::terrain(zR, v = \"slope\",  unit = \"radians\")\n  aspR &lt;- terra::terrain(zR, v = \"aspect\", unit = \"radians\")\n  cosiR &lt;- if (!is.null(alt) && !is.null(az)) {\n    ci &lt;- cos(slpR)*cos(pi/2 - alt) + sin(slpR)*sin(pi/2 - alt)*cos(az - aspR)\n    terra::ifel(ci &lt; 0, 0, ci)\n  } else NULL\n  \n  # ---- 2) Werte an Stationen extrahieren (wie bei dir)\n  if (!all(c(\"x\",\"y\") %in% names(stn_sf))) {\n    xy &lt;- sf::st_coordinates(stn_sf); stn_sf$x &lt;- xy[,1]; stn_sf$y &lt;- xy[,2]\n  }\n  fill_med &lt;- function(v) { m &lt;- stats::median(v[is.finite(v)], na.rm = TRUE); ifelse(is.finite(v), v, m) }\n  stn_sf$z_surf_R &lt;- fill_med(.extract_to_pts(zR,   stn_sf))\n  stn_sf$slp_R    &lt;- fill_med(.extract_to_pts(slpR, stn_sf))\n  stn_sf$cosi_R   &lt;- if (is.null(cosiR)) rep(NA_real_, nrow(stn_sf)) else fill_med(.extract_to_pts(cosiR, stn_sf))\n  \n  # ---- 3) Blöcke bauen und zuordnen (einheitlich mit bs)\n  bb_poly &lt;- sf::st_as_sfc(sf::st_bbox(stn_sf), crs = sf::st_crs(stn_sf))\n  blocks  &lt;- sf::st_make_grid(bb_poly, cellsize = c(bs, bs), what = \"polygons\")\n  blocks  &lt;- sf::st_sf(block_id = seq_along(blocks), geometry = blocks)\n  \n  stn_blk &lt;- sf::st_join(stn_sf, blocks, join = sf::st_intersects, left = TRUE)\n  if (anyNA(stn_blk$block_id)) {\n    i &lt;- is.na(stn_blk$block_id)\n    stn_blk$block_id[i] &lt;- blocks$block_id[sf::st_nearest_feature(stn_blk[i,], blocks)]\n  }\n  \n  if (!all(c(\"x\",\"y\") %in% names(stn_blk))) {\n    xy &lt;- sf::st_coordinates(stn_blk); stn_blk$x &lt;- xy[,1]; stn_blk$y &lt;- xy[,2]\n  }\n  \n  # ---- 4) CV-Schleife (dein Code unverändert weiter)\n  bids  &lt;- sort(unique(stn_blk$block_id))\n  preds &lt;- vector(\"list\", length(bids)); j &lt;- 0L\n  for (b in bids) {\n    te &lt;- stn_blk[stn_blk$block_id == b, ]\n    tr &lt;- stn_blk[stn_blk$block_id != b, ]\n    \n    dtr &lt;- sf::st_drop_geometry(tr)\n    need &lt;- c(\"temp\",\"x\",\"y\",\"z_surf_R\",\"slp_R\",\"cosi_R\")\n    dtr  &lt;- dtr[stats::complete.cases(dtr[, intersect(need, names(dtr)), drop = FALSE]), , drop = FALSE]\n    if (nrow(dtr) &lt; 10) next\n    \n    n_xy &lt;- dplyr::n_distinct(paste0(round(dtr$x,3), \"_\", round(dtr$y,3)))\n    k_xy &lt;- .k_for_xy(nrow(dtr), n_xy)\n    k_z  &lt;- .kcap_unique(dtr$z_surf_R, 20)\n    k_sl &lt;- .kcap_unique(dtr$slp_R,    12)\n    k_ci &lt;- .kcap_unique(dtr$cosi_R,   12)\n    \n    terms &lt;- c()\n    terms &lt;- c(terms, if (n_xy &gt;= 4) sprintf(\"s(x,y,bs='tp',k=%d)\", k_xy) else \"x + y\")\n    terms &lt;- c(terms, if (k_z  &gt;= 4) sprintf(\"s(z_surf_R,bs='tp',k=%d)\", k_z)  else \"z_surf_R\")\n    if (length(unique(dtr$slp_R[is.finite(dtr$slp_R)])) &gt; 1)\n      terms &lt;- c(terms, if (k_sl &gt;= 4) sprintf(\"s(slp_R,bs='tp',k=%d)\", k_sl) else \"slp_R\")\n    if (any(is.finite(dtr$cosi_R)) && length(unique(dtr$cosi_R[is.finite(dtr$cosi_R)])) &gt; 1)\n      terms &lt;- c(terms, if (k_ci &gt;= 4) sprintf(\"s(cosi_R,bs='tp',k=%d)\", k_ci) else \"cosi_R\")\n    \n    form &lt;- as.formula(paste(\"temp ~\", paste(terms, collapse = \" + \")))\n    gm &lt;- mgcv::gam(form, data = dtr, method = \"REML\", select = TRUE)\n    \n    dte &lt;- sf::st_drop_geometry(te)\n    ph  &lt;- try(stats::predict(gm, newdata = dte, type = \"response\"), silent = TRUE)\n    if (inherits(ph, \"try-error\")) ph &lt;- rep(NA_real_, nrow(dte))\n    \n    j &lt;- j + 1L\n    preds[[j]] &lt;- tibble::tibble(id = te$id, obs = te$temp, pred = as.numeric(ph), block_id = b)\n  }\n  \n  preds &lt;- preds[seq_len(j)]\n  if (!length(preds)) {\n    return(list(cv = tibble::tibble(id = integer(), obs = numeric(), pred = numeric(), block_id = integer()),\n                RMSE = NA_real_))\n  }\n  out  &lt;- dplyr::bind_rows(preds)\n  rmse &lt;- sqrt(mean((out$pred - out$obs)^2, na.rm = TRUE))\n  list(cv = out, RMSE = rmse)\n}\n\n# --- U-curve tuning -----------------------------------------------------------\n# ------------------------------------------------------------------------------\n# tune_Rstar_ucurve(stn_sf, E, alt, az, L50, L95, block_fallback, n_grid, extra)\n# Purpose:\n#   Scan candidate R values (around the L50–L95 interval) and pick R* that\n#   minimises blocked-CV RMSE. Returns the scan table and chosen R*.\n# Returns:\n#   list(grid = data.frame(R, RMSE), R_star, block_m)\n# ------------------------------------------------------------------------------\ntune_Rstar_ucurve &lt;- function(stn_sf, E, alt, az, L50, L95, block_fallback = 120, n_grid = 6, extra = c(0.8, 1.2)) {\n  L50 &lt;- as.numeric(L50); L95 &lt;- as.numeric(L95)\n  if (!is.finite(L50) || !is.finite(L95) || L95 &lt;= L50) {\n    e &lt;- terra::ext(E)\n    dom_diag &lt;- sqrt((terra::xmax(e)-terra::xmin(e))^2 + (terra::ymax(e)-terra::ymin(e))^2)\n    L50 &lt;- dom_diag/10; L95 &lt;- dom_diag/4\n  }\n  block_m &lt;- max(block_fallback, round(L50))\n  R_min &lt;- max(10, round(L50*extra[1])); R_max &lt;- round(L95*extra[2])\n  R_grid &lt;- unique(round(seq(R_min, R_max, length.out = n_grid)))\n  df &lt;- do.call(rbind, lapply(R_grid, function(R) { z &lt;- cv_gam_with_R(stn_sf, E, alt, az, R = R, block_size_m = block_m); c(R = R, RMSE = z$RMSE) })) |&gt; as.data.frame()\n  R_star &lt;- df$R[which.min(df$RMSE)]\n  list(grid = df, R_star = as.numeric(R_star), block_m = block_m)\n}\n\n# ------------------------------------------------------------------------------\n# plot_ucurve(df, R_star, title)\n# Purpose:\n#   Visual helper to display the U-curve of RMSE vs. drift radius R with a\n#   dashed marker at the selected R*.\n# Returns: a ggplot object.\n# ------------------------------------------------------------------------------\nplot_ucurve &lt;- function(df, R_star, title = \"U-curve: tune R\") {\n  ggplot2::ggplot(df, ggplot2::aes(R, RMSE)) +\n    ggplot2::geom_line() + ggplot2::geom_point() +\n    ggplot2::geom_vline(xintercept = R_star, linetype = \"dashed\", colour = \"#de2d26\") +\n    ggplot2::theme_minimal() + ggplot2::labs(title = title, x = \"Drift radius R (m)\", y = \"RMSE (block-CV)\")\n}\n\n# --- Factor alignment (robust) -----------------------------------------------\n.align_factor_to_model &lt;- function(x, lev_model) {\n  xs &lt;- as.character(x)\n  if (length(lev_model) == 0L) return(factor(rep(NA_character_, length(xs))))\n  y &lt;- factor(xs, levels = lev_model)\n  if (anyNA(y)) {\n    xs[is.na(y)] &lt;- lev_model[1]\n    y &lt;- factor(xs, levels = lev_model)\n  }\n  y\n}\n\n\n\n\nCode\n# ---------- SF-only learners ----------\npred_Voronoi &lt;- function(train_sf, test_sf) {\n  idx &lt;- sf::st_nearest_feature(test_sf, train_sf)\n  as.numeric(train_sf$temp)[idx]\n}\n\npred_IDW &lt;- function(train_sf, test_sf, idp = 2) {\n  pr &lt;- suppressWarnings(gstat::idw(temp ~ 1, locations = train_sf, newdata = test_sf, idp = idp))\n  as.numeric(pr$var1.pred)\n}\n\n.default_vgm &lt;- function(values, model = \"Exp\", range = 100) {\n  psill &lt;- stats::var(values, na.rm = TRUE); nug &lt;- 0.1 * psill\n  gstat::vgm(psill = psill, model = model, range = range, nugget = nug)\n}\n\npred_OK &lt;- function(train_sf, test_sf) {\n  vg      &lt;- suppressWarnings(gstat::variogram(temp ~ 1, data = train_sf))\n  vgm_fit &lt;- try(suppressWarnings(gstat::fit.variogram(vg, gstat::vgm(\"Exp\"))), silent = TRUE)\n  if (inherits(vgm_fit, \"try-error\")) vgm_fit &lt;- .default_vgm(train_sf$temp)\n  pr &lt;- suppressWarnings(gstat::krige(temp ~ 1, locations = train_sf, newdata = test_sf, model = vgm_fit))\n  as.numeric(pr$var1.pred)\n}\n\n.align_factor_to_model &lt;- function(x, lev_model) {\n  y &lt;- factor(as.character(x), levels = lev_model)\n  if (anyNA(y)) y[is.na(y)] &lt;- lev_model[1]\n  y\n}\n.fill_num_na_vec &lt;- function(x, ref) {\n  m &lt;- stats::median(ref[is.finite(ref)], na.rm = TRUE)\n  x[!is.finite(x)] &lt;- m\n  x\n}\n\n# --- sf-only KED (schluckt extra Args wie E=E) ----------------------\npred_KED &lt;- function(train_sf, test_sf, ...) {\n  stopifnot(inherits(train_sf, \"sf\"), inherits(test_sf, \"sf\"))\n  need &lt;- c(\"z_surf\",\"slp\",\"cosi\")\n  miss &lt;- setdiff(need, names(train_sf))\n  if (length(miss)) stop(\"pred_KED(): fehlende Drift-Spalten im Training: \",\n                         paste(miss, collapse = \", \"))\n  \n  # Optional LC als Faktor angleichen\n  use_lc &lt;- \"lc\" %in% names(train_sf) && \"lc\" %in% names(test_sf)\n  tr &lt;- train_sf\n  te &lt;- test_sf\n  if (use_lc) {\n    tr$lc &lt;- droplevels(factor(tr$lc))\n    te$lc &lt;- factor(as.character(te$lc), levels = levels(tr$lc))\n    te$lc[is.na(te$lc)] &lt;- levels(tr$lc)[1]\n  }\n  \n  # fehlende numerische Drifts im TEST mit Trainingsmedian auffüllen\n  for (nm in need) {\n    m &lt;- stats::median(tr[[nm]][is.finite(tr[[nm]])], na.rm = TRUE)\n    te[[nm]][!is.finite(te[[nm]])] &lt;- m\n  }\n  \n  # nur vollständige Trainingszeilen\n  keep_tr &lt;- c(\"temp\", need, if (use_lc) \"lc\")\n  dtr &lt;- sf::st_drop_geometry(tr)[, keep_tr, drop = FALSE]\n  ok  &lt;- stats::complete.cases(dtr)\n  tr  &lt;- tr[ok, ]\n  if (nrow(tr) &lt; 5) return(rep(NA_real_, nrow(te)))\n  \n  # Formel: lineare Drifts + optional LC\n  form &lt;- stats::as.formula(paste(\"temp ~\", paste(c(need, if (use_lc) \"lc\"), collapse = \" + \")))\n  \n  # Variogramm + robuster Fit\n  vg      &lt;- suppressWarnings(gstat::variogram(form, data = tr))\n  vgm_fit &lt;- try(suppressWarnings(gstat::fit.variogram(vg, gstat::vgm(\"Exp\"))), silent = TRUE)\n  if (inherits(vgm_fit, \"try-error\")) {\n    ps &lt;- stats::var(sf::st_drop_geometry(tr)$temp, na.rm = TRUE)\n    vgm_fit &lt;- gstat::vgm(psill = ps, model = \"Exp\", range = max(vg$dist, na.rm = TRUE)/3, nugget = 0.1*ps)\n  }\n  \n  # Kriging mit externen Drifts (UK/KED)\n  pr &lt;- suppressWarnings(gstat::krige(form, locations = tr, newdata = te, model = vgm_fit))\n  as.numeric(pr$var1.pred)\n}"
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html#block-wise-and-per-station-errors",
    "href": "reader/mc_2025_pipemodel.html#block-wise-and-per-station-errors",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "8.2 Block-wise and per-station errors",
    "text": "8.2 Block-wise and per-station errors\n\n\nCode\n# --- Leave-Block-Out CV -------------------------------------------------------\n# ------------------------------------------------------------------------------\n# make_blocks_and_assign(pts_sf, E, block_size)\n# Purpose:\n#   Build a square grid of spatial blocks and assign each station to a block\n#   (nearest if on edge). Used by leave-block-out cross-validation.\n# Inputs:\n#   - pts_sf: station sf with geometry.\n#   - E: reference raster for domain extent/CRS.\n#   - block_size: block edge length in meters.\n# Returns:\n#   list(blocks = sf polygons, pts = station sf with block_id).\n# ------------------------------------------------------------------------------\nmake_blocks_and_assign &lt;- function(pts_sf, E, block_size = 100) {\n  bb &lt;- sf::st_as_sfc(sf::st_bbox(c(xmin = terra::xmin(E), ymin = terra::ymin(E), xmax = terra::xmax(E), ymax = terra::ymax(E)), crs = sf::st_crs(pts_sf)))\n  gr &lt;- sf::st_make_grid(bb, cellsize = c(block_size, block_size), what = \"polygons\")\n  blocks &lt;- sf::st_sf(block_id = seq_along(gr), geometry = gr)\n  pts_blk &lt;- sf::st_join(pts_sf, blocks, join = sf::st_intersects, left = TRUE)\n  if (any(is.na(pts_blk$block_id))) {\n    nearest &lt;- sf::st_nearest_feature(pts_blk[is.na(pts_blk$block_id), ], blocks)\n    pts_blk$block_id[is.na(pts_blk$block_id)] &lt;- blocks$block_id[nearest]\n  }\n  list(blocks = blocks, pts = pts_blk)\n}\n\n# saubere Farbskala für viele Blöcke\n.discrete_cols &lt;- function(n) scales::hue_pal()(n)\n\nplot_blocks_grid &lt;- function(blocks, pts_blk, title = \"Blocks & stations\") {\n  # Ziel-CRS = CRS der Daten (UTM32N), Achsen in Metern\n  crs_plot &lt;- sf::st_crs(pts_blk)\n  bb       &lt;- sf::st_bbox(blocks)\n  n_blocks &lt;- dplyr::n_distinct(pts_blk$block_id)\n  cols     &lt;- .discrete_cols(max(1, n_blocks))\n\n  ggplot() +\n    geom_sf(data = blocks, fill = NA, color = \"grey50\", linewidth = 0.25) +\n    geom_sf(data = pts_blk, aes(color = factor(block_id)), size = 2, alpha = 0.95) +\n    scale_color_manual(values = cols, name = \"Block\") +\n    coord_sf(\n      crs  = crs_plot,    # &lt;- erzwingt UTM32N als Plot-CRS (Meterachsen)\n      datum = NA,         # keine Gradnetz-Beschriftung\n      xlim = c(bb[\"xmin\"], bb[\"xmax\"]),\n      ylim = c(bb[\"ymin\"], bb[\"ymax\"]),\n      expand = FALSE\n    ) +\n    theme_minimal() +\n    labs(title = title, x = \"Easting (m)\", y = \"Northing (m)\")\n}\n\n\n# ------------------------------------------------------------------------------\n# run_lbo_cv(stn_sf, E, block_size, models)\n# Purpose:\n#   Perform leave-block-out cross-validation across the requested set of models.\n#   Each block is held out in turn; models are trained on the remainder and\n#   predictions are collected for the held-out stations.\n# Returns:\n#   list(cv = long per-point table, metrics = summary table, diag_plot, blocks_plot)\n# Notes:\n#   - No model settings are changed; this wrapper only orchestrates the CV.\n# ------------------------------------------------------------------------------\nrun_lbo_cv &lt;- function(stn_sf, E, block_size = 100, models = models_use) {\n  if (!all(c(\"x\",\"y\") %in% names(stn_sf))) { xy &lt;- sf::st_coordinates(stn_sf); stn_sf$x &lt;- xy[,1]; stn_sf$y &lt;- xy[,2] }\n  blk &lt;- make_blocks_and_assign(stn_sf, E, block_size = block_size)\n  blocks_sf &lt;- blk$blocks; stn_blk &lt;- blk$pts\n  restore &lt;- function(nm) if (!(nm %in% names(stn_blk))) stn_blk[[nm]] &lt;&lt;- stn_sf[[nm]][match(stn_blk$id, stn_sf$id)]\n  for (nm in c(\"temp\",\"z_surf\",\"slp\",\"cosi\",\"lc\",\"x\",\"y\")) restore(nm)\n\n  block_ids &lt;- sort(unique(stn_blk$block_id))\n  out_list &lt;- vector(\"list\", length(block_ids))\n  for (k in seq_along(block_ids)) {\n    b &lt;- block_ids[k]\n    test_idx  &lt;- which(stn_blk$block_id == b)\n    train_idx &lt;- which(stn_blk$block_id != b)\n    train_sf &lt;- stn_blk[train_idx, ]; test_sf &lt;- stn_blk[test_idx, ]\n    pred_tbl &lt;- lapply(models, function(m) {\n      p &lt;- switch(m,\n        \"Voronoi\" = pred_Voronoi(train_sf, test_sf),\n        \"IDW\"     = pred_IDW(train_sf, test_sf),\n        \"OK\"      = pred_OK(train_sf, test_sf),\n        \"KED\"     = pred_KED(train_sf, test_sf, E = E),\n        \"RF\"      = pred_RF(train_sf, test_sf),\n        \"GAM\"     = pred_GAM(train_sf, test_sf),\n        stop(\"Unknown model: \", m)\n      )\n      tibble::tibble(model = m, id = test_sf$id, obs = test_sf$temp, pred = p, block_id = b)\n    })\n    out_list[[k]] &lt;- dplyr::bind_rows(pred_tbl)\n  }\n\n  cv_tbl &lt;- dplyr::bind_rows(out_list)\n  metrics &lt;- cv_tbl %&gt;%\n    dplyr::group_by(model) %&gt;%\n    dplyr::summarise(\n      n    = dplyr::n(),\n      n_ok = sum(is.finite(obs) & is.finite(pred)),\n      MAE  = {i &lt;- is.finite(obs) & is.finite(pred); if (any(i)) mean(abs(pred[i]-obs[i])) else NA_real_},\n      RMSE = {i &lt;- is.finite(obs) & is.finite(pred); if (any(i)) sqrt(mean((pred[i]-obs[i])^2)) else NA_real_},\n      Bias = {i &lt;- is.finite(obs) & is.finite(pred); if (any(i)) mean(pred[i]-obs[i]) else NA_real_},\n      R2   = safe_r2(obs, pred),\n      .groups = \"drop\"\n    ) |&gt;\n    dplyr::arrange(RMSE)\n\n  diag_plot &lt;- ggplot(cv_tbl, aes(obs, pred)) +\n    geom_abline(slope=1, intercept=0, linetype=\"dashed\") +\n    geom_point(alpha=0.7) +\n    coord_equal() + theme_minimal() +\n    labs(title = sprintf(\"LBO-CV (block = %dm) — Observed vs Predicted\", block_size), x = \"Observed\", y = \"Predicted\") +\n    facet_wrap(~ model)\n\n  blocks_plot &lt;- plot_blocks_grid(blocks_sf, stn_blk, title = sprintf(\"Blocks (%.0f m) & stations\", block_size))\n  list(cv = cv_tbl, metrics = metrics, diag_plot = diag_plot, blocks_plot = blocks_plot)\n}\n\n\n\n\nCode\nmessage(\"Running LBO-CV and building maps for T14 ...\")\nlbo_cv_14_result &lt;- run_for_time(stn_sf_14, scen$R14, \"T14\")\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\nCode\nmessage(\"Running LBO-CV and building maps for T05 ...\")\nlbo_cv_05_result &lt;- run_for_time(stn_sf_05, scen$R05, \"T05\")\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\n\n\nCode\nlbo_cv_14_result$res$blocks_plot \n\n\n\n\n\n\n\n\n\nCode\nlbo_cv_14_result$res$diag_plot\n\n\n\n\n\n\n\n\n\nCode\nlbo_cv_05_result$res$blocks_plot  \n\n\n\n\n\n\n\n\n\nCode\nlbo_cv_05_result$res$diag_plot\n\n\n\n\n\n\n\n\n\nCode\nlbo_cv_14_result$panel\n\n\n$`1`\n\n\n\n\n\n\n\n\n\nCode\nlbo_cv_05_result$panel\n\n\n$`1`\n\n\n\n\n\n\n\n\n\nCode\np_block_box14  &lt;- make_block_metric_box(lbo_cv_14_result$res$cv, \"T14\")\np_abserr_box14 &lt;- make_abs_error_box(lbo_cv_14_result$res$cv,  \"T14\")\np_block_box05  &lt;- make_block_metric_box(lbo_cv_05_result$res$cv, \"T05\")\np_abserr_box05 &lt;- make_abs_error_box(lbo_cv_05_result$res$cv, \"T05\")\n\n(p_block_box14 | p_abserr_box14) / (p_block_box05 | p_abserr_box05)"
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html#lbo-cv-metrics-and-residuals",
    "href": "reader/mc_2025_pipemodel.html#lbo-cv-metrics-and-residuals",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "8.3 LBO-CV metrics and residuals",
    "text": "8.3 LBO-CV metrics and residuals\n\n\nCode\nknitr::kable(lbo_cv_14_result$res$metrics, digits = 3, caption = \"LBO-CV metrics — T14\")\n\n\n\nLBO-CV metrics — T14\n\n\nmodel\nn\nn_ok\nMAE\nRMSE\nBias\nR2\n\n\n\n\nGAM\n60\n60\n0.275\n0.323\n-0.005\n0.798\n\n\nKED\n60\n60\n0.276\n0.333\n0.000\n0.784\n\n\nRF\n60\n60\n0.375\n0.480\n0.011\n0.553\n\n\nIDW\n60\n60\n0.541\n0.711\n0.088\n0.032\n\n\nOK\n60\n60\n0.548\n0.716\n0.049\n0.016\n\n\nVoronoi\n60\n60\n0.667\n0.897\n0.205\n0.039\n\n\n\n\n\nCode\nknitr::kable(lbo_cv_05_result$res$metrics, digits = 3, caption = \"LBO-CV metrics — T05\")\n\n\n\nLBO-CV metrics — T05\n\n\nmodel\nn\nn_ok\nMAE\nRMSE\nBias\nR2\n\n\n\n\nGAM\n60\n60\n0.232\n0.315\n-0.013\n0.933\n\n\nRF\n60\n60\n0.250\n0.323\n-0.036\n0.932\n\n\nOK\n60\n60\n0.624\n0.933\n-0.067\n0.424\n\n\nIDW\n60\n60\n0.792\n1.024\n-0.127\n0.358\n\n\nVoronoi\n60\n60\n0.728\n1.169\n-0.165\n0.346\n\n\nKED\n60\n60\n0.794\n1.633\n0.218\n0.225\n\n\n\n\n\nCode\nmake_obs_pred_scatter(lbo_cv_14_result$res$cv, \"T14\")\n\n\n\n\n\n\n\n\n\nCode\nmake_obs_pred_scatter(lbo_cv_05_result$res$cv, \"T05\")\n\n\n\n\n\n\n\n\n\nCode\nmake_residual_density(lbo_cv_14_result$res$cv, \"T14\")\n\n\n\n\n\n\n\n\n\nCode\nmake_residual_density(lbo_cv_05_result$res$cv, \"T05\")"
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html#g-diagnostic",
    "href": "reader/mc_2025_pipemodel.html#g-diagnostic",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "8.4 G Diagnostic",
    "text": "8.4 G Diagnostic\nAwesome—let’s read your baseline (no R*) results explicitly through the lens of process (what drives T) and scale (over what distances the drivers operate), model-by-model and time-by-time, then close with a scale+process summary and concrete upgrades."
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html#summary",
    "href": "reader/mc_2025_pipemodel.html#summary",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "12.1 Summary",
    "text": "12.1 Summary\nDaytime temperature is controlled by very local facet and LC effects layered over a gentle lapse; models that encode those drivers at the right (small) scale—notably GAM, then RF—generalize across blocks with low error.\nPre-dawn temperature is anisotropic with a short cross-valley pooling scale, slope, and LC offsets; RF captures these thresholdy interactions best, with GAM second. Purely spatial smoothers (OK/IDW/Voronoi) underperform because their smoothing scale and mean process are mismatched.\nBring kriging back into contention by giving it the right drifts (cos(i), LC, distance-to-axis, hill-block) at tuned feature scales (R*), and by acknowledging anisotropy at night; if you want the best of both worlds, use regression-kriging with the learned mean from GAM/RF and an anisotropic residual field."
  },
  {
    "objectID": "reader/mc_2025_pipemodel.html#i.-scale-analysis-l50l95-tuned-ked-drift-r",
    "href": "reader/mc_2025_pipemodel.html#i.-scale-analysis-l50l95-tuned-ked-drift-r",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "17.1 I. Scale analysis — L50/L95 & tuned KED drift (R*)",
    "text": "17.1 I. Scale analysis — L50/L95 & tuned KED drift (R*)\nThis section adds a four-stage pipeline:\n\nScale inference: global variogram → L50/L95\n\nScale-matched predictors: drift from smoothed E at radius R\n\nTune R* with blocked CV (U-curve)\n\nDiagnostics: full benchmark + simple error budget\n\n\nWhy: Matching the model scale to the process scale reduces scale-mismatch error and makes gains attributable to scale rather than algorithm choice.\n\n\n\nCode\n# --- Scale inference (variogram -&gt; L50/L95) ----------------------------------\n# --- SCALE → TUNING → FEATURES @R* → CV → MAPS → PANELS ----------------------\n# add smoothed predictors to both station sets\nadd_drifts_at_R &lt;- function(stn_sf, E, alt, az, R, lc = NULL, lc_levels = NULL) {\n  # R-geglättete Prädiktor-Raster bauen (E*, slp*, cosi*)\n  dr &lt;- smooth_dem_and_derive(E, alt, az, radius_m = R)\n  XY &lt;- sf::st_coordinates(stn_sf)\n  \n  # Extraktion der bei R geglätteten Prädiktoren\n  stn_sf$z_surf &lt;- as.numeric(terra::extract(dr$Es,   XY)[,1])\n  stn_sf$slp    &lt;- as.numeric(terra::extract(dr$slp,  XY)[,1])\n  stn_sf$cosi   &lt;- as.numeric(terra::extract(dr$cosi, XY)[,1])\n  \n  # Optional: Landnutzung als Faktor (nicht geglättet, aber konsistent gemappt)\n  if (!is.null(lc)) {\n    lc_codes &lt;- as.integer(terra::extract(lc, XY)[,1])\n    if (!is.null(lc_levels)) {\n      lc_codes[is.na(lc_codes)] &lt;- 1L\n      lc_codes &lt;- pmax(1L, pmin(lc_codes, length(lc_levels)))\n      stn_sf$lc &lt;- factor(lc_levels[lc_codes], levels = lc_levels)\n    } else {\n      stn_sf$lc &lt;- factor(lc_codes)\n    }\n  }\n  \n  stn_sf\n}\n\n# ---- SF Variogram Scales (ohne sp::) --------------------------------\ncompute_Ls_from_points &lt;- function(stn_sf, value_col = \"temp\",\n                                   maxdist = NULL, nlag = 18, smooth_k = 3) {\n  stopifnot(inherits(stn_sf, \"sf\"), value_col %in% names(stn_sf))\n  pts &lt;- stn_sf[is.finite(stn_sf[[value_col]]), ]\n  if (is.null(maxdist)) {\n    bb &lt;- sf::st_bbox(pts)\n    dom_diag &lt;- sqrt((bb[\"xmax\"]-bb[\"xmin\"])^2 + (bb[\"ymax\"]-bb[\"ymin\"])^2)\n    maxdist &lt;- dom_diag / 2\n  }\n  form &lt;- stats::as.formula(sprintf(\"%s ~ 1\", value_col))\n  vg  &lt;- gstat::variogram(form, data = pts, cutoff = maxdist, width = maxdist/nlag)\n  \n  if (nrow(vg) &gt;= smooth_k) {\n    vg$gamma &lt;- stats::filter(vg$gamma, rep(1/smooth_k, smooth_k), sides = 2)\n    vg$gamma[!is.finite(vg$gamma)] &lt;- zoo::na.approx(vg$gamma, na.rm = FALSE)\n    vg$gamma &lt;- zoo::na.locf(zoo::na.locf(vg$gamma, fromLast = TRUE))\n  }\n  sill &lt;- max(vg$gamma, na.rm = TRUE)\n  if (!is.finite(sill) || sill &lt;= 0) sill &lt;- stats::median(vg$gamma, na.rm = TRUE)\n  \n  L_at_q &lt;- function(q) {\n    thr &lt;- q * sill\n    i   &lt;- which(vg$gamma &gt;= thr)[1]\n    if (is.na(i)) return(NA_real_)\n    if (i == 1) return(vg$dist[1])\n    d0 &lt;- vg$dist[i-1]; d1 &lt;- vg$dist[i]\n    g0 &lt;- vg$gamma[i-1]; g1 &lt;- vg$gamma[i]\n    if (!is.finite(d0) || !is.finite(d1) || g1 == g0) return(d1)\n    d0 + (thr - g0) * (d1 - d0) / (g1 - g0)\n  }\n  list(vg = vg, sill = sill, L50 = L_at_q(0.5), L95 = L_at_q(0.95), cutoff = maxdist)\n}\n\n\n## 1) Skalen (Variogramm) aus den Stationspunkten\nLs14 &lt;- compute_Ls_from_points(stn_sf_14, value_col = \"temp\")\nLs05 &lt;- compute_Ls_from_points(stn_sf_05, value_col = \"temp\")\n\n## (optional) Variogramm-Plots\np_vg14 &lt;- plot_variogram_with_scales(Ls14$vg, Ls14$L50, Ls14$L95, Ls14$sill,\n                                     title = \"T14 — empirical variogram with L50/L95\")\np_vg05 &lt;- plot_variogram_with_scales(Ls05$vg, Ls05$L50, Ls05$L95, Ls05$sill,\n                                     title = \"T05 — empirical variogram with L50/L95\")\n\n## 2) R* via U-Kurve mit GAM@R (geblockte CV; Blockgröße an globalem block_size)\ntune14 &lt;- tune_Rstar_ucurve(\n  stn_sf = stn_sf_14, E = scen$E, alt = sun14$alt, az = sun14$az,\n  L50 = Ls14$L50, L95 = Ls14$L95, block_fallback = block_size, n_grid = 6\n)\ntune05 &lt;- tune_Rstar_ucurve(\n  stn_sf = stn_sf_05, E = scen$E, alt = sun05$alt, az = sun05$az,\n  L50 = Ls05$L50, L95 = Ls05$L95, block_fallback = block_size, n_grid = 6\n)\n\n\n## (optional) U-Kurven plotten\nprint(plot_ucurve(tune14$grid, tune14$R_star, title = \"T14 — U-curve\"))\n\n\n\n\n\n\n\n\n\nCode\nprint(plot_ucurve(tune05$grid, tune05$R_star, title = \"T05 — U-curve\"))\n\n\n\n\n\n\n\n\n\nCode\nmessage(sprintf(\"Chosen R* — T14: %d m | blocks ≈ %d m\", tune14$R_star, tune14$block_m))\nmessage(sprintf(\"Chosen R* — T05: %d m | blocks ≈ %d m\", tune05$R_star, tune05$block_m))\n\n## 3) Feature-Raster @R* (E*, slp*, cosi*)\nfr14 &lt;- smooth_dem_and_derive(scen$E, sun14$alt, sun14$az, radius_m = tune14$R_star)\nfr05 &lt;- smooth_dem_and_derive(scen$E, sun05$alt, sun05$az, radius_m = tune05$R_star)\n\n## 4) Stations-Features @R* (inkl. LC, falls vorhanden)\nstn14_R &lt;- add_drifts_at_R(stn_sf_14, scen$E, sun14$alt, sun14$az, tune14$R_star,\n                           lc = scen$lc, lc_levels = scen$lc_levels)\nstn05_R &lt;- add_drifts_at_R(stn_sf_05, scen$E, sun05$alt, sun05$az, tune05$R_star,\n                           lc = scen$lc, lc_levels = scen$lc_levels)\n\n\n## 5) Block-CV gegen E* (nicht gegen rohes E)\nbench14 &lt;- run_lbo_cv(stn14_R, E = fr14$Es, block_size = block_size, models = models_use)\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\nCode\nbench05 &lt;- run_lbo_cv(stn05_R, E = fr05$Es, block_size = block_size, models = models_use)\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\nCode\n## 6) Karten mit feature_rasters = {E*, slp*, cosi*}\nmaps14_tuned &lt;- predict_maps(\n  stn_sf = stn14_R, truth_raster = scen$R14, which_time = \"T14\",\n  scen = scen, models = models_use, lc_levels = scen$lc_levels,\n  feature_rasters = list(E = fr14$Es, slp = fr14$slp, cosi = fr14$cosi)\n)\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\nCode\nmaps05_tuned &lt;- predict_maps(\n  stn_sf = stn05_R, truth_raster = scen$R05, which_time = \"T05\",\n  scen = scen, models = models_use, lc_levels = scen$lc_levels,\n  feature_rasters = list(E = fr05$Es, slp = fr05$slp, cosi = fr05$cosi)\n)\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\nCode\n## 7) Panels (Truth | Predictions | Error/Residuals) – horizontal, gut lesbar\npanel_pages_T14 &lt;- build_panels_truth_preds_errors_paged(\n  maps          = maps14_tuned,      # list with $pred_rasters etc.\n  truth_raster  = scen$R14,\n  cv_tbl        = bench14$cv,\n  which_time    = \"T14\",\n  models_per_page     = 7,            # all models on one page\n  scatter_next_to_truth = TRUE,\n  top_widths           = c(1.1, 0.9), # optional\n  show_second_legend   = FALSE        # keep only one °C legend\n)\n# render the (only) page\npanel_pages_T05 &lt;- build_panels_truth_preds_errors_paged(\n  maps          = maps05_tuned,      # list with $pred_rasters etc.\n  truth_raster  = scen$R05,\n  cv_tbl        = bench05$cv,\n  which_time    = \"T05\",\n  models_per_page     = 7,            # all models on one page\n  scatter_next_to_truth = TRUE,\n  top_widths           = c(1.1, 0.9), # optional\n  show_second_legend   = FALSE        # keep only one °C legend\n)\n# render the (only) page\nprint(panel_pages_T14[[1]])\n\n\n\n\n\n\n\n\n\nCode\nprint(panel_pages_T05[[1]])\n\n\n\n\n\n\n\n\n\nCode\nprint(bench14)\n\n\n$cv\n# A tibble: 360 × 5\n   model      id   obs  pred block_id\n   &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;\n 1 Voronoi    20  28.6  28.3        1\n 2 Voronoi    45  28.3  28.4        1\n 3 Voronoi    52  28.4  28.3        1\n 4 IDW        20  28.6  28.4        1\n 5 IDW        45  28.3  28.2        1\n 6 IDW        52  28.4  28.2        1\n 7 OK         20  28.6  28.5        1\n 8 OK         45  28.3  28.4        1\n 9 OK         52  28.4  28.3        1\n10 KED        20  28.6  28.8        1\n# ℹ 350 more rows\n\n$metrics\n# A tibble: 6 × 7\n  model       n  n_ok   MAE  RMSE     Bias     R2\n  &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 KED        60    58 0.285 0.368  0.0280  0.752 \n2 GAM        60    57 0.301 0.464 -0.0535  0.641 \n3 RF         60    57 0.378 0.495 -0.00124 0.545 \n4 IDW        60    60 0.541 0.711  0.0880  0.0316\n5 OK         60    60 0.548 0.716  0.0493  0.0159\n6 Voronoi    60    60 0.667 0.897  0.205   0.0393\n\n$diag_plot\n\n\n\n\n\n\n\n\n\n\n$blocks_plot\n\n\n\n\n\n\n\n\n\nCode\nprint(bench05)\n\n\n$cv\n# A tibble: 360 × 5\n   model      id   obs  pred block_id\n   &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;\n 1 Voronoi    20  8.60  9.15        1\n 2 Voronoi    45  8.86  9.03        1\n 3 Voronoi    52  9.03  9.15        1\n 4 IDW        20  8.60  8.32        1\n 5 IDW        45  8.86  8.56        1\n 6 IDW        52  9.03  8.80        1\n 7 OK         20  8.60  7.84        1\n 8 OK         45  8.86  8.44        1\n 9 OK         52  9.03  8.77        1\n10 KED        20  8.60 NA           1\n# ℹ 350 more rows\n\n$metrics\n# A tibble: 6 × 7\n  model       n  n_ok    MAE   RMSE    Bias     R2\n  &lt;chr&gt;   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 GAM        60    43  0.370  0.462  0.0530  0.881\n2 RF         60    43  0.369  0.561  0.0180  0.834\n3 OK         60    60  0.624  0.933 -0.0671  0.424\n4 IDW        60    60  0.792  1.02  -0.127   0.358\n5 Voronoi    60    60  0.728  1.17  -0.165   0.346\n6 KED        60     0 NA     NA     NA      NA    \n\n$diag_plot\n\n\n\n\n\n\n\n\n\n\n$blocks_plot\n\n\n\n\n\n\n\n\n\n\n17.1.1 Reading the outputs\n\nVariogram: dotted sill; dashed L50/L95 → scale anchors for smoothing and block sizes.\n\nU-curve: R* at lowest blocked-CV RMSE; include R = 0 so the tuner can prefer the raw drift.\n\nBenchmark: compare OK / KED / GAM / RF / IDW / Voronoi under the same blocked CV; document block size and R*.\n\nError budget (illustrative): OK → KED(base) → KED(R*) shows gains from drift and from scale matching.\n\n\nFrom concept to practice (pipeline mapping).\n\nEstimate scales: variogram \\(\\rightarrow\\) \\(\\sigma_{\\text{proc}}^2\\), \\(L_{50}\\), \\(L_{95}\\).\nCouple scales: smooth predictors / choose grids according to \\(R_{\\text{micro}}\\), \\(R_{\\text{local}}\\).\nTune \\(R^*\\): block‑CV, U‑curve \\(\\rightarrow\\) stable drift radius.\nBenchmark methods: compare OK/KED/GAM/RF/Trend/IDW/Voronoi at \\(R^*\\) (RMSE/MAE/Bias, document block size).\nProducts: write maps/grids at \\(R^*\\) (and optionally \\(L_{95}\\)); report the error budget.\n\n\n\nKey takeaway: The “smartest” algorithm doesn’t win — the one whose scale matches the process does.\n\n\n\n17.1.2 I.5 Reading the outputs (tables & plots)\nThis section explains how to interpret the key tables and figures produced by the pipeline and how to turn them into a model choice and a scale statement.\n\n17.1.2.1 1) Variogram & scale table (chunk scale-Ls)\n\nWhat you see: Empirical variogram points/line, horizontal dotted line at the (structural) sill, and vertical dashed lines at L50 and L95.\nHow to read it:\n\nNugget (near‑zero intercept) ≈ measurement/microscale noise. A large nugget means close points differ substantially; no method can beat this noise floor.\nSill (plateau) ≈ total variance once pairs are effectively uncorrelated.\nL50 / L95 ≈ pragmatic correlation distances (half vs. ~all structure spent). They are your scale anchors for smoothing radii, neighborhood ranges, and CV block sizes.\n\nQuality checks:\n\nIf no clear plateau: trend/non‑stationarity is likely → consider a drift (elev/sun terms) or a larger domain.\nIf L95 is near the domain size: scales are long; block sizes should be generous to avoid leakage.\nIf the variogram is noisy at large lags: rely more on L50 and the U‑curve outcome.\n\n\n\n\n17.1.2.2 2) U‑curve for tuned drift (chunk scale-tune)\n\nWhat you see: A line plot of RMSE vs. smoothing radius R for KED under blocked CV.\nDecision rule: R* is the radius with the lowest CV‑RMSE.\nWhat shapes mean:\n\nLeft side high (too small R): drift carries microscale noise → overfitting → higher CV error.\nRight side high (too large R): drift is oversmoothed → loses meaningful gradient → bias ↑.\nFlat bottom/plateau: a range of R values are equivalent → pick the smallest R on the plateau for parsimony.\n\nEdge cases: If the minimum sits at the search boundary, widen the R grid and re‑run; if still at the boundary, the field may be trend‑dominated or the covariate is weak.\n\n\n\n17.1.2.3 3) LBO‑CV metrics table (res$metrics)\nFor each model (Voronoi, IDW, OK, KED, GAM, RF) we report:\n\nRMSE (primary): square‑error penalty; most sensitive to outliers. Use this to rank models.\nMAE: median‑like robustness; a useful tie‑breaker alongside RMSE.\nBias (mean error): systematic over/under‑prediction; prefer |Bias| close to 0.\nR²: variance explained in held‑out blocks; interpret cautiously under spatial CV.\nn: number of held‑out predictions contributing.\n\nChoosing a winner:\n\nRank by lowest RMSE under the tuned configuration.\nIf RMSEs are within ~5–10%: prefer the model with lower MAE, lower |Bias|, and more stable block‑wise errors (see next point).\nIf KED (R*) ≈ OK: the drift adds little; the covariate is weak or the process is long‑range. If GAM/RF wins, the relationship is nonlinear or interaction‑rich.\n\n\n\n17.1.2.4 4) Block‑wise diagnostics\n\nBlock error boxes/scatter: Look for narrow distributions (stable across space). Large spread or outliers indicate location‑dependent performance.\nStability index (optional): CV_rmse = sd(RMSE_block) / mean(RMSE_block). Values &lt; 0.25 are typically stable; &gt; 0.4 suggests uneven performance.\nObs vs Pred scatter: Slope ~1 and tight cloud = good calibration; bowed patterns imply bias or missing drift terms.\n\n\n\n17.1.2.5 5) Error budget table (make_simple_error_budget)\nThree rows show how error decreases as structure is added and matched:\n\nBaseline (OK): no drift; sets a structure‑free reference.\nAdd drift (KED base): uses raw covariate; improvement here quantifies signal in the covariate.\nScale‑match drift (KED R*): covariate smoothed at R*; additional gain isolates scale alignment. The Gain_vs_prev column is the incremental improvement at each step.\n\n\nIf KED base ~ KED R*, scale matching adds little (either the raw drift is already at a compatible scale, or the field is insensitive to R). If OK &gt; KED base, the covariate may inject noise or the drift term is mis‑specified.\n\n\n\n\n17.1.3 I.6 Deciding on the best model (and documenting the scale)\nUse this practical, auditable rule set:\n\nPrimary criterion: Lowest CV‑RMSE under blocked CV.\nTie‑breakers: Lower MAE, smaller |Bias|, and better block‑stability.\nParsimony: If multiple models tie, choose the simplest (OK/KED &lt; GAM &lt; RF).\nScale sanity check: Report L50/L95 and verify that R* lies roughly in [L50, 1.5·L95]. If not, discuss why (e.g., strong trend, weak covariate, anisotropy).\nReproducibility: Record the block size, R grid, winning R*, and the full metrics table.\n\n\n\n17.1.4 I.7 Typical patterns & what they imply\n\nHigh nugget, short L50: Expect modest absolute accuracy; prefer coarser R and conservative models. IDW/OK with tight neighborhoods can perform on par with KED.\nLong L95, clear sill: Favor larger neighborhoods and smoother drifts; KED (R*) often dominates.\nGAM/RF &gt; KED: Nonlinear covariate effects or interactions (e.g., slope×aspect). Still align covariates to R* to avoid noise chasing.\nOK ~ KED: Elevation (or chosen drift) is weak for this synthetic setup; consider enriching covariates (slope/aspect/TRI) at matched scales.\n\n\n\n17.1.5 I.8 Checklist before you trust the numbers\n\nBlock size reflects correlation scale (≈ L95).\nU‑curve scanned a broad enough R range; minimum not at boundary.\nR* reported along with L50/L95.\nWinner chosen by blocked CV (not random folds).\nBias near zero; residuals pattern‑free in space.\nFigures/tables archived for reproducibility.\n\n\n\nCode\n# ====================== EXPORT: Plots, Tabellen, Raster ======================\nout_dir &lt;- file.path(\"reader\", \"exports\")\nfig_dir &lt;- file.path(out_dir, \"figs\")\ntab_dir &lt;- file.path(out_dir, \"tables\")\nras_dir &lt;- file.path(out_dir, \"rasters\")\ndat_dir &lt;- file.path(out_dir, \"data\")\n\nfor (d in c(out_dir, fig_dir, tab_dir, ras_dir, dat_dir)) {\n  if (!dir.exists(d)) dir.create(d, recursive = TRUE, showWarnings = FALSE)\n}\n\n# --------- Helfer (nur EINMAL definieren) ---------\nsafe_save_plot &lt;- function(p, file, w = 9, h = 6, dpi = 300) {\n  if (!is.null(p) && inherits(p, c(\"gg\",\"ggplot\",\"patchwork\"))) {\n    try(ggplot2::ggsave(filename = file, plot = p, width = w, height = h,\n                        dpi = dpi, bg = \"white\"), silent = TRUE)\n  }\n}\n\nsafe_write_csv &lt;- function(x, file) {\n  if (!is.null(x)) {\n    # Verzeichnis absichern (falls file Pfad enthält)\n    dir.create(dirname(file), recursive = TRUE, showWarnings = FALSE)\n    try(utils::write.csv(x, file, row.names = FALSE), silent = TRUE)\n  }\n}\n\n# ROBUST: kable als eigenständige HTML-Datei speichern (ohne globales setwd)\nsafe_save_kable &lt;- function(df, file_html, caption = NULL, digits = 3, self_contained = TRUE, ...) {\n  stopifnot(is.character(file_html), length(file_html) == 1, nzchar(file_html))\n  dir_target &lt;- dirname(file_html)\n  if (!dir.exists(dir_target)) dir.create(dir_target, recursive = TRUE, showWarnings = FALSE)\n\n  # Pakete sicherstellen (once)\n  if (!requireNamespace(\"withr\", quietly = TRUE)) install.packages(\"withr\")\n  if (!requireNamespace(\"kableExtra\", quietly = TRUE)) install.packages(\"kableExtra\")\n\n  tab &lt;- knitr::kable(df, digits = digits, caption = caption, format = \"html\")\n\n  # WICHTIG: innerhalb des Zielverzeichnisses speichern und nur den Basisnamen übergeben\n  withr::with_dir(dir_target, {\n    kableExtra::save_kable(tab, basename(file_html), self_contained = self_contained, ...)\n  })\n\n  invisible(normalizePath(file_html, mustWork = FALSE))\n}\n\n# --------- Plots sammeln & speichern ---------\nplots &lt;- list(\n  \"T14_panel_tuned.png\"  = if (exists(\"panel14_tuned\")) panel14_tuned else NULL,\n  \"T05_panel_tuned.png\"  = if (exists(\"panel05_tuned\")) panel05_tuned else NULL,\n  \"T14_blocks.png\"       = if (exists(\"out14\")) out14$res$blocks_plot else NULL,\n  \"T14_diag.png\"         = if (exists(\"out14\")) out14$res$diag_plot   else NULL,\n  \"T05_blocks.png\"       = if (exists(\"out05\")) out05$res$blocks_plot else NULL,\n  \"T05_diag.png\"         = if (exists(\"out05\")) out05$res$diag_plot   else NULL,\n  \"T14_truth.png\"        = if (exists(\"out14\")) out14$maps$p_truth else NULL,\n  \"T14_pred.png\"         = if (exists(\"out14\")) out14$maps$p_pred  else NULL,\n  \"T05_truth.png\"        = if (exists(\"out05\")) out05$maps$p_truth else NULL,\n  \"T05_pred.png\"         = if (exists(\"out05\")) out05$maps$p_pred  else NULL,\n  \"T14_truth_TUNED.png\"  = if (exists(\"maps14_tuned\")) maps14_tuned$p_truth else NULL,\n  \"T14_pred_TUNED.png\"   = if (exists(\"maps14_tuned\")) maps14_tuned$p_pred  else NULL,\n  \"T05_truth_TUNED.png\"  = if (exists(\"maps05_tuned\")) maps05_tuned$p_truth else NULL,\n  \"T05_pred_TUNED.png\"   = if (exists(\"maps05_tuned\")) maps05_tuned$p_pred  else NULL,\n  \"T14_block_box.png\"    = if (exists(\"p_block_box14\"))  p_block_box14  else NULL,\n  \"T14_abserr_box.png\"   = if (exists(\"p_abserr_box14\")) p_abserr_box14 else NULL,\n  \"T05_block_box.png\"    = if (exists(\"p_block_box05\"))  p_block_box05  else NULL,\n  \"T05_abserr_box.png\"   = if (exists(\"p_abserr_box05\")) p_abserr_box05 else NULL,\n  \"T14_obs_pred.png\"     = if (exists(\"p_scatter14\")) p_scatter14 else NULL,\n  \"T05_obs_pred.png\"     = if (exists(\"p_scatter05\")) p_scatter05 else NULL,\n  \"T14_resid_density.png\"= if (exists(\"p_dens14\"))    p_dens14    else NULL,\n  \"T05_resid_density.png\"= if (exists(\"p_dens05\"))    p_dens05    else NULL,\n  \"T14_variogram.png\"    = if (exists(\"p_vg14\")) p_vg14 else NULL,\n  \"T05_variogram.png\"    = if (exists(\"p_vg05\")) p_vg05 else NULL,\n  \"T14_ucurve.png\"       = if (exists(\"p_uc14\")) p_uc14 else NULL,\n  \"T05_ucurve.png\"       = if (exists(\"p_uc05\")) p_uc05 else NULL,\n  \"landcover_vertical.png\" = if (exists(\"p_landcover_vert\")) p_landcover_vert else NULL,\n  \"overview_2x2.png\"       = if (exists(\"p_overview2x2\"))    p_overview2x2    else NULL\n)\nfor (nm in names(plots)) safe_save_plot(plots[[nm]], file.path(fig_dir, nm), w = 9, h = 6, dpi = 300)\n\n# --------- Tabellen & Daten ---------\nif (exists(\"out14\")) {\n  safe_write_csv(out14$res$cv, file.path(dat_dir, \"cv_points_T14.csv\"))\n  safe_write_csv(out14$maps$pred_df, file.path(dat_dir, \"grid_pred_T14.csv\"))\n  safe_write_csv(out14$res$metrics, file.path(tab_dir, \"metrics_T14_base.csv\"))\n  safe_save_kable(out14$res$metrics, file.path(tab_dir, \"metrics_T14_base.html\"), \"LBO-CV metrics — T14\")\n}\nif (exists(\"out05\")) {\n  safe_write_csv(out05$res$cv, file.path(dat_dir, \"cv_points_T05.csv\"))\n  safe_write_csv(out05$maps$pred_df, file.path(dat_dir, \"grid_pred_T05.csv\"))\n  safe_write_csv(out05$res$metrics, file.path(tab_dir, \"metrics_T05_base.csv\"))\n  safe_save_kable(out05$res$metrics, file.path(tab_dir, \"metrics_T05_base.html\"), \"LBO-CV metrics — T05\")\n}\nif (exists(\"bench14\")) {\n  safe_write_csv(bench14$metrics, file.path(tab_dir, \"metrics_T14_tuned.csv\"))\n  safe_save_kable(bench14$metrics, file.path(tab_dir, \"metrics_T14_tuned.html\"), \"Metrics — tuned @ R* (T14)\")\n}\nif (exists(\"bench05\")) {\n  safe_write_csv(bench05$metrics, file.path(tab_dir, \"metrics_T05_tuned.csv\"))\n  safe_save_kable(bench05$metrics, file.path(tab_dir, \"metrics_T05_tuned.html\"), \"Metrics — tuned @ R* (T05)\")\n}\n\n# Skalen & R*\nif (exists(\"tune14\") && exists(\"Ls14\")) {\n  safe_write_csv(tune14$table, file.path(tab_dir, \"Ucurve_T14.csv\"))\n  safe_write_csv(data.frame(L50 = Ls14$L50, L95 = Ls14$L95, R_star = tune14$R_star),\n                 file.path(tab_dir, \"scales_T14.csv\"))\n}\nif (exists(\"tune05\") && exists(\"Ls05\")) {\n  safe_write_csv(tune05$table, file.path(tab_dir, \"Ucurve_T05.csv\"))\n  safe_write_csv(data.frame(L50 = Ls05$L50, L95 = Ls05$L95, R_star = tune05$R_star),\n                 file.path(tab_dir, \"scales_T05.csv\"))\n}\n\n# Optional: Error-Budget\nif (exists(\"eb14\")) safe_write_csv(eb14, file.path(tab_dir, \"error_budget_T14.csv\"))\nif (exists(\"eb05\")) safe_write_csv(eb05, file.path(tab_dir, \"error_budget_T05.csv\"))\n\n# Raster\nif (exists(\"scen\")) {\n  try(terra::writeRaster(scen$E,   file.path(ras_dir, \"E_dem.tif\"),    overwrite = TRUE), silent = TRUE)\n  try(terra::writeRaster(scen$R14, file.path(ras_dir, \"R14_truth.tif\"), overwrite = TRUE), silent = TRUE)\n  try(terra::writeRaster(scen$R05, file.path(ras_dir, \"R05_truth.tif\"), overwrite = TRUE), silent = TRUE)\n  if (\"lc\" %in% names(scen)) try(terra::writeRaster(scen$lc, file.path(ras_dir, \"landcover.tif\"),\n                                                    overwrite = TRUE), silent = TRUE)\n}\nif (exists(\"bench14\") && \"E_star\" %in% names(bench14))\n  try(terra::writeRaster(bench14$E_star, file.path(ras_dir, \"E_star_T14.tif\"), overwrite = TRUE), silent = TRUE)\nif (exists(\"bench05\") && \"E_star\" %in% names(bench05))\n  try(terra::writeRaster(bench05$E_star, file.path(ras_dir, \"E_star_T05.tif\"), overwrite = TRUE), silent = TRUE)\n\n# Sessioninfo\ntry(saveRDS(sessionInfo(), file.path(out_dir, \"sessionInfo.rds\")), silent = TRUE)\n\nmessage(\"✔ Export fertig. Siehe Ordner: \", normalizePath(out_dir))\n# ======================================================================"
  },
  {
    "objectID": "reader/mc_2025_0.html",
    "href": "reader/mc_2025_0.html",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "Quantitative spatial methods rely on the dependence structure of the phenomenon. While Euclidean proximity is a common default, process-defined neighborhoods and relevant covariates often explain spatial patterns more effectively, especially for gap-filling.\n\nLets look at our project region.\n\n\n\nPolsterhaus excursion research area stations 2023 (stars) 2024 (crosses)",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "reader/mc_2025_0.html#why-do-we-want-to-fill-spatial-gaps",
    "href": "reader/mc_2025_0.html#why-do-we-want-to-fill-spatial-gaps",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "Quantitative spatial methods rely on the dependence structure of the phenomenon. While Euclidean proximity is a common default, process-defined neighborhoods and relevant covariates often explain spatial patterns more effectively, especially for gap-filling.\n\nLets look at our project region.\n\n\n\nPolsterhaus excursion research area stations 2023 (stars) 2024 (crosses)",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "reader/mc_2025_0.html#distance-and-data-representation",
    "href": "reader/mc_2025_0.html#distance-and-data-representation",
    "title": "Spatial Interpolation",
    "section": "Distance and data representation",
    "text": "Distance and data representation\nLet’s look more closely at proximity. What is it, exactly, and how can we express it so that space becomes analytically meaningful?\nIn general, spatial relationships are described by neighborhoods (positional) and distances (metric). For spatial analysis or prediction, we also need to quantify spatial influence—how strongly one location affects another. Tobler’s First Law of Geography captures one common objective: near things are more related than distant things. In many real cases spatial influence cannot be measured directly and must be estimated.\n\nNeighborhood\nNeighborhood is a foundational concept. Higher‑dimensional geo‑objects (areas) are neighbors if they touch (e.g., adjacent countries). For points (0‑D objects), neighborhood is commonly defined by distance, however choose k and radius to reflect process scale; consider directional or network-constrained neighborhoods (e.g., k nearest neighbors within a search radius).\n\n\nDistance\nProximity analyses often ask about areas of influence or catchments—the spatial footprints of effects or processes.\n\nInfluence kernels. Specify a weight function \\(w(d)\\) (e.g., exponential or Gaussian) or a covariance model \\(C(h)\\). Estimate parameters (range/sill/nugget) from the data; allow anisotropy if effects propagate preferentially (e.g., along slope or wind). Combine process covariates for the mean, plus a spatial residual for remaining structure.\n\nBecause space can be discretized as vector or raster, distance computation differs accordingly. When contextual constraints are unknown, it is useful to begin with a simple metric such as *Euclidean distance and later refine to network distance, travel time, or cost distance** as the question demands. Also consider feature-space proximity (similar covariates) when processes are driven by environment more than location.\nThe key is operationalization: qualitative notions like “near” and “far” must be translated into a distance metric and units (meters, minutes, etc.). There is rarely a single “correct” choice—only measures more or less suitable for the objective—so defining a meaningful neighborhood relation for the objects of interest is critical.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "reader/mc_2025_0.html#filling-spatial-gaps---the-concepts-of-doing-it",
    "href": "reader/mc_2025_0.html#filling-spatial-gaps---the-concepts-of-doing-it",
    "title": "Spatial Interpolation",
    "section": "Filling spatial gaps - the concepts of doing it",
    "text": "Filling spatial gaps - the concepts of doing it\nWith the basics of distance and neighborhood in place, we can turn to interpolation/prediction of values in space.\nFor decades, deterministic techniques such as nearest neighbor, inverse distance weighting, and spline methods have been popular.\nIn contrast, geostatistical (stochastic) methods like kriging model spatial autocorrelation explicitly. Extensions such as external‑drift kriging and regression kriging combine covariates with the spatial variogram model.\nMore recently, machine learning (ML) approaches (e.g., random forest) have become common for environmental prediction, thanks to their ability to capture non‑linear and complex relationships. In addition spatial structure can be incorporated via coordinates, distance‑based features, spatial cross‑validation, and residual modeling, complementing geostatistical tools rather than replacing them.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "reader/mc_2025_0.html#proximity-a-general-framework",
    "href": "reader/mc_2025_0.html#proximity-a-general-framework",
    "title": "Spatial Interpolation",
    "section": "Proximity: a general framework",
    "text": "Proximity: a general framework\nTo predict in space we must first define who can influence whom and how strongly:\n\nNeighborhood — which locations are eligible to interact (e.g., k-nearest, radius, directional, network/flow-aligned, feature-space).\nMetric — how separation is measured (Euclidean, anisotropic/geodesic, cost, network time, feature-space, spatio-temporal).\nInfluence — how effect decays with separation (kernel weights or covariance/variogram), with optional anisotropy and barriers.\n\nProcess knowledge enters as:\n\na mean component (covariates encoding mechanisms), and\na spatial residual (autocorrelation left after the mean), which you model with an influence function consistent with the process and its characteristic scales.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "reader/mc_2025_0.html#stepwise-integration-of-process-and-scale",
    "href": "reader/mc_2025_0.html#stepwise-integration-of-process-and-scale",
    "title": "Spatial Interpolation",
    "section": "Stepwise integration of process and scale",
    "text": "Stepwise integration of process and scale\nStep 0 — Baseline geometry. Start with simple, isotropic neighborhoods and Euclidean distance. Choose grid resolution consistent with observation support. Report a preliminary characteristic scale (search radius, k, or empirical range).\n\nVoronoi polygons — dividing space geometrically\nVoronoi polygons (Thiessen polygons) provide an elementary geometric definition of proximity. They partition space into regions that are closest to each generator point. In 2D, every location within a polygon is nearer to its seed point than to any other.\n\n\n\nThe blue dots are irregularly distributed stations (rain gauges in Switzerland). The overlaid polygons are the corresponding Voronoi cells that define the closest geometric areas (gisma 2021).\n\n\nVoronoi tessellations mirror organizational principles seen in nature (e.g., plant cells) and human geography (e.g., central places per Christaller). Two assumptions usually apply: (1) nothing else is known about the space between sampled locations; and (2) the boundary between two samples is an approximation—a convenient abstraction rather than a physical discontinuity. Within each polygon, the attribute is implicitly treated as homogeneous.\n\nIf we know more about spatial relationships, we can go beyond purely geometric proximity.\n\nStep 1 — Process in the mean. Add covariates that carry mechanism (elevation, radiation/SVF, LAD, land cover, distance to ridge, flow accumulation). This captures broad gradients and reduces bias.\nStep 2 — Scale and direction in the residual. Fit a residual influence structure that matches the process: directional/anisotropic where flow or wind channels influence; cost/geodesic where uphill/roughness penalizes spread; barriers across ridges, walls, or water.\nStep 3 — Nonstationarity where needed. Allow parameters to vary (local variograms, moving windows) or use flexible smoothers to accommodate gradual changes in scale or orientation.\nStep 4 — Validation and applicability. Use spatial/block CV aligned with process (e.g., flow-aligned folds). Check area of applicability in feature space to avoid “near in XY, far in process.” Sensitivity-test metric, kernel, and scale choices.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "reader/mc_2025_0.html#what-each-method-encodes-concept-process-scale",
    "href": "reader/mc_2025_0.html#what-each-method-encodes-concept-process-scale",
    "title": "Spatial Interpolation",
    "section": "What each method encodes (concept, process, scale)",
    "text": "What each method encodes (concept, process, scale)\nNearest Neighbor (NN)\n\nProximity: Voronoi cells (closest site wins).\nProcess assumption: Piecewise constant; no smoothing.\nScale control: Implicitly set by station spacing.\nUse when: Quick gap diagnostic or very sharp local patchiness.\n\nInverse Distance Weighted (IDW)\n\nProximity: Monotone kernel in Euclidean distance.\nProcess assumption: Isotropic decay; no explicit uncertainty.\nScale control: Power and search radius (higher power = shorter effective range).\nUse when: Simple, transparent weighting with mild smoothing.\n\nKriging (OK)\n\nProximity: Variogram/covariance defines influence.\nProcess assumption: Second-order stationarity of residual; supports anisotropy.\nScale control: Variogram range(s), nugget; directional ranges for anisotropy.\nUse when: You want uncertainty and a principled spatial model.\n\nKriging with External Drift (KED)\n\nProximity: Covariates in the mean + variogram on residuals.\nProcess assumption: Mechanisms captured by covariates; remaining structure is correlated noise.\nScale control: Covariate smoothness sets large-scale trend; residual variogram sets local scale.\nUse when: Strong drivers are known (radiation, elevation, LAD), but local correlation persists.\n\nThin Plate Spline Regression (TPS)\n\nProximity: Smoothness prior via spline kernel; influence decays with distance and penalty.\nProcess assumption: Broad, smooth fields (trend-like behavior).\nScale control: Smoothing parameter (penalty) and basis complexity.\nUse when: Continuous, gently varying surfaces; robust baseline trend.\n\nTriangular Irregular Surface (TIN)\n\nProximity: Local planar facets on the Delaunay triangulation.\nProcess assumption: Locally planar continuity with sharp breaks possible along edges.\nScale control: Triangle density (data spacing) and any enforced smoothing.\nUse when: Elevation-like surfaces or where local planes are appropriate.\n\nGeneralized Additive Model (GAM)\n\nProximity: Encoded through smooth functions of covariates (and optionally s(x,y)).\nProcess assumption: Nonlinear process relationships in the mean; residuals ideally weakly correlated.\nScale control: Basis dimension and penalties per smoother (feature-space scale); add spatial residual modeling if needed.\nUse when: Rich covariates, nonlinear effects, and desire to keep spatial residuals small.\n\nAdd this right after Generalized Additive Model (GAM):\nRandom Forest (RF)\n\nProximity: Encoded indirectly via covariates (you can include s(x,y), distances, neighborhood stats, cost distances, etc. as features). Spatial dependence handled post-hoc via residual mapping if needed.\nProcess assumption: Complex nonlinear relationships and interactions in the mean; no explicit spatial covariance or uncertainty model. Extrapolation beyond training feature space is weak—check area of applicability.\nScale control: Through feature engineering (window sizes, multi-scale covariates), and tree hyperparameters (min node size, mtry, tree depth). Effective spatial scale emerges from the features you provide.\nUse when: Many and rich covariates, strong nonlinearities, and you want robust prediction without specifying a parametric mean. Pair with residual kriging if spatial autocorrelation remains or if you need uncertainty maps.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "reader/mc_2025_0.html#minimal-decision-guide",
    "href": "reader/mc_2025_0.html#minimal-decision-guide",
    "title": "Spatial Interpolation",
    "section": "Minimal decision guide",
    "text": "Minimal decision guide\n\nOnly positions, need a fast baseline: NN → IDW → TIN (if local planes fit).\nSmooth trend dominates: TPS or GAM (covariates preferred).\nClear drivers + local correlation: KED (covariates) + residual kriging.\nUncertainty mapping required / correlation central: Kriging family (OK/KED).\nDirectional/channelized processes: Use anisotropic/cost-based metrics in the residual stage.\nRich covariates, nonlinear effects: GAM or RF (prefer RF when interactions dominate; add residual kriging if spatial correlation persists).\n\nThe map at http://earth-observation-network.github.io/EON2025/block4_5/swiss_models_one_legend_bluewhite.html contrasts six interpolation methods for precipitation in Switzerland (plus the Voronoi tessellation for reference).\n\n\n\nBlue dots: station locations; dot size: precipitation (mm). Overlaid polygons: Voronoi cells (gisma 2021). Top‑left: k‑nearest neighbor (k = 3–5); Top‑right: IDW; Middle‑left: automatic kriging (default settings); Middle‑right: thin‑plate spline regression; Bottom‑left: TIN surface; Bottom‑right: generalized additive model (GAM).\n\n\n\nChoosing an interpolation method: scales & processes\nThink in terms of the process you want to capture and the scale at which it operates.\n\nProcess & characteristic scale: What is the expected range/decay length of spatial dependence? Sample spacing should be finer than this scale; choose kernels/variogram ranges to match it and avoid aliasing.\nSupport & resolution: Are observations point‑like or area‑averaged? Align prediction grid (cell size) and smoothing with observation support; be explicit about change‑of‑support.\nStationarity & anisotropy: If trends or directional effects exist (e.g., along valleys/ridges), include drift/covariates, use local models, or fit directional variograms—don’t assume isotropy.\nSampling layout vs gradients: Ensure coverage across major drivers (elevation, aspect, land cover). Space‑filling/stratified designs beat clustered convenience samples.\nSample size vs model complexity: Parameter‑rich models (nested variograms, many ML features) need more data. Keep models as simple as the data allow.\nUncertainty & validation: Use spatial cross‑validation; map predictions and uncertainty (kriging variance, ensemble spread). Report MAE/RMSE at the scale of use.\nScale interactions (MAUP): State analysis scale explicitly; resample/aggregate covariates consistently to avoid scale mismatch.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "reader/mc_2025_0.html#handson-our-data",
    "href": "reader/mc_2025_0.html#handson-our-data",
    "title": "Spatial Interpolation",
    "section": "Hands‑on: our data",
    "text": "Hands‑on: our data\nDownload the exercises repository (as a ZIP), then unpack it locally. Open the .Rproj in RStudio and start with the provided exercises.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "reader/digitize.html",
    "href": "reader/digitize.html",
    "title": "Untitled",
    "section": "",
    "text": "With mapedit, each class must be digitized individually. Once the training areas are available as vector data, the features of the respective raster stack can be extracted into a table according to the digitized classes and corrected for possible missing values.\n\n\nFor this exercise, we use mapedit, a small but powerful package that allows you to digitize and edit vector data in Rstudio or an external browser. In combination with mapview, any [color composite] (https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/composites/) can also be used as a basis for digitization.\n\n## ##\n# ---- 0 Projekt Setup ----\nrequire(\"pacman\")\n#remotes::install_github(\"zivankaraman/CDSE\")\n# packages installing if necessary and loading\npacman::p_load(mapview, mapedit, tmap, tmaptools, raster, terra, stars, gdalcubes, sf, dplyr,CDSE, downloader, tidyverse,RStoolbox,rprojroot, exactextractr, randomForest, ranger, e1071, caret, link2GI, rstac, OpenStreetMap,colorspace)\n\n#--- Switch to determine whether digitization is required. If set to FALSE, the\nroot_folder = find_rstudio_root_file()\n\nm1 = tm_shape(pred_stack_2019) + tm_rgb(r=4, g=3, b=2) +\n  tm_layout(legend.outside.position = \"right\",\n            legend.outside = T,\n            panel.label.height=0.6,\n            panel.label.size=0.6,\n            panel.labels = c(\"r=1, g=2, b=3\")) +\n  tm_grid()\n\nm2 = tm_shape(pred_stack_2019) + tm_rgb(r=8, g=4, b=3) +\n  tm_layout(legend.outside.position = \"right\",\n            legend.outside = T,\n            panel.label.height=0.6,\n            panel.label.size=0.6,\n            panel.labels = c(\"r=8, g=4, b=3\")) +\n  tm_grid()\ntmap::tmap_arrange(m1,m2)\n\n\nThe planes can be switched using the plane control. In true-color composites, the visible spectral channels Red (B04), Green (B03), and Blue (B02) are mapped to the corresponding red, green, and blue color channels, respectively, producing an image of the surface that closely resembles the natural “color” as it would be seen by a human sitting on the spacecraft. False color images are often created using the spectral channels for near-infrared, red, and green. They are particularly useful for assessing vegetation because plants reflect near-infrared and green light while absorbing red light (red-edge effect). Dense vegetation appears a darker red. Cities and open ground appear gray or light brown, water appears blue or black.\n\n## ##\n\n#---- Digitization of training data ----\n\nif (digitize) {\n# For the supervised classification, we need training areas. You can digitize them as shown below or alternatively use QGis, for example\n\n# clearcut\n\n# For the false color composite r = 8, g = 4, b = 3, maxpixels = 1693870)\n# maxpixels has significantly higher memory requirements, vegetation in red\n# below the true color composite\ntrain_area_2019 &lt;- mapview::viewRGB(pred_stack_2019, r = 4, g = 3, b = 2, maxpixels = 1693870) %&gt;% mapedit::editMap()\n# Adding the attributes class (text) and id/year (integer)\nclearcut_2019 &lt;- train_area_2019$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"clearcut\", id = 1,year=2019)\ntrain_area_2020 &lt;- mapview::viewRGB(pred_stack_2020, r = 4, g = 3, b = 2,maxpixels = 1693870) %&gt;% mapedit::editMap()\nclearcut_2020 &lt;- train_area_2020$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"clearcut\", id = 1,year=2020)\n\n# other: all areas not belonging to clear cutting as representative as possible\ntrain_area_2019 &lt;- mapview::viewRGB(pred_stack_2019, r = 4, g = 3, b = 2) %&gt;% mapedit::editMap()\nother_2019 &lt;- train_area_2019$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"other\", id = 2,year=2019)\ntrain_area_2020 &lt;- mapview::viewRGB(pred_stack_2020, r = 4, g = 3, b = 2) %&gt;% mapedit::editMap()\nother_2020 &lt;- train_area_2020$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"other\", id = 2,year=2020)\n\ntrain_areas_2019_2020 &lt;- rbind(clearcut_2019,clearcut_2020, other_2019,other_2020) # Reproject to the raster file\ntrain_areas_2019 = sf::st_transform(train_areas_2019_2020,crs = sf::st_crs(pred_stack_2019))\nmapview(filter(train_areas_2019_2020,year==2019), zcol=\"class\")\n# save geometries\nst_write(train_areas_2019_2020,paste0(envrmt$path_data,\"train_areas_2019_2020.gpkg\"))\n\n# Extract the training data for the digitized areas\ntDF_2019 = exactextractr::exact_extract(pred_stack_2019, filter(train_areas_2019_2020,year==2019), force_df = TRUE,\ninclude_cell = TRUE,include_xy = TRUE,full_colnames = TRUE,include_cols = \"class\")\ntDF_2020 = exactextractr::exact_extract(pred_stack_2020, filter(train_areas_2019_2020,year==2020), force_df = TRUE,\ninclude_cell = TRUE,include_xy = TRUE,full_colnames = TRUE,include_cols = \"class\")\n\n# again, copy together into a file\ntDF_2019 = dplyr::bind_rows(tDF_2019)\ntDF_2019$year = 2019\ntDF_2020 = dplyr::bind_rows(tDF_2020)\ntDF_2020$year = 2020\n# Delete any rows that contain NA (no data) values\ntDF_2019 = tDF_2019[complete.cases(tDF_2019) ,]\ntDF_2020 = tDF_2020[complete.cases(tDF_2020) ,]\n\ntDF= rbind(tDF_2019,tDF_2020)\n\n# check the extracted data\nsummary(tDF)\n\n# Save as R internal data format\n# is stored in the repo and can therefore be loaded (line below)\nsaveRDS(tDF, paste0(envrmt$path_data,\"tDF.rds\"))\n\n\n\n} else {\ntDF = readRDS(paste0(envrmt$path_data,\"tDF.rds\"))\n}\n\nThe result is a table with training data for 2019 and 2020. The data set contains all raster information for all bands covered by the polygons for the classes “clearcut” and “other”.\n\n## ##\nhead(tDF)"
  },
  {
    "objectID": "reader/digitize.html#creating-training-areas-with-mapedit",
    "href": "reader/digitize.html#creating-training-areas-with-mapedit",
    "title": "Untitled",
    "section": "",
    "text": "With mapedit, each class must be digitized individually. Once the training areas are available as vector data, the features of the respective raster stack can be extracted into a table according to the digitized classes and corrected for possible missing values.\n\n\nFor this exercise, we use mapedit, a small but powerful package that allows you to digitize and edit vector data in Rstudio or an external browser. In combination with mapview, any [color composite] (https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/composites/) can also be used as a basis for digitization.\n\n## ##\n# ---- 0 Projekt Setup ----\nrequire(\"pacman\")\n#remotes::install_github(\"zivankaraman/CDSE\")\n# packages installing if necessary and loading\npacman::p_load(mapview, mapedit, tmap, tmaptools, raster, terra, stars, gdalcubes, sf, dplyr,CDSE, downloader, tidyverse,RStoolbox,rprojroot, exactextractr, randomForest, ranger, e1071, caret, link2GI, rstac, OpenStreetMap,colorspace)\n\n#--- Switch to determine whether digitization is required. If set to FALSE, the\nroot_folder = find_rstudio_root_file()\n\nm1 = tm_shape(pred_stack_2019) + tm_rgb(r=4, g=3, b=2) +\n  tm_layout(legend.outside.position = \"right\",\n            legend.outside = T,\n            panel.label.height=0.6,\n            panel.label.size=0.6,\n            panel.labels = c(\"r=1, g=2, b=3\")) +\n  tm_grid()\n\nm2 = tm_shape(pred_stack_2019) + tm_rgb(r=8, g=4, b=3) +\n  tm_layout(legend.outside.position = \"right\",\n            legend.outside = T,\n            panel.label.height=0.6,\n            panel.label.size=0.6,\n            panel.labels = c(\"r=8, g=4, b=3\")) +\n  tm_grid()\ntmap::tmap_arrange(m1,m2)\n\n\nThe planes can be switched using the plane control. In true-color composites, the visible spectral channels Red (B04), Green (B03), and Blue (B02) are mapped to the corresponding red, green, and blue color channels, respectively, producing an image of the surface that closely resembles the natural “color” as it would be seen by a human sitting on the spacecraft. False color images are often created using the spectral channels for near-infrared, red, and green. They are particularly useful for assessing vegetation because plants reflect near-infrared and green light while absorbing red light (red-edge effect). Dense vegetation appears a darker red. Cities and open ground appear gray or light brown, water appears blue or black.\n\n## ##\n\n#---- Digitization of training data ----\n\nif (digitize) {\n# For the supervised classification, we need training areas. You can digitize them as shown below or alternatively use QGis, for example\n\n# clearcut\n\n# For the false color composite r = 8, g = 4, b = 3, maxpixels = 1693870)\n# maxpixels has significantly higher memory requirements, vegetation in red\n# below the true color composite\ntrain_area_2019 &lt;- mapview::viewRGB(pred_stack_2019, r = 4, g = 3, b = 2, maxpixels = 1693870) %&gt;% mapedit::editMap()\n# Adding the attributes class (text) and id/year (integer)\nclearcut_2019 &lt;- train_area_2019$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"clearcut\", id = 1,year=2019)\ntrain_area_2020 &lt;- mapview::viewRGB(pred_stack_2020, r = 4, g = 3, b = 2,maxpixels = 1693870) %&gt;% mapedit::editMap()\nclearcut_2020 &lt;- train_area_2020$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"clearcut\", id = 1,year=2020)\n\n# other: all areas not belonging to clear cutting as representative as possible\ntrain_area_2019 &lt;- mapview::viewRGB(pred_stack_2019, r = 4, g = 3, b = 2) %&gt;% mapedit::editMap()\nother_2019 &lt;- train_area_2019$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"other\", id = 2,year=2019)\ntrain_area_2020 &lt;- mapview::viewRGB(pred_stack_2020, r = 4, g = 3, b = 2) %&gt;% mapedit::editMap()\nother_2020 &lt;- train_area_2020$finished$geometry %&gt;% st_sf() %&gt;% mutate(class = \"other\", id = 2,year=2020)\n\ntrain_areas_2019_2020 &lt;- rbind(clearcut_2019,clearcut_2020, other_2019,other_2020) # Reproject to the raster file\ntrain_areas_2019 = sf::st_transform(train_areas_2019_2020,crs = sf::st_crs(pred_stack_2019))\nmapview(filter(train_areas_2019_2020,year==2019), zcol=\"class\")\n# save geometries\nst_write(train_areas_2019_2020,paste0(envrmt$path_data,\"train_areas_2019_2020.gpkg\"))\n\n# Extract the training data for the digitized areas\ntDF_2019 = exactextractr::exact_extract(pred_stack_2019, filter(train_areas_2019_2020,year==2019), force_df = TRUE,\ninclude_cell = TRUE,include_xy = TRUE,full_colnames = TRUE,include_cols = \"class\")\ntDF_2020 = exactextractr::exact_extract(pred_stack_2020, filter(train_areas_2019_2020,year==2020), force_df = TRUE,\ninclude_cell = TRUE,include_xy = TRUE,full_colnames = TRUE,include_cols = \"class\")\n\n# again, copy together into a file\ntDF_2019 = dplyr::bind_rows(tDF_2019)\ntDF_2019$year = 2019\ntDF_2020 = dplyr::bind_rows(tDF_2020)\ntDF_2020$year = 2020\n# Delete any rows that contain NA (no data) values\ntDF_2019 = tDF_2019[complete.cases(tDF_2019) ,]\ntDF_2020 = tDF_2020[complete.cases(tDF_2020) ,]\n\ntDF= rbind(tDF_2019,tDF_2020)\n\n# check the extracted data\nsummary(tDF)\n\n# Save as R internal data format\n# is stored in the repo and can therefore be loaded (line below)\nsaveRDS(tDF, paste0(envrmt$path_data,\"tDF.rds\"))\n\n\n\n} else {\ntDF = readRDS(paste0(envrmt$path_data,\"tDF.rds\"))\n}\n\nThe result is a table with training data for 2019 and 2020. The data set contains all raster information for all bands covered by the polygons for the classes “clearcut” and “other”.\n\n## ##\nhead(tDF)"
  },
  {
    "objectID": "reader/cd2.html",
    "href": "reader/cd2.html",
    "title": "Data retrieval with R",
    "section": "",
    "text": "Access to spatial and spatio-temporal data sets, which are available in ever greater quantities and quality, is an essential basic task of any spatial or descriptive data analysis in the geosciences and related fields. An ever-growing number of websites, R packages and tutorials support this data access and thus lower a significant barrier to entry.\nBelow you will find a preliminary and unorganized list of links that provide access to packages, data and tutorials. Most of the links lead directly to the corresponding example code.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Data retrieval"
    ]
  },
  {
    "objectID": "reader/cd2.html#final-remarks",
    "href": "reader/cd2.html#final-remarks",
    "title": "Data retrieval with R",
    "section": "Final Remarks",
    "text": "Final Remarks",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Data retrieval"
    ]
  },
  {
    "objectID": "slides/slidelist.html",
    "href": "slides/slidelist.html",
    "title": "Presentations",
    "section": "",
    "text": "Sortieren nach\n      Voreinstellung\n      \n        Titel\n      \n      \n        Autor:in\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitel\n\n\n\nAutor:in\n\n\n\n\n\n\n\n\nChange detection part 1\n\n\nChris Reudenbach\n\n\n\n\n\n\nLandscape Metrics Reloaded\n\n\nChris Reudenbach\n\n\n\n\n\n\nLandscape Patterns\n\n\nChris Reudenbach\n\n\n\n\n\n\nPattern-based spatial analysis\n\n\nChris Reudenbach\n\n\n\n\n\n\nSlides and extensions\n\n\ngisma team\n\n\n\n\n\n\nSpatial Patterns\n\n\nChris Reudenbach\n\n\n\n\n\n\nSpatial patterns of non-categorical rasters\n\n\nChris Reudenbach\n\n\n\n\n\n\nKeine Treffer"
  },
  {
    "objectID": "slides/slides_session3.html#what-is-the-meaning",
    "href": "slides/slides_session3.html#what-is-the-meaning",
    "title": "Landscape Metrics Reloaded",
    "section": "What is the meaning?",
    "text": "What is the meaning?\n\n\n\nSpatial patterns can be quantified using landscape metrics (O’Neill et al. 1988; Turner and Gardner 1991; Li and Reynolds 1993; He et al. 2000; Jaeger 2000; Kot i in. 2006; McGarigal 2014).\n\n\n\nAnd well, we can calculate technically very complex metrics and indices, but what is the meaning?\n\n\n\nParametrization of two IT metrics\n\n\n\nMarginal entropy [H(x)] - diversity (composition) of spatial categories - from monothematic patterns to multithematic patterns\nRelative mutual information [U] - clumpiness (configuration) of spatial categories from fragmented patterns to consolidated patterns",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns Reloaded"
    ]
  },
  {
    "objectID": "slides/slides_session3.html#back-to-theory",
    "href": "slides/slides_session3.html#back-to-theory",
    "title": "Landscape Metrics Reloaded",
    "section": "Back to theory",
    "text": "Back to theory\nSo let’s go one step back:\n\nRead the following section of Fragstats Manual and answer the following question:\n\n\n\nWhat are the basic concept an meaning meaning of landscape metrics?\n\n\n\n\n\nForm teams of two and please try to answer the following questions:\n\n\n\n\nWhat is now the specific purpose in scientific application?\nWhat can be expressed with them at all?",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns Reloaded"
    ]
  },
  {
    "objectID": "slides/slides_session3.html#now-try-to-find-an-interpretation",
    "href": "slides/slides_session3.html#now-try-to-find-an-interpretation",
    "title": "Landscape Metrics Reloaded",
    "section": "Now try to find an interpretation",
    "text": "Now try to find an interpretation\n\n\n\nParametrization of two IT metrics\n\n\n\nMarginal entropy [H(x)] - diversity (composition) of spatial categories - from monothematic patterns to multithematic patterns\nRelative mutual information [U] - clumpiness (configuration) of spatial categories from fragmented patterns to consolidated patterns",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns Reloaded"
    ]
  },
  {
    "objectID": "slides/slides_session3.html#ercercise",
    "href": "slides/slides_session3.html#ercercise",
    "title": "Landscape Metrics Reloaded",
    "section": "Ercercise",
    "text": "Ercercise\nPlease navigate to the landscapemetrics package.\nRun the following tutorials with the example data of this course:\n\n\n\n\nSample lsm\nIrregular Areas\nUtilities\n\n\n\n\nFor the irregular vector data download the OSM data e.g. from Geofabrik and use the category landuse. Note you need to merge Niedersachsen and Sachsen-Anhalt\nAddon\n\n\n\n\nDownload the FragStats tutorial Analyzing a single grid.\nRun the tutorial once with FragStats and then use the data and indices for the same analysis with landscapemetrics instead.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns Reloaded"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#spatial-patterns",
    "href": "slides/slides_session1.html#spatial-patterns",
    "title": "Spatial Patterns",
    "section": "Spatial Patterns",
    "text": "Spatial Patterns\nDiscovering and describing spatial patterns is an important part of many geographical studies, and spatial patterns are linked to natural and social processes.\n\n\n\n\n\n\n\n\nCorona Virus Distribution, https://coronavirus.jhu.edu/map.html\n\n\n\n\n\n\n\nIPCC Interactive Atlas, Bias Adjusted TX35\n\n\n\n\n\n\nAbbildung 1: Examples for spatial patterns",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#spatial-patterns-of-categorical-raster-data",
    "href": "slides/slides_session1.html#spatial-patterns-of-categorical-raster-data",
    "title": "Spatial Patterns",
    "section": "Spatial patterns of categorical raster data",
    "text": "Spatial patterns of categorical raster data\nNumerous geographical studies are linked to all kind of classified raster based spatial data\n\n\n\n\n\nhttps://doi.org/10.1016/j.agee.2020.107052\n\n\n\n\n\n\n\n\nhttps://doi.org/10.1016/j.agee.2020.107052",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#point-patterns-versus-continous-patterns",
    "href": "slides/slides_session1.html#point-patterns-versus-continous-patterns",
    "title": "Spatial Patterns",
    "section": "Point patterns versus continous patterns",
    "text": "Point patterns versus continous patterns\n\n\n\n\n\nPoint Distributions, (https://gisgeography.com/spatial-patterns\n\n\n\n\n\n\nRaster based patterns, Source: https://www.cell.com/cell/fulltext/S0092-8674%2823%2901219-9",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#dynamics-of-spatial-patterns",
    "href": "slides/slides_session1.html#dynamics-of-spatial-patterns",
    "title": "Spatial Patterns",
    "section": "Dynamics of spatial patterns",
    "text": "Dynamics of spatial patterns\n\n\n\nDynamic evaluation of landscape transformations, https://www.landscape-online.org/index.php/lo/article/view/LO.2022.1097/LO.2022.1097",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#quantification-of-categorical-spatial-patterns",
    "href": "slides/slides_session1.html#quantification-of-categorical-spatial-patterns",
    "title": "Spatial Patterns",
    "section": "Quantification of categorical spatial patterns",
    "text": "Quantification of categorical spatial patterns\nSpatial patterns can be quantified using landscape metrics (O’Neill et al. 1988; Turner and Gardner 1991; Li and Reynolds 1993; He et al. 2000; Jaeger 2000; Kot i in. 2006; McGarigal 2014).\nSoftware such as FRAGSTATS, GuidosToolbox, or landscapemetrics has proven useful in many scientific studies (&gt; 12,000 citations).\n\nThere is a relationship between an area’s pattern composition and configuration and ecosystem characteristics, such as vegetation diversity, animal distributions, and water quality within this area (Hunsaker i Levine, 1995; Fahrig i Nuttle, 2005; Klingbeil i Willig, 2009; Holzschuh et al., 2010; Fahrig et al., 2011; Carrara et al., 2015; Arroyo-Rodŕıguez et al. 2016; Duflot et al., 2017, many others..)",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#example-data",
    "href": "slides/slides_session1.html#example-data",
    "title": "Spatial Patterns",
    "section": "Example data",
    "text": "Example data\n\nLand cover data for the year 2016 from the CCI-LC project\nSimplified into nine main categories\nPartitioned into 30 x 30 kilometers square blocks\n13,909 categorical rasters (100x100 cells) https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2020JD033031\n\n  —",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#example-data-1",
    "href": "slides/slides_session1.html#example-data-1",
    "title": "Spatial Patterns",
    "section": "Example Data",
    "text": "Example Data\nRandomely selected 16 rasters with different proportions of forest (green) areas:",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#landscape-metrics",
    "href": "slides/slides_session1.html#landscape-metrics",
    "title": "Spatial Patterns",
    "section": "Landscape Metrics",
    "text": "Landscape Metrics\n\nIn the last 40 or so years, several hundred different landscape metrics were developed\nThey quantify the composition and configuration of spatial patterns of categorical rasters\nGeneral assumption is that the spatial pattern of a landscape influences the processes that occur within it\nLandscape metrics can be calculated for three different levels: patch, class, and landscape (here we focus on the landscape level)\nThey can be divided into several groups: (1) area and edge metrics, (2) shape metrics, (3) core metrics, (4) aggregation metrics, (5) diversity metrics, (6) complexity metrics",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#landscape-metrics-1",
    "href": "slides/slides_session1.html#landscape-metrics-1",
    "title": "Spatial Patterns",
    "section": "Landscape Metrics",
    "text": "Landscape Metrics\nImportant considerations:\n\nScale: the size of the area over which the metrics are calculated\nExtent: the borders of the study area\nSpatial resolution: the size of the raster cells\nCategorization: the categories used in the analysis\nRedundancy: many metrics are highly correlated",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#landscape-metrics-2",
    "href": "slides/slides_session1.html#landscape-metrics-2",
    "title": "Spatial Patterns",
    "section": "Landscape Metrics",
    "text": "Landscape Metrics\nSHDI - Shannon’s diversity index - takes both the number of classes and the abundance of each class into account; larger values indicate higher diversity\n\nAI - Aggregation index - from 0 for maximally disaggregated to 100 for maximally aggregated classes",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#code-examples",
    "href": "slides/slides_session1.html#code-examples",
    "title": "Spatial Patterns",
    "section": "Code examples",
    "text": "Code examples\nTaken from : “The landscapemetrics and motif packages for measuring landscape patterns and processes”\nRead and visualize the data\nlibrary(landscapemetrics)\nlibrary(terra)\nr9 = rast(\"exdata/r9.tif\")\nr1 = rast(\"exdata/r1.tif\")\nplot(r1); plot(r9)",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#get-the-indices",
    "href": "slides/slides_session1.html#get-the-indices",
    "title": "Spatial Patterns",
    "section": "Get the indices",
    "text": "Get the indices\nlsm_l_shdi(r9)\n\n# A tibble: 1 × 6\n  layer level     class    id metric value\n  &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1 landscape    NA    NA shdi    1.06\n\nlsm_l_ai(r9)\n\n# A tibble: 1 × 6\n  layer level     class    id metric value\n  &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1 landscape    NA    NA ai      82.1\n\ncalculate_lsm(r9, what = c(\"lsm_l_shdi\", \"lsm_l_ai\"))\n\n# A tibble: 2 × 6\n  layer level     class    id metric value\n  &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1 landscape    NA    NA ai     82.1 \n2     1 landscape    NA    NA shdi    1.06\n\ntwo_r = list(r1, r9)\ncalculate_lsm(two_r, what = c(\"lsm_l_shdi\", \"lsm_l_ai\"))\n\n# A tibble: 4 × 6\n  layer level     class    id metric   value\n  &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n1     1 landscape    NA    NA ai     98.7   \n2     1 landscape    NA    NA shdi    0.0811\n3     2 landscape    NA    NA ai     82.1   \n4     2 landscape    NA    NA shdi    1.06",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#code-examples-1",
    "href": "slides/slides_session1.html#code-examples-1",
    "title": "Spatial Patterns",
    "section": "Code examples",
    "text": "Code examples\nmat_window = matrix(1, nrow = 11, ncol = 11)\nw_result = window_lsm(r9, window = mat_window, what = \"lsm_l_ai\")\nplot(r9); plot(w_result$layer_1$lsm_l_ai)",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#package-landscapemetrics",
    "href": "slides/slides_session1.html#package-landscapemetrics",
    "title": "Spatial Patterns",
    "section": "Package landscapemetrics",
    "text": "Package landscapemetrics\nhttps://r-spatialecology.github.io/landscapemetrics/\n# A tibble: 133 × 5\n   metric name                                type           level function_name\n   &lt;chr&gt;  &lt;chr&gt;                               &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;        \n 1 area   patch area                          area and edge… patch lsm_p_area   \n 2 cai    core area index                     core area met… patch lsm_p_cai    \n 3 circle related circumscribing circle       shape metric   patch lsm_p_circle \n 4 contig contiguity index                    shape metric   patch lsm_p_contig \n 5 core   core area                           core area met… patch lsm_p_core   \n# ℹ 134 more rows\n\nSampling around points of interest\nMoving window calculations\nCalculating landscape metrics for irregular areas\nVisualizations\nMore…",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session1.html#exercises",
    "href": "slides/slides_session1.html#exercises",
    "title": "Spatial Patterns",
    "section": "Exercises",
    "text": "Exercises\n\nRead the data from exdata/lc_small.tif and visualize it. What is the location of the data? What are the extent of the data and its spatial resolution? How many categories it contains?\nCalculate Aggregation Index (AI) for the raster. Interpret the results.\nCalculate Total Edge (TE) for the raster. Interpret the results. Next, read the data from `exdata/lc_small2.tif, calculate AI and TE for this raster, and compare the results with the previous raster.\nCalculate Total Edge (TE) for the raster, but this time in a moving window of 9 by 9 cells. Visualize the results.\n(Extra) Using the read_sf() function from the sf package, read the exdata/points.gpkg file. Next, calculate SHDI and AI of an area of 3000 meters from each sampling point (see the sample_lsm() function).",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial Pattern Description"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#climate-data---best-practicse",
    "href": "slides/slides_session5.html#climate-data---best-practicse",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "Climate data - best practicse",
    "text": "Climate data - best practicse\nFirst we need to setup our environment.\n\ninstall.packages(\"geodata\")\ndevtools::install_github(\"Nowosad/spquery\")\ndevtools::install_github(\"Nowosad/patternogram\")\ndevtools::install_github(\"Nowosad/supercells\")\nlibrary(terra); library(sf);library(geodata);library(mapview);library(spquery);library(patternogram)\n\nUsing the geodata package (helper package for terra) we can download in a very comfortable often used core data sets. In this case Worldclim and\n\n# German border on top level (nation)\nde = geodata::gadm(country = \"DEU\",level = 0,path = tempdir())\n# Worldclim historical climate data for Germany https://www.worldclim.org/data/index.html\nwc_tmin_de = worldclim_country(\"Germany\", var=\"tmin\", path=tempdir())\n# WCRP Coupled Model Intercomparison Project data https://www.wcrp-climate.org/wgcm-cmip\ncmip_tmin_2_5  = geodata::cmip6_world(\"CNRM-CM6-1\", \"585\", \"2061-2080\", var=\"tmin\", res=10, path=tempdir())\n# cropping and masking\nwc_tmin_de = crop(wc_tmin_de,de,mask=T)\ncmip_tmin_2_5_de = crop(cmip_tmin_2_5 , de,mask=T)\n# set plotting layout\nnf &lt;- layout( matrix(c(1,2), ncol=2) )\n\nplot(cmip_tmin_2_5_de)\nplot(wc_tmin_de)\n\n\n\n\nNote: Please check the websites for the meaning of the data",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#minimum-temperature-c",
    "href": "slides/slides_session5.html#minimum-temperature-c",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "Minimum temperature (°C)",
    "text": "Minimum temperature (°C)\n\n\n\n\n\nCMIP6 downscaled future climate projection for 2061-2080 [model: CNRM-ESM2-1; ssp: “585”]\n\n\n\n\n\n\nWorldClim version 2.1 climate data for 1970-2000",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#identifying-and-comparing-similar-spatial-patterns",
    "href": "slides/slides_session5.html#identifying-and-comparing-similar-spatial-patterns",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "Identifying and comparing similar spatial patterns",
    "text": "Identifying and comparing similar spatial patterns\nUsing the package spquery for finding similarities in continous raster data (e.g., raster time-series)\n\n\n\n# get Marburg and make it a spatial point\nmr = st_sf(st_sfc(st_point(c( 8.770833, 50.810001)),\n                  crs = \"EPSG:4326\"))\nvec = as.numeric(extract(wc_tmin_de, mr, ID = FALSE))\nvec\n# search for similarity in the data set\nsearch_tmin = spq_search(vec, cmip_tmin_2_5_de,\n                         dist_fun = \"euclidean\")\n\n# visualize\nplot(search_tmin,\n     plg = list( loc = \"topright\",title = \"Dissimilarity\"))\nplot(mr, add=TRUE, col='red', lwd=4)\n\n\n\n\nComparison historical Marburg vs CMIP\n\n\n\n\n# some cleaning\ncrop= crop( wc_tmin_de,cmip_tmin_2_5_de)\nres = resample(crop,cmip_tmin_2_5_de)\n\n# call comparison\ncompare_tmin = spq_compare(cmip_tmin_2_5_de, res,\n                           dist_fun = \"euclidean\")\n\n# visualize\nplot(compare_tmin,\n     plg = list( loc = \"topright\",title = \"Dissimilarity\"), \n     col =viridis(256))\nplot(mr, add=TRUE, col='red', lwd=4)\n\n\n\n\nComparison Worldclim vs CMIP",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#patternogram",
    "href": "slides/slides_session5.html#patternogram",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "patternogram",
    "text": "patternogram\nDescribing the range of spatial autocorrelation\n\nexplore spatial autocorrelations of predictors in machine learning models\ndetect spatial autocorrelation in various data structures\ncompare the spatial autocorrelation of variables over time\ninvestigate spatial autocorrelation of categorical spatial patterns\n\n\nlibrary(patternogram)\nlibrary(ggplot2)\npg1_100 = patternogram(wc_tmin_de, sample_size = 10)\npg2_100 = patternogram(cmip_tmin_2_5_de, sample_size = 10)\npg1_500 = patternogram(wc_tmin_de, sample_size = 100)\npg2_500 = patternogram(cmip_tmin_2_5_de, sample_size = 100)\npg1_1000 = patternogram(wc_tmin_de, sample_size = 1000)\npg2_1000 = patternogram(cmip_tmin_2_5_de, sample_size = 1000)\nggplot() + geom_point(data=pg1_100, aes(x=dist, y=dissimilarity), color='red4') +\n           geom_point(data=pg2_100, aes(x=dist, y=dissimilarity), color='red') +\n           geom_point(data=pg1_500, aes(x=dist, y=dissimilarity), color='blue') +\n           geom_point(data=pg2_500, aes(x=dist, y=dissimilarity), color='lightblue') +\n           geom_point(data=pg1_1000, aes(x=dist, y=dissimilarity), color='green') +\n           geom_point(data=pg2_1000, aes(x=dist, y=dissimilarity), color='darkgreen')            \n\n\nlibrary(patternogram)\nlibrary(ggplot2)\npg1_100 = patternogram(wc_tmin_de, width = 100, sample_size = 10)\npg2_100 = patternogram(cmip_tmin_2_5_de, width = 100 ,sample_size = 10)\npg1_500 = patternogram(wc_tmin_de,  width = 100,sample_size = 10)\npg2_500 = patternogram(cmip_tmin_2_5_de,  width = 100,sample_size = 10)\npg1_1000 = patternogram(wc_tmin_de,  width = 100,sample_size = 10)\npg2_1000 = patternogram(cmip_tmin_2_5_de, width = 100,sample_size = 10)\nggplot() + geom_point(data=pg1_100, aes(x=dist, y=dissimilarity), color='red4') +\n           geom_point(data=pg2_100, aes(x=dist, y=dissimilarity), color='red') +\n           geom_point(data=pg1_500, aes(x=dist, y=dissimilarity), color='blue') +\n           geom_point(data=pg2_500, aes(x=dist, y=dissimilarity), color='lightblue') +\n           geom_point(data=pg1_1000, aes(x=dist, y=dissimilarity), color='green') +\n           geom_point(data=pg2_1000, aes(x=dist, y=dissimilarity), color='darkgreen')",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#slic-supercells",
    "href": "slides/slides_session5.html#slic-supercells",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "Slic /supercells",
    "text": "Slic /supercells\nsupercells: an extension of SLIC (Simple Linear Iterative Clustering; Achanta et al. (2012), doi:10.1109/TPAMI.2012.120) that can be applied to non-imagery geospatial rasters that carry:\n\npattern information (co-occurrence matrices)\ncompositional information (histograms)\ntime-series information (ordered sequences)\nother forms of information for which the use of Euclidean distance may not be justified\nSegmentation/regionalization: partitioning space into smaller segments while minimizing internal inhomogeneity and maximizing external isolation\n\n\n\n\nSLIC/supercells are a way to improve the output and reduce the cost of segmentation",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#code-examples",
    "href": "slides/slides_session5.html#code-examples",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "Code examples",
    "text": "Code examples\n\nlibrary(terra)\nlibrary(supercells)\n# Version 1\nmintemp_zones = supercells(cmip_tmin_2_5_de, k = 250, compactness = 4)\nplot(cmip_tmin_2_5_de[[1]]); plot(mintemp_zones, add = TRUE, col = NA)\n# Version 2\nmintemp_zones = supercells(cmip_tmin_2_5_de, k = 50, compactness = 4)\nplot(cmip_tmin_2_5_de[[1]]); plot(mintemp_zones, add = TRUE, col = NA)\n# Version 3\nmintemp_zones = supercells(cmip_tmin_2_5_de, k = 250, compactness = 1)\nplot(cmip_tmin_2_5_de[[1]]); plot(mintemp_zones, add = TRUE, col = NA)\n\n\n\n\n\n\n\n\n\nk = 250, compactness = 4\n\n\n\n\n\n\n\nk = 50, compactness = 4\n\n\n\n\n\n\n\nk = 250, compactness = 1\n\n\n\n\n\n\nAbbildung 2",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#code-examples---regionalisation-based-on-supercells",
    "href": "slides/slides_session5.html#code-examples---regionalisation-based-on-supercells",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "Code Examples - Regionalisation based on supercells",
    "text": "Code Examples - Regionalisation based on supercells\n\n# create supercells based on the 2D time-series ---------------------------\nsp = supercells(c(ta, pr), step = 15, compactness = 0.01, dist_fun = dtw_2d)\nplot(sp)\n\n# create 3, 7, 11, and 15 regions based on the 2D time-series -------------\nsp_regions = ?map_dfr(c(3, 7, 11, 15), regionalize_dtw_2d, sp)\n\ntm_shape(sp_regions) +\n  tm_polygons() +\n  tm_facets(\"k\")",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session5.html#exercises",
    "href": "slides/slides_session5.html#exercises",
    "title": "Spatial patterns of non-categorical rasters",
    "section": "Exercises",
    "text": "Exercises\nThe goal: to regionalize Germany’s climates\n\nUse Worldclim versus CMIP data\nUse the upper helper functions and code for Great Britain\n\nExtended SLIC workflow uses the dynamic time warping (DTW) distance function rather than the Euclidean distance.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Continous data spatial analysis"
    ]
  },
  {
    "objectID": "base/impressum.html#content-responsibility",
    "href": "base/impressum.html#content-responsibility",
    "title": "Impressum",
    "section": "Content Responsibility",
    "text": "Content Responsibility\nThe responsibility for the content rests with the instructors. Statements, opinions and/or conclusions are the ones from the instructors and do not necessarily reflect the opinion of the representatives of Marburg University."
  },
  {
    "objectID": "base/impressum.html#content-license",
    "href": "base/impressum.html#content-license",
    "title": "Impressum",
    "section": "Content License",
    "text": "Content License\n   This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\nPrivacy Policy\n\n\nAs of 21. October 2021\n\n\nIntroduction\n\n\nWith the following data protection declaration, we would like to inform you about the types of your personal data (hereinafter also referred to as “data” for short) that we process, for what purposes and to what extent. The privacy policy applies to all processing of personal data carried out by us, both in the context of the provision of our services and in particular on our websites, in mobile applications and within external online presences, such as our social media profiles (hereinafter collectively referred to as “Online Offerings”).\n\n\nThe terms used are not gender-specific.\n\n\nResponsible\n\n\nDr. Christoph Reudenbach Deutschhaustr. 10  35047 Marburg\n\n\nEmail address: reudenbach@uni-marburg.de.\n\n\nImprint: https://www.uni-marburg.de/en/legal.\n\n\nOverview of Processing\n\n\nThe following overview summarizes the types of data processed and the purposes of their processing, and refers to the data subjects.\n\n\nTypes of Data Processed\n\n\n\nContent data (e.g. input in online forms).\n\n\nContact data (e.g. email, phone numbers).\n\n\nMeta/communication data (e.g. device information, IP addresses).\n\n\nUse data (e.g. websites visited, interest in content, access times).\n\n\n\nCategories of data subjects\n\n\n\nCommunication partners.\n\n\nUsers (e.g., website visitors, users of online services).\n\n\n\nPurposes of processing\n\n\n\nDirect marketing (e.g., by email or postal mail).\n\n\nContact requests and communications.\n\n\n\nRelevant legal basis\n\n\nThe following is an overview of the legal basis of the GDPR on the basis of which we process personal data. Please note that in addition to the provisions of the GDPR, national data protection regulations may apply in your or our country of residence or domicile. Furthermore, should more specific legal bases be decisive in individual cases, we will inform you of these in the data protection declaration.\n\n\n\nConsent (Art. 6 para. 1 p. 1 lit. a. DSGVO) – The data subject has given his or her consent to the processing of personal data concerning him or her for a specific purpose or purposes.\n\n\nLegitimate interests (Art. 6 para. 1 p. 1 lit. f. DSGVO) – Processing is necessary to protect the legitimate interests of the controller or a third party, unless such interests are overridden by the interests or fundamental rights and freedoms of the data subject which require the protection of personal data.\n\n\n\nNational data protection regulations in Germany: In addition to the data protection regulations of the General Data Protection Regulation, national regulations on data protection apply in Germany. These include, in particular, the Act on Protection against Misuse of Personal Data in Data Processing (Federal Data Protection Act – BDSG). In particular, the BDSG contains special regulations on the right to information, the right to erasure, the right to object, the processing of special categories of personal data, processing for other purposes and transmission, as well as automated decision-making in individual cases, including profiling. Furthermore, it regulates data processing for employment purposes (Section 26 BDSG), in particular with regard to the establishment, implementation or termination of employment relationships as well as the consent of employees. Furthermore, state data protection laws of the individual federal states may apply.\n\n\nSecurity measures\n\n\nWe take appropriate technical and organizational measures in accordance with the legal requirements, taking into account the state of the art, the implementation costs and the nature, scope, circumstances and purposes of the processing, as well as the different probabilities of occurrence and the extent of the threat to the rights and freedoms of natural persons, in order to ensure a level of protection appropriate to the risk.\n\n\nMeasures include, in particular, ensuring the confidentiality, integrity, and availability of data by controlling physical and electronic access to data as well as access to, entry into, disclosure of, assurance of availability of, and segregation of data concerning them. Furthermore, we have established procedures to ensure the exercise of data subjects’ rights, the deletion of data, and responses to data compromise. Furthermore, we take the protection of personal data into account as early as the development or selection of hardware, software as well as procedures in accordance with the principle of data protection, through technology design and through data protection-friendly default settings.\n\n\nDeletion of data\n\n\nThe data processed by us will be deleted in accordance with legal requirements as soon as their consents permitted for processing are revoked or other permissions cease to apply (e.g., if the purpose of processing this data has ceased to apply or it is not necessary for the purpose).\n\n\nIf the data are not deleted because they are required for other and legally permissible purposes, their processing will be limited to these purposes. That is, the data will be blocked and not processed for other purposes. This applies, for example, to data that must be retained for reasons of commercial or tax law or whose storage is necessary for the assertion, exercise or defense of legal claims or for the protection of the rights of another natural person or legal entity.\n\n\nOur privacy notices may also include further information on the retention and deletion of data that takes precedence for the processing operations in question.\n\n\nUse of cookies\n\n\nCookies are text files that contain data from websites or domains visited and are stored by a browser on the user’s computer. The primary purpose of a cookie is to store information about a user during or after their visit within an online site. Stored information may include, for example, language settings on a website, login status, a shopping cart, or where a video was watched. We further include in the term cookies other technologies that perform the same functions as cookies (e.g., when user details are stored using pseudonymous online identifiers, also referred to as “user IDs”).\n\n\nThe following cookie types and functions are distinguished:\n\n\n\nTemporary cookies (also: session or session cookies): Temporary cookies are deleted at the latest after a user has left an online offer and closed their browser.\n\n\nPermanent cookies: Permanent cookies remain stored even after closing the browser. For example, the login status can be saved or preferred content can be displayed directly when the user revisits a website. Likewise, the interests of users used for range measurement or marketing purposes can be stored in such a cookie.\n\n\nFirst-party cookies: First-party cookies are set by ourselves.\n\n\nThird-party cookies: Third-party cookies are mainly used by advertisers (so-called third parties) to process user information.\n\n\nNecessary cookies: Cookies may be absolutely necessary for the operation of a website (e.g., to store logins or other user input or for security reasons).\n\n\nStatistics, marketing and personalization cookies: Furthermore, cookies are usually also used in the context of range measurement and when the interests of a user or their behavior (e.g., viewing certain content, use of functions, etc.) on individual web pages are stored in a user profile (also “tracking”).\n\n\n\nNotes on legal bases: On which legal basis we process your personal data using cookies depends on whether we ask you for consent. If this is the case and you consent to the use of cookies, the legal basis for the processing of your data is the declared consent. Otherwise, the data processed with the help of cookies is processed on the basis of our legitimate interests (e.g., in a business operation of our online offer and its improvement) or, if the use of cookies is necessary to fulfill our contractual obligations.\n\n\nDuration of storage: If we do not provide you with explicit information about the storage period of permanent cookies (e.g., in the context of a so-called cookie opt-in), please assume that the storage period can be up to two years.\n\n\nGeneral information on revocation and objection (opt-out): Depending on whether the processing is based on consent or legal permission, you have the option at any time to revoke any consent given or to object to the processing of your data by cookie technologies (collectively referred to as “opt-out”). You can initially declare your objection by means of your browser settings, e.g., by deactivating the use of cookies (whereby this may also restrict the functionality of our online offer). An objection to the use of cookies for online marketing purposes can also be declared by means of a variety of services, especially in the case of tracking, via the websites https://optout.aboutads.info and https://www.youronlinechoices.com/. In addition, you can receive further objection notices in the context of the information on the service providers and cookies used.\n\n\nProcessing of cookie data on the basis of consent: We use a cookie consent management procedure, in the context of which the consent of users to the use of cookies, or the processing and providers mentioned in the cookie consent management procedure can be obtained, managed and revoked by users. Here, the declaration of consent is stored in order not to have to repeat its query and to be able to prove the consent in accordance with the legal obligation. The storage can take place on the server side and/or in a cookie (so-called opt-in cookie, or with the help of comparable technologies), in order to be able to assign the consent to a user or their device. Subject to individual information on the providers of cookie management services, the following information applies: The duration of the storage of consent can be up to two years. Here, a pseudonymous user identifier is formed and stored with the time of consent, information on the scope of consent (e.g., which categories of cookies and/or service providers) as well as the browser, system and end device used.\n\n\n\nTypes of data processed: Usage data (e.g., websites visited, interest in content, access times), meta/communication data (e.g., device information, IP addresses).\n\n\nPersons concerned: Users (e.g., website visitors, users of online services).\n\n\nLegal basis: Consent (Art. 6 para. 1 p. 1 lit. a. DSGVO), Legitimate Interests (Art. 6 para. 1 p. 1 lit. f. DSGVO).\n\n\n\nSurveys and polls\n\n\nThe surveys and polls (hereinafter “surveys”) conducted by us are evaluated anonymously. Personal data is only processed insofar as this is necessary for the provision and technical implementation of the surveys (e.g., processing of the IP address to display the survey in the user’s browser or to enable a resumption of the survey with the help of a temporary cookie (session cookie)) or users have consented.\n\n\nNotes on legal basis: If we ask participants for consent to process their data, this is the legal basis of the processing; otherwise the processing of participants’ data is based on our legitimate interests in conducting an objective survey.\n\n\n\nTypes of data processed: Contact data (e.g., email, phone numbers), content data (e.g., input in online forms), usage data (e.g., web pages visited, interest in content, access times), meta/communication data (e.g., device information, IP addresses).\n\n\nParticipants concerned: Communication partners.\n\n\nPurposes of processing: Contact requests and communication, direct marketing (e.g., by e-mail or postal mail).\n\n\nLegal basis: Consent (Art. 6 para. 1 p. 1 lit. a. DSGVO), Legitimate Interests (Art. 6 para. 1 p. 1 lit. f. DSGVO).\n\n\n\nChange and Update Privacy Policy\n\n\nWe encourage you to periodically review the contents of our Privacy Policy. We adapt the Privacy Policy as soon as the changes in the data processing activities we carry out make it necessary. We will inform you as soon as the changes require an act of cooperation on your part (e.g., consent) or other individual notification.\n\n\nWhere we provide addresses and contact information for companies and organizations in this Privacy Policy, please note that addresses may change over time and please check the information before contacting us.\n\n\nRights of data subjects\n\n\nAs a data subject, you are entitled to various rights under the GDPR, which arise in particular from Art. 15 to 21 DSGVO:\n\n\n\nRight to object: You have the right to object at any time, on grounds relating to your particular situation, to the processing of personal data relating to you which is carried out on the basis of Art. 6(1)(e) or (f) DSGVO; this also applies to profiling based on these provisions. If the personal data concerning you is processed for the purpose of direct marketing, you have the right to object at any time to the processing of personal data concerning you for the purpose of such marketing; this also applies to profiling, insofar as it is associated with such direct marketing.\n\n\nRight of withdrawal in the case of consent: You have the right to withdraw any consent you have given at any time.\n\n\nRight of access: You have the right to request confirmation as to whether data in question is being processed and to information about this data, as well as further information and copy of the data in accordance with the legal requirements.\n\n\nRight of rectification: You have the right, in accordance with the legal requirements, to request the completion of the data concerning you or the correction of incorrect data concerning you.\n\n\nRight to erasure and restriction of processing: You have, in accordance with the law, the right to request that data concerning you be erased without undue delay, or alternatively, in accordance with the law, to request restriction of the processing of the data.\n\n\nRight to data portability: You have the right to receive data concerning you, which you have provided to us, in a structured, common and machine-readable format in accordance with the legal requirements, or to demand its transfer to another responsible party.\n\n\nComplaint to supervisory authority: Without prejudice to any other administrative or judicial remedy, you have the right to lodge a complaint with a supervisory authority, in particular in the Member State of your habitual residence, place of work or the place of the alleged infringement, if you consider that the processing of personal data concerning you infringes the requirements of the GDPR.\n\n\n\nCreated with free Datenschutz-Generator.de by Dr. Thomas Schwenke"
  },
  {
    "objectID": "base/impressum.html#comments-suggestions",
    "href": "base/impressum.html#comments-suggestions",
    "title": "Impressum",
    "section": "Comments & Suggestions",
    "text": "Comments & Suggestions"
  },
  {
    "objectID": "base/about.html",
    "href": "base/about.html",
    "title": "About",
    "section": "",
    "text": "Organisation: gisma spatial science ressources Kurs: LV-19-d19-006-25 Kontakt: reudenbach@uni-marburg.de"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intended learning outcomes",
    "section": "",
    "text": "One could claim that the fact living on the surface of the earth and only get to know a small space through direct personal experience is the most important motivation for most of the geographic work. Compensation for this lack of direct experience has been and is being made, especially in scientific geography, with the help of efficient spatio-temporal techniques of abstraction.\nKnowledge of spatial and/or temporal aspects of our environment is increasingly in demand for action-relevant relationships. Whether we ask as tourists, consumers, producers or planners spatial information, or even knowledge.\nGeographic Information Science (GIS) is based on versatile and powerful software tools that are used in modeling, analysis, data mining merging and numerous other spatio-temporal applications. Nevertheless the most powerful tool is our mind developing the concepts and developing the necessary algorithms.\n\nIntended learning outcomes\nAt the end of this course you should be able\n\nUnderstand, adapt and develop geoinformatics methods\nDesign workflows that are suitable for solving spatio-temporal data-related regionalization issues\nCritically evaluate their spatio-temporal analysis\nCommunicate their workflows and analysis results\n\n\n\nCourse features\nThe course is intended as a blended learning module in our study program although the provided introductions, explanations and examples might be useful for self-study, too.\n\n\nDeliverables\nThe graded exam is based on a scientific presentation of 10 + 10 minutes (discussion) on a topic of your choice, applying the main techniques taught in the course. The topic should be motivated by personal interest and based on available data sets/your own classifications. Motivation, research questions and selected results as well as a critical discussion should be part of the presentation. The presentation as well as the scripts and sources used are to be published as a team portfolio (2 persons) on Github. One week before the presentation, an abstract of no more than one page is to be submitted.\n\n\n\n\n\n\nHinweis\n\n\n\nThe presentation will take place in the last course session of the semester.\n\n\n\n\nPreparation and prerequisites\nThe courses assumes basic knowledge and skills in remote sensing and GIS."
  },
  {
    "objectID": "base/faq.html",
    "href": "base/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Wo ändere ich Branding?\n\n\n\ncss/custom.scss(Light) und optionalcss/theme-dark.scss` (Dark).\n\n\n\n\n\n\n\n\nWie bearbeite ich Inhalte?\n\n\n\nSeiten sind .qmd-Dateien. Änderungen committen und pushen; GitHub Actions baut die Site.\n\n\n\n\n\n\n\n\nLearn More…\n\n\n\n\n\nDiese Seite nutzt Quarto. Änderbare Farben/Branding liegen in css/custom.scss. Alles weitere auf der Quarto Seite"
  },
  {
    "objectID": "slides/slides_session2.html#landscape-metrics",
    "href": "slides/slides_session2.html#landscape-metrics",
    "title": "Landscape Patterns",
    "section": "Landscape Metrics",
    "text": "Landscape Metrics\n\nProblem no. 1: which of the hundreds of spatial metrics should we choose?\nProblem no. 2: many landscape metrics are highly correlated…",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#possible-approach---pca-of-type-samples-of-landscape-metrics",
    "href": "slides/slides_session2.html#possible-approach---pca-of-type-samples-of-landscape-metrics",
    "title": "Landscape Patterns",
    "section": "Possible Approach - PCA of type samples of landscape metrics",
    "text": "Possible Approach - PCA of type samples of landscape metrics\nWe performed a principal component analysis (PCA) using 17 landscape-level metrics:",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#pca-of-landscape-metrics",
    "href": "slides/slides_session2.html#pca-of-landscape-metrics",
    "title": "Landscape Patterns",
    "section": "PCA of landscape metrics",
    "text": "PCA of landscape metrics\n\n\n\n\n\nPCA 1\n\n\n\n\n\n\nPCA 2\n\n\n\n\nFirst two principal components explained ~71% of variability",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#pca-of-landscapes",
    "href": "slides/slides_session2.html#pca-of-landscapes",
    "title": "Landscape Patterns",
    "section": "PCA of landscapes",
    "text": "PCA of landscapes\nThe result allows to distinguish between:\n\nsimple and complex rasters (left&lt;-&gt;right)\nagmented and consolidated rasters (bottom&lt;-&gt;top)\n\n\n\n\nPCA 2\n\n\nHowever, there are still some problems here…",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#pca-of-landscapes-1",
    "href": "slides/slides_session2.html#pca-of-landscapes-1",
    "title": "Landscape Patterns",
    "section": "PCA of landscapes",
    "text": "PCA of landscapes\n\nWe performed a second PCA using data from the United Kingdom only\nNext, we predicted the results on the data for the whole Europe\n\n\n\n\n\n\nPCA 1\n\n\n\n\n\n\nPCA 2",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#pca-of-landscapes-2",
    "href": "slides/slides_session2.html#pca-of-landscapes-2",
    "title": "Landscape Patterns",
    "section": "PCA of landscapes",
    "text": "PCA of landscapes\n\n\n\nPCA 1 PCA 2 UK EU\n\n\nIssues with the PCA approach:\n\nEach new dataset requires recalculation of both, landscape metrics and principal components analysis (PCA)\nHighly correlated landscape metrics are used\nPCA results interpretation is not straightforward",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#it-metrics",
    "href": "slides/slides_session2.html#it-metrics",
    "title": "Landscape Patterns",
    "section": "IT metrics",
    "text": "IT metrics\n\nFive information theory metrics based on a co-occurrence matrix exist (Nowosad and Stepinski, 2019, https://doi.org/10.1007/s10980-019-00830-x)\nMarginal entropy [H(x)] - diversity (composition) of spatial categories - from monothematic patterns to multithematic patterns\nRelative mutual information [U] - clumpiness (configuration) of spatial categories from fragmented patterns to consolidated patterns)\nH(x) and U are uncorrelated\n\n\n\n\n\n\nEntropy\n\n\n\n\n\n\nRelative mutual information",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#it-metrics-1",
    "href": "slides/slides_session2.html#it-metrics-1",
    "title": "Landscape Patterns",
    "section": "IT metrics",
    "text": "IT metrics\n2D parametrization of categorical rasters’ configurations based on two weakly correlated IT metrics groups similar patterns into distinct regions of the parameters space",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#it-metrics-final-results",
    "href": "slides/slides_session2.html#it-metrics-final-results",
    "title": "Landscape Patterns",
    "section": "IT metrics final results",
    "text": "IT metrics final results\n\n\n\n\n\nLand cover data\n\n\n\n\n\n\nParametrization of two IT metrics",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#exercises",
    "href": "slides/slides_session2.html#exercises",
    "title": "Landscape Patterns",
    "section": "Exercises",
    "text": "Exercises\n\nThe marginal entropy and relative mutual information can be calculated using the landscapemetrics package’s functions: lsm_l_ent() and lsm_l_relmutinf(). Calculate both of these metrics for the exdata/lc_small.tif raster.\nRead the exdata/lc_europe.tif raster using rast() from the **terra** package and theexdata/polygons.gpkgvector data using theread_sf()function from the **sf** package. Calculate the marginal entropy and relative mutual information for each polygon using thesample_lsm()` function.\nJoin the calculated values with the polygons (see https://r-spatialecology.github.io/landscapemetrics/articles/irregular_areas.html for more details).\nCalculate SHDI and AI for the polygons. Compare the values of SHDI and AI with the marginal entropy and relative mutual information (e.g., using a scatterplot or by calculating the correlation coefficient). Are the results similar?\n(Extra) Create your own polygonal grid using st_make_grid() function from the sf package for the area from the exdata/polygons.gpkg file. Calculate the marginal entropy and relative mutual information for each square using the sample_lsm() function. Visualize the results.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session2.html#it-metrics-2",
    "href": "slides/slides_session2.html#it-metrics-2",
    "title": "Landscape Patterns",
    "section": "IT metrics",
    "text": "IT metrics\nThese metrics still leave some questions open…\n\nRelative mutual information is a result of dividing mutual information by entropy. What to do when the entropy is zero?\nHow to incorporate the meaning of categories into the analysis?\n\n\n\n\nParametrization of two IT metrics",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Landscape Patterns"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#spatial-patterns",
    "href": "slides/slides_session4.html#spatial-patterns",
    "title": "Pattern-based spatial analysis",
    "section": "Spatial Patterns",
    "text": "Spatial Patterns\nIn recent years, the ideas of analyzing spatial patterns have been extended through an approach called pattern-based spatial analysis (Long in in. 2010; Cardille in in. 2010; Cardille in in. 2012; Jasiewicz i in. 2013; Jasiewicz i in. 2015).\nThe fundamental idea is to divide a big area into a large number of smaller areas which we may call local landscapes patches.\n\n\n\nNote: The patch size is depending on spatial data resolution and scale of the landscape\n\n\n\n\n\n\n\nThe idea is to represent each of this arbitrary areas using a statistical description of the spatial pattern - a spatial signature.\n\nThis spatial signatures can be compared using a large number of existing distance or dissimilarity measures (Lin 1991; Cha 2007), which enables spatial analyses such as searching, change detection, clustering, or segmentation.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#spatial-signatures",
    "href": "slides/slides_session4.html#spatial-signatures",
    "title": "Pattern-based spatial analysis",
    "section": "Spatial Signatures",
    "text": "Spatial Signatures\nMost landscape metrics are single numbers representing specific features of a local landscape.\nSpatial signatures, on the other hand, are multi-element representations of landscape composition and configuration.\n\n\n\n\nThe basic signature is the co-occurrence matrix:",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#spatial-signatures-dimension-reduction-normalisation",
    "href": "slides/slides_session4.html#spatial-signatures-dimension-reduction-normalisation",
    "title": "Pattern-based spatial analysis",
    "section": "Spatial signatures dimension reduction & normalisation",
    "text": "Spatial signatures dimension reduction & normalisation\n\n\n\n\n\n\n\n\nReduced co-occurrence vector (cove)\n\n\n\n\n\n\n\nNormalized co-occurrence vector(cove)\n\n\n\n\n\n\n\nCo-occurrence vector (cove)",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#dissimilarity-measures-example-1",
    "href": "slides/slides_session4.html#dissimilarity-measures-example-1",
    "title": "Pattern-based spatial analysis",
    "section": "Dissimilarity measures Example 1",
    "text": "Dissimilarity measures Example 1\nMeasuring the distance between two signatures in the form of normalised vectors allows the dissimilarity between spatial structures to be determined. The package [motif] (https://jakubnowosad.com/motif/) is designed to do this work.\n\n\n \n\n\n\n \n\n\n\n\n\n\n\n\n\nJensen-Shannon distance between the above rasters: 0.0684",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#dissimilarity-measures-example-2",
    "href": "slides/slides_session4.html#dissimilarity-measures-example-2",
    "title": "Pattern-based spatial analysis",
    "section": "Dissimilarity measures Example 2",
    "text": "Dissimilarity measures Example 2\nMeasuring the distance between two signatures in the form of normalized vectors allows determining dissimilarity between spatial structures.\n\n\n \n\n\n\n \n\n\n\n\n\n\n\n\n\nJensen-Shannon distance between the above rasters: 0.444",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#pattern-based-spatial-analysis",
    "href": "slides/slides_session4.html#pattern-based-spatial-analysis",
    "title": "Pattern-based spatial analysis",
    "section": "Pattern-based spatial analysis",
    "text": "Pattern-based spatial analysis\nThe distance between spatial signatures provides a powerful possibility to identify (dis)similarities in several contexts.\n\n\n\n\n\n\nfinding similar spatial structures - one to many comparison\n\n\n\n\n\n\n\nquantitative assessment of changes in spatial structures - one to one comparison\n\n\n\n\n\n\n\nclustering similar spatial structures - many to many comparison",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#one-to-many",
    "href": "slides/slides_session4.html#one-to-many",
    "title": "Pattern-based spatial analysis",
    "section": "One to many",
    "text": "One to many\nFinding areas with similar topography to the Suwalski Landscape Park\n\n\n\n\n\n\nTopography Indices\n\n\n\n\n\n\n\nJSD Index",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#one-to-one",
    "href": "slides/slides_session4.html#one-to-one",
    "title": "Pattern-based spatial analysis",
    "section": "One to one",
    "text": "One to one\nThe left maps are showing that many areas in the Amazon have undergone significant land cover changes between 1992 and 2018. The challenge now is to determine which areas have changed the most. The right map shows these areas identified by high JSD values.\n\n\n\nNote that changes in both category and spatial configuration are measured.\n\n\n\n\n\n\n\n\n\nLanduse Types\n\n\n\n\n\n\n\nJSD Index",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#many-to-many",
    "href": "slides/slides_session4.html#many-to-many",
    "title": "Pattern-based spatial analysis",
    "section": "Many to many",
    "text": "Many to many\nAreas in Africa with similar spatial structures for two themes have been identified - land cover and landforms.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#many-to-many-1",
    "href": "slides/slides_session4.html#many-to-many-1",
    "title": "Pattern-based spatial analysis",
    "section": "Many to many",
    "text": "Many to many\n\n\n\n\n\nThe quality of each cluster can be assessed using metrics:\n\nIntra-cluster heterogeneity: determines distances between all landscapes within a group\nInter-cluster isolation: determines distances between a given group and all others",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#examples",
    "href": "slides/slides_session4.html#examples",
    "title": "Pattern-based spatial analysis",
    "section": "Examples",
    "text": "Examples\ncoma\n\nlibrary(terra)\nlibrary(motif)\nr9 = rast(\"../exdata/r9.tif\")\nr9_sign_coma = lsp_signature(r9, type = \"coma\")\nr9_sign_coma\nr9_sign_coma$signature\n\ncove\n\nr9_sign = lsp_signature(r9, type = \"cove\")\nr9_sign\nr9_sign$signature\n\nsearch\n\nlibrary(sf)\nlandcover = rast(system.file(\"raster/landcover2015s.tif\", package = \"motif\"))\necoregions = read_sf(system.file(\"vector/ecoregionss.gpkg\", package = \"motif\"))\necoregion1 = ecoregions[1, ]\nlandcover1 = crop(landcover, ecoregion1, mask = TRUE)\nplot(landcover)\nplot(landcover1)\n\nsearch_result = lsp_search(landcover1, landcover, \n                           type = \"cove\", dist_fun = \"jensen-shannon\", window = 25,\n                           output = \"sf\")\nplot(search_result[\"dist\"])\n\nmin/max\n\nsearch_result$id[which.min(search_result$dist)]\nsearch_results_75 = lsp_extract(landcover, window = 25, id = 75)\nplot(search_results_75)\n\nsearch_result$id[which.max(search_result$dist)]\nsearch_results_215 = lsp_extract(landcover, window = 25, id = 215)\nplot(search_results_215)",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slides_session4.html#exercises",
    "href": "slides/slides_session4.html#exercises",
    "title": "Pattern-based spatial analysis",
    "section": "Exercises",
    "text": "Exercises\n\nRead the study area polygon from the exdata/harz_borders.gpkg file using the read_sf() function from the sf package.\nRead the land cover raster data for Europe from the file exdata/lc_europe.tif using the function rast() from the package terra. Visualise both datasets.\nCrop and mask the raster to the polygon boundaries. Visualise the results.\nCompute a spatial signature for the study area. Can you understand its meaning?\nFind out which areas of the Europe raster are most similar to the study area (this may take a minute or so). Try different window sizes (e.g. 200 or 500).",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Pattern-based spatial analysis"
    ]
  },
  {
    "objectID": "slides/slide1.html#header-12",
    "href": "slides/slide1.html#header-12",
    "title": "Slides and extensions",
    "section": "Header (1|2)",
    "text": "Header (1|2)\nThe support of header and footer logic is provided by the plugin reveal-header. it is activated by:\nfilters:\n  - reveal-header"
  },
  {
    "objectID": "slides/slide1.html#header-22",
    "href": "slides/slide1.html#header-22",
    "title": "Slides and extensions",
    "section": "Header (2|2)",
    "text": "Header (2|2)\nIn this example you will find a basic header and footer text, pagination and a logo in the upper left corner .\n---\ntitle: \"Slides and extensions\"\nsubtitle: \"basically shows the 3 extensions samples\"\ntitle-slide-attributes:\n  data-background-image: slide1/mof.png\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\nformat: \n  revealjs:\n    slide-number: true\n    footer: &lt;gisma 2023&gt;\n    header: This is the header extension\n    header-logo: slide1/logooil.jpg\n[...]\n---"
  },
  {
    "objectID": "slides/slide1.html#spotlight-12",
    "href": "slides/slide1.html#spotlight-12",
    "title": "Slides and extensions",
    "section": "Spotlight (1|2)",
    "text": "Spotlight (1|2)\nThe support of a pointer or similar pointing features is provided by the plugin spotlight. it is activated by:\nrevealjs-plugins:\n  - spotlight"
  },
  {
    "objectID": "slides/slide1.html#spotlight-22",
    "href": "slides/slide1.html#spotlight-22",
    "title": "Slides and extensions",
    "section": "Spotlight (2|2)",
    "text": "Spotlight (2|2)\nCurrently the spotlight is set to a red dot pointer. Just press the left mouse button and use it. It is defined in the header:\n---\n[...]\nformat: \n  revealjs:\n    slide-number: true\n    footer: &lt;gisma 2023&gt;\n    header: This is the header extension\n    header-logo: slide1/logooil.jpg\n    spotlight:\n      useAsPointer: true\n      size: 5\n\nfilters:\n  - roughnotation\n  - reveal-header\nrevealjs-plugins:\n  - spotlight\n---"
  },
  {
    "objectID": "slides/slide1.html#highlighting-concept",
    "href": "slides/slide1.html#highlighting-concept",
    "title": "Slides and extensions",
    "section": "Highlighting concept",
    "text": "Highlighting concept\nThe support of complex highlighting etc. is provided by the plugin roughnotation. it is activated by:\nfilters:\n  - roughnotation\nTo activate the highlighting interactively press the r key. It will start any notation animations:\nI will be highlighted, and so will these words right here"
  },
  {
    "objectID": "slides/slide1.html#options",
    "href": "slides/slide1.html#options",
    "title": "Slides and extensions",
    "section": "Options",
    "text": "Options\nThere are many types of options we can use (Press r to show)\n\ntype\nanimate\nanimationDuration\ncolor\nstrokeWidth\nmultiline multiline multiline multiline multiline multiline multiline multiline multiline multiline\niterations\nrtl"
  },
  {
    "objectID": "slides/slide1.html#options-1",
    "href": "slides/slide1.html#options-1",
    "title": "Slides and extensions",
    "section": "Options",
    "text": "Options\n(Press r to show)\nThe options are applied by adding arguments like so {.rn rn-color=orange rn-type=circle}\nSo to add a orange circle or turn off animations by adding rn-animate=false\nNote that the arguments are all prefixed with rn-, are not comma-separated, logical values are written as true or false and that strings do not have to be in quotes"
  },
  {
    "objectID": "slides/slide1.html#options---types",
    "href": "slides/slide1.html#options---types",
    "title": "Slides and extensions",
    "section": "Options - types",
    "text": "Options - types\n(Press r to show)\n\n\nUnderline\nBox\nCircle\nHighlight\nStrike-Through\nCrossed-off\n\nMany types to choose from!\nHyphenated options can be used like so rn-type=strike-through"
  },
  {
    "objectID": "slides/slide1.html#options---multiline",
    "href": "slides/slide1.html#options---multiline",
    "title": "Slides and extensions",
    "section": "Options - Multiline",
    "text": "Options - Multiline\n(Press r to show)\nThe options rn-multiline=true can be added to make a highligher work across multiple lines.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed accumsan nisi hendrerit augue molestie tempus. Phasellus purus quam, aliquet nec commodo quis, pharetra ut orci. Donec laoreet ligula nisl, placerat molestie mauris luctus id. Fusce dapibus non libero nec lobortis."
  },
  {
    "objectID": "slides/slide1.html#all-about-time",
    "href": "slides/slide1.html#all-about-time",
    "title": "Slides and extensions",
    "section": "All about Time",
    "text": "All about Time\n(Press r to show)\nUnless otherwise specified, all annotations will occur at the same time. Set the rn-index to specify order\nNo rn-index\nrn-index set to 1\nrn-index set to 2\nrn-index set to 3\nrn-index set to 4"
  },
  {
    "objectID": "slides/slide1.html#fenced-divs",
    "href": "slides/slide1.html#fenced-divs",
    "title": "Slides and extensions",
    "section": "Fenced divs",
    "text": "Fenced divs\nYou can also use fenced divs if you want to apply the changes to larger sections of of the slide\n::: {.rn rn-type=box rn-color=red}\nHere is some text\n\nAnd there is more here\n:::\n\nHere is some text\nAnd there is more here"
  },
  {
    "objectID": "slides/slide1.html#known-issues",
    "href": "slides/slide1.html#known-issues",
    "title": "Slides and extensions",
    "section": "Known issues",
    "text": "Known issues\ndoesn’t show correctly in RStudio IDE\nDepending on Browser and setting use the CTRL +/- zoom to place the highlights at the correct places"
  },
  {
    "objectID": "slides/slide1.html#basic-reference",
    "href": "slides/slide1.html#basic-reference",
    "title": "Slides and extensions",
    "section": "Basic Reference",
    "text": "Basic Reference\nFind more informations at Quarto RevealJS Documentation"
  },
  {
    "objectID": "slides/slides_session6.html#retrieving-sentinel-data",
    "href": "slides/slides_session6.html#retrieving-sentinel-data",
    "title": "Change detection part 1",
    "section": "Retrieving Sentinel data",
    "text": "Retrieving Sentinel data"
  },
  {
    "objectID": "slides/slides_session6.html#cloud-optimised-geotiffs-cogs",
    "href": "slides/slides_session6.html#cloud-optimised-geotiffs-cogs",
    "title": "Change detection part 1",
    "section": "Cloud-Optimised GeoTIFFs (COGs)",
    "text": "Cloud-Optimised GeoTIFFs (COGs)\nUnfortunately, the official Sentinel-2 archives are anything but user-friendly. Even with very convenient tools such as sen2r it is sometimes tedious to process them.Technically, the processed product levels are available for download pre-processed as L1C and L2A products in JP2K format. The preferred file format is JP2K, which is storage efficient but has to be downloaded in its entirety locally by the user, resulting in high access costs and huge local storage requirements. The cloud-optimised GeoTIFFs (COGs) allow only the areas of interest to be downloaded and are also much faster to process. However, this requires optimised cloud services and a technically different access logic than in the processing chains used so far."
  },
  {
    "objectID": "slides/slides_session6.html#spatiotemporal-asset-catalog-stac",
    "href": "slides/slides_session6.html#spatiotemporal-asset-catalog-stac",
    "title": "Change detection part 1",
    "section": "SpatioTemporal Asset Catalog (STAC)",
    "text": "SpatioTemporal Asset Catalog (STAC)\nThe [Spatial-Temporal Asset Catalogue] (https://stacspec.org/) (STAC) provides a common language for simplified indexing and discovery of geospatial data. A “Spatio-Temporal Asset” is a file that contains information in a specific space and time.\nThis approach allows any provider of spatio-temporal data (imagery, SAR, point clouds, data cubes, full motion video, etc.) to provide Spatio-Temporal Asset Catalogues (STAC) for their data. STAC focuses on an easy-to-implement standard that organisations can use to make their data available in a durable and reliable way.\nElement84 has provided a public API called Earth-search, a central search catalogue for all public AWS datasets using STAC (including the new Sentinel-2 COGs), which contains more than 11.4 million Sentinel-2 scenes worldwide as of 1 November 2017."
  },
  {
    "objectID": "slides/slides_session6.html#setup-the-working-environment",
    "href": "slides/slides_session6.html#setup-the-working-environment",
    "title": "Change detection part 1",
    "section": "Setup the working environment",
    "text": "Setup the working environment\n\n# adapt your directory this is following the concept of the envimaR structure\ntutorialDir = path.expand(\"data/\")\n\nlibrary(sf)\nlibrary(raster)\nlibrary(tidyverse)\nlibrary(downloader)\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(mapview)\nlibrary(gdalcubes)\nlibrary(OpenStreetMap)\nlibrary(stars)\nlibrary(colorspace)\nlibrary(rstac)\n\n\nndvi.col = function(n) {\n  rev(sequential_hcl(n, \"Green-Yellow\"))\n}\n\nano.col = diverging_hcl(7, palette = \"Red-Green\",  register = \"rg\")"
  },
  {
    "objectID": "slides/slides_session6.html#defining-the-area-of-interest",
    "href": "slides/slides_session6.html#defining-the-area-of-interest",
    "title": "Change detection part 1",
    "section": "Defining the Area of Interest",
    "text": "Defining the Area of Interest\nOne major challenge is the fact that most of the earth surface related remote sensing activities are heavily “disturbed” by the atmosphere, especially by clouds. So to find cloud free satellite imagery is a common and cumbersome task. This task is supported by the rstac package which provides a convenient tool to find and filter adequate Sentinel-2 images out of the COG data storage. However, to address the AOI we need to provide the extend via the bbox argument of the corresponding function stac_search(). So first we need to derive and transform the required bounding box to WGS84 geo-coordinates, easily done with the sf functions st_bbox() and st_transform(). In addition we adapt the projection of the referencing vector objects to all other later projection needs.\n\n\n\nPlease note to project to three different CRS is for this examples convenience and clarity and somewhat superfluous. Only the corner coordinates of the sections are required and not the complete geometries. However, it creates more clarity for the later process to already have the data needed in different projections.\n\n\n\n\nlibrary(tmap)\nlibrary(tmaptools)\nutils::download.file(url=\"https://github.com/gisma/gismaData/raw/master/MOF/MOF_CORE.gpkg\",destfile=\"../data/MOF_CORE.gpkg\")\n\nforest_mask = st_read(\"../data/MOF_CORE.gpkg\")\nsf::st_crs(forest_mask) &lt;- 4326\nfm=bb(forest_mask,projection=4326)\nforest_4326 =st_transform(forest_mask,crs = 4326)\nforest_3035 =st_transform(forest_mask,crs = 3035)\nforest_32632 =st_transform(forest_mask,crs = 32632)\n\n\n# call background map\nosm_forest &lt;- tmaptools::read_osm(fm,type = \"bing\")\n\n# mapping the extents and boundaries of the choosen geometries\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE) + \n  tm_basemap(server = c(\"Esri.WorldGrayCanvas\", \"OpenStreetMap\", \"Esri.WorldTopoMap\",\"Esri.WorldImagery\")) +\n  tm_shape(forest_mask) +   \n  tm_polygons(alpha = 0.4, col=\"mainTreeSp\")\n\nOur study area is pretty small, covering roughly 2.5 by 2.5 km forest area northwest of Marburg. The core part of the AOI is the Marburg Open Forest (MOF) a field research facility of the Philipps University Marburg.\nQuerying images with rstac\nUsing the rstac package, we first request all available images from 2017 to 2020 that intersect with our region of interest. Here, since the polygon has WGS84 as CRS, we do not need to transform the bounding box before using the stac_search() function.\n\n# search the data stack for the given period and area\nlibrary(rstac)\n\ns = stac(\"https://earth-search.aws.element84.com/v0\")\nitems &lt;- s |&gt;\n  stac_search(collections = \"sentinel-s2-l2a-cogs\",\n              bbox = c(st_bbox(forest_4326)[\"xmin\"],\n                       st_bbox(forest_4326)[\"ymin\"],\n                       st_bbox(forest_4326)[\"xmax\"],\n                       st_bbox(forest_4326)[\"ymax\"]), \n              datetime = \"2018-01-01/2022-12-31\",\n              limit = 1000) |&gt;\n  post_request() \nitems\n\n# print date and time of first and last images\nrange(sapply(items$features, function(x) {x$properties$datetime}))\n\nThis gives us 315 matching images recorded between Januar 2019 and December 2023.\nCreating a monthly Sentinel-2 data cube\nTo obtain a Sentinel data cube, a gdalcube image collection must be created from the STAC query result. To do this, the asset names must be explicitly named in order to apply the SCL channel with the quality characteristics per pixel (classification as clouds, cloud shadows, etc.). In this query, a filter is set to cloud cover &lt;= 50%.\n\nlibrary(gdalcubes)\nassets = c(\"B01\",\"B02\",\"B03\",\"B04\",\"B05\",\"B06\", \"B07\",\"B08\",\"B8A\",\"B09\",\"B11\",\"SCL\")\ns2_collection = stac_image_collection(items$features, asset_names = assets, property_filter = function(x) {x[[\"eo:cloud_cover\"]] &lt; 50}) \ns2_collection\n\nThe result is 107 images, i.e. approx. 1.8 images per month, from which we can now create a data cube. To do this, we use the UTM bounding box of our polygon as a spatial boundary, a spatial resolution of 10 metres, a bilinear spatial interpolation (useful for the spatially lower-resolution sentinel channels) and calculate monthly median values for all pixel values from the available images of a month. In addition, we add a buffer (b) on each side of the cube.\n\n\n\nThe gdalcube image collection can be considered as a proxy structure object which will be applied on the COGs.\n\n\n\nb = 100\nv = cube_view(srs = \"EPSG:32632\", \n              dx = 10, \n              dy = 10, \n              dt = \"P1M\",  \n              aggregation = \"median\", \n              extent = list(t0 = \"2018-01-01\",\n                            t1 = \"2022-12-31\", \n                            left = st_bbox(forest_32632)[\"xmin\"] - b, \n                            right = st_bbox(forest_32632)[\"xmax\"] + b,\n                            bottom = st_bbox(forest_32632)[\"ymin\"] - b, \n                            top = st_bbox(forest_32632)[\"ymax\"] + b),\n              resampling = \"bilinear\")\nv\n\nNext we create a data cube, subset the red and near infrared bands and crop by our polygon, which simply sets pixel values outside the polygon to NA. We then save the data cube as a single netCDF file. Note that this is not necessary, but saving intermediate results sometimes makes debugging easier, especially if the methods applied afterwards are computationally intensive.\n\n\n\nOnly calling a final action will start the processing on the COG-Server. In this case ‘write_ncdf’.\n\n\n\n# we \"download\" the data and write it t a netcdf file\n  s2.mask = image_mask(\"SCL\", values = c(3,8,9))\n  gdalcubes_options(parallel = 16, ncdf_compression_level = 5)\n  raster_cube(s2_collection, v, mask = s2.mask) |&gt;\n    write_ncdf(\"../data/MOF_10_2.nc\",overwrite=TRUE)\n\nIn the following examples we will show a simple plot in a generic pipe. However, a more common visualisation is quite simple. A typical pipe might be Save a resulting netCDF file -&gt; Convert it to a stars object -&gt; Visualise the results with tmap, mapview, ggplot2 or whatever package you prefer to produce (interactive) map(s).\n\nReductions\nPossible reducers include min\", mean\", median\", max\", count\" (count non-missing values), sum\", var\" (variance), and sd\" (standard deviation). Reducer expressions are always given as a string, starting with the reducer name, followed by the band name in parentheses. Note that it is possible to mix reducers and bands.\nCounting\nTo get an idea of the data, we can compute simple summary statistics using a ‘reducer’ function over dimensions. For example, using the count reducer, we can learn about the temporal coverage for each aggregated pixel, giving us an initial understanding of the temporal quality of the dataset.\n\nncdf_cube(\"../data/MOF_10_2.nc\") |&gt;\n  reduce_time(\"count(B04)\") |&gt; \n  plot(key.pos = 1,  col = viridis::viridis, nbreaks = 10)\n\nWe can see that most time series contain valid observations of around 60 months, which should be sufficient for our example. Similarly, it is also possible to reduce over space, leading to summary time series.\nBelow you will find various examples dealing with the common NDVI and the kNDVI Indices.\nkNDVI\nBelow, we derive mean monthly kNdvi values over all pixel time series.\n\nncdf_cube(\"../data/MOF_10_2.nc\") |&gt;\n    apply_pixel(\"tanh(((B08-B04)/(B08+B04))^2)\", \"kNDVI\") |&gt;\n  reduce_time(\"mean(kNDVI)\") |&gt;\n  plot(key.pos = 1,  col = ndvi.col, nbreaks = 12)\n\n\n\nNDVI\nBelow, we derive mean monthly Ndvi values over all pixel time series.\n\nncdf_cube(\"../data/MOF_10_2.nc\") |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  reduce_time(\"mean(NDVI)\") |&gt;\n  plot(key.pos = 1, zlim=c(-0.2,1), col = ndvi.col, nbreaks = 12)\n\nZonal Statistics\n\nzstats = ncdf_cube(\"../data/MOF_10_2.nc\") |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  extract_geom(forest_32632,FUN=median) \n\nforest_32632$FID = rownames(forest_32632)\nx = merge(forest_32632, zstats, by = \"FID\")\nmapview(x[x$time == \"2020-07\", \"NDVI\"],col.regions = ndvi.col)\n\nTimeseries\n\nncdf_cube(\"../data/MOF_10.nc\") |&gt;\n  apply_pixel(\"tanh(((B08-B04)/(B08+B04))^2)\", \"kNDVI\") |&gt;\n  reduce_space(\"min(kNDVI)\", \"max(kNDVI)\", \"mean(kNDVI)\") |&gt;\n  plot(join.timeseries = TRUE)\n\nCalculate the annual NDVI difference - using functions\nThe following function will iterate through 3 years of NDVI data and calculating NDVI difference maps for each pair of years.\n\ngdalcubes_options(parallel =  12)\n\n# ndvi operand\nndvi_type = \"median(kNDVI)\"\n\n# time slots as tibble\nts = tibble(t0 = c(\"2018-04-01\",\"2019-04-01\",\"2020-04-01\",\"2021-04-01\"), \n            t1= c(\"2018-09-01\",\"2019-09-01\",\"2020-09-01\",\"2021-09-01\"))\n\n# names of the time slots\nnames_ndvi = paste(ts$t0,ts$t1,sep = \" to \")\n\nbasic_kndvi &lt;- function(fncube=NULL, t0=NULL, t1=NULL) {\n  ncdf_cube(fncube) |&gt; \n    select_bands(c(\"B04\", \"B08\")) |&gt;\n    select_time(c(t0,t1)) |&gt;\n  apply_pixel(\"tanh(((B08-B04)/(B08+B04))^2)\", \"kNDVI\") |&gt;\n    reduce_time(ndvi_type)\n}\n\n# (stac-search -&gt; stac and cloud filter image collection -&gt; create cube -&gt; call user function)\nndvi = list()\nfor (i in 1:nrow(ts)){\n  # call user function\n  basic_kndvi(fncube = \"../data/MOF_10.nc\", t0 = ts$t0[i], t1 = ts$t1[i]) %&gt;%\n    st_as_stars() -&gt; ndvi[[i]]\n  \n}\n\n# now we may create a mask according to the NDVI extent and resolution\nstars::write_stars(ndvi[[1]] * 0, paste0(tutorialDir,\"forest/only_forestmask.tif\"))\ngi=link2GI::linkGDAL(searchLocation = \"/usr/bin/\")\ncmd=gi$bin[[1]]$gdal_bin[14]\nsystem(paste(cmd ,'-burn 1.0 -tr 10.0 10.0 -a_nodata 0.0 -te ', st_bbox(ndvi[[1]])$xmin,' ',st_bbox(ndvi[[1]])$ymin,' ',st_bbox(ndvi[[1]])$xmax,' ',st_bbox(ndvi[[1]])$ymax,' -ot Float32 -of GTiff', paste0(tutorialDir,\"forest/MOF_mask_pr.shp \"),paste0(tutorialDir,\"forest/only_forestmask.tif\")))\nmask &lt;- raster(paste0(tutorialDir,\"forest/only_forestmask.tif\"))\nplot(mask)                                \n\nmask[mask == 0] = NA\n\n# mapping the results\ntmap_mode(\"plot\")\ntm_ndvi = lapply(seq(2:length(names_ndvi) -1),function(i){\n  m = tm_shape(osm_forest) + tm_rgb() +\n    tm_shape((ndvi[[i]] - ndvi[[i+1]]) * st_as_stars(mask )) +\n    tm_raster(title = ndvi_type,pal =diverging_hcl(11, \"rg\")) +\n    tm_layout(panel.labels = paste(\"Difference \",names_ndvi[[i]],\"/\",names_ndvi[[i+1]]),\n              legend.show = TRUE,\n              panel.label.color = \"darkblue\",\n              panel.label.size =0.5,\n              panel.label.height=1.2,\n              legend.text.size = 0.3,\n              legend.outside = TRUE) +\n    tm_grid()\n})\n\n\n# tmap_arrange(tm_ndvi,nrow = 1,asp = NA,widths = c(0.33,0.33,0.33),outer.margins = 0.001)\n\nResulting maps\n\n\nUse case: Spatial identification of magnitudes and time periods of kNDVI changes\nTo apply a more complex time series method such as bfastmonitor(), the data cube operations below allow to provide custom user-defined R functions instead of string expressions, which translate to built-in reducers. It is very important that these functions receive arrays as input and must return arrays as output, too. Depending on the operation, the dimensionality of the arrays is different:\n\n\n\n\n\n\n\n\nOperator\nInput\nOutput\n\n\n\n\napply_pixel\nVector of band values for one pixel\nVector of band values of one pixel\n\n\nreduce_time\nMulti-band time series as a matrix\nVector of band values\n\n\nreduce_space\nThree-dimensional array with dimensions bands, x, and y\nVector of band values\n\n\napply_time\nMulti-band time series as a matrix\nMulti-band time series as a matrix\n\n\n\nThere is almost no limit of what R function we use, but we must take care of a few things: 1. The reducer function is executed in a new R process without access to the current workspace. It is not possible to access variables defined outside of the function and packages must be loaded within the function. 2. The reducer function must always return a vector with the same length (for all time series). 3. It is a good idea to think about NA values, i.e. you should check whether the complete time series is NA, and that missing values do not produce errors.\nAnother possibility to apply R functions to data cubes is of course to convert data cubes to stars objects and use the stars package for further analysis.\nApplying bfastmonitor as a user-defined reducer function\nIn our example, bfastmonitor returns change date and change magnitude values per time series so we can use reduce_time(). The script below (1) calculates the kNDVI, (2) applies bfastmonitor(), and properly handles errors e.g. due to missing data with tryCatch(), and (3) finally writes out the resulting change dates and magnitudes of change for all pixels of the time series as a netCDF file. The results shows the changes starting at 1/2019 until 12/2021 and identify pretty well the dynamical impacts of drought and bark beetle in the Marburg University Forest (MOF).\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(gdalcubes)\nlibrary(stars)\nlibrary(colorspace)\nndvi.col = function(n) {\n  rev(sequential_hcl(n, \"Green-Yellow\"))\n}\ntutorialDir = path.expand(\"~/edu/agis/doc/data/tutorial/\")\n\nfigtrim &lt;- function(path) {\n  img &lt;- magick::image_trim(magick::image_read(path))\n  magick::image_write(img, path)\n  path\n}\ngdalcubes_options(parallel = 12)\n## start analysis\nsystem.time(\n  ncdf_cube(\"../data/MOF_10.nc\") |&gt;\n    reduce_time(names = c(\"change_date\", \"change_magnitude\",\"kndvi\"), FUN = function(x) {\n      kndvi = tanh(((x['B08',]-x['B04',])/(x['B08',]+x['B04',]))^2)\n      if (all(is.na(kndvi))) {\n        return(c(NA,NA))\n      }\n      kndvi_ts = ts(kndvi, start = c(2017, 1), frequency = 12)\n      library(bfast)\n      tryCatch({\n        result = bfastmonitor(kndvi_ts, start = c(2020,1), \n                              history = \"all\", level = 0.01)\n        return(c(result$breakpoint, result$magnitude))\n      }, error = function(x) {\n        return(c(NA,NA))\n      })\n    }) |&gt;\n    write_ncdf(paste0(tutorialDir,\"bf_results.nc\"),overwrite = TRUE))\n\nNow we can use the netCDF file and map the results with any preferred visualisation tool. In this case tmap.\n\nlibrary(tmap)\n\nmask &lt;- raster(paste0(tutorialDir,\"forest/only_forestmask.tif\"))\n# plotting it from the local ncdf  \ntmap_mode(\"view\")\ngdalcubes::ncdf_cube(paste0(tutorialDir,\"bf_results.nc\")) |&gt;\n  stars::st_as_stars() -&gt; x\ntm_shape(osm_forest) + tm_rgb() +\n  tm_shape(x[1] * st_as_stars(mask )) + \n  tm_raster(n = 6)  +\n  tm_layout(\n    legend.show = TRUE,\n    panel.label.height=0.6,\n    panel.label.size=0.6,\n    legend.text.size = 0.4,\n    legend.outside = TRUE) +\n  tm_grid()\n\ntm_shape(osm_forest) + tm_rgb() +\n  tm_shape(x[2]* st_as_stars(mask ))  + tm_raster() +\n  tm_layout(legend.title.size = 1,\n            panel.label.height=0.6,\n            panel.label.size=0.6,\n            legend.text.size = 0.4,\n            legend.outside = TRUE) +\n  tm_grid()\n\n\n\n\nMagnitude Change Map\n\n\n\n\n\nPeriod of Change Map\n\nRunning bfastmonitor() is computationally expensive. However, since in common (not in our case) the data should be located in the cloud, it would be obvious to launch one of the (payed) more powerful machine instance types with many processors. Parallelization within one instance can be controlled entirely by gdalcubes using gdalcubes_options() which is extremely simple.\nFrom a scientific point of view we need to tune some aspects. The kNDVI Index together with the bfastmonitor approach is somewhat sophisticated and need a validation strategy. Also the correlation to the different tree species could be promising. So there is certainly a need for more analysis to understand the processes and to identify false results.\n\n\nSummary\nThis examples have shown, how simple and more more complex methods can be applied on data cubes. Especially the chosen approach to just “download” the data from a COG server and perform the processing on the local machine is kind of promising due to the fact that the code can be run almost identically on a cloud instance. The use of more common or well known functionalities and packages helps a lot migrating to gdalcubes concept.\nIt is even more powerful to use this workflow for extremely comfortable multisensor multiscale approaches.\n\n\n\nReferences\n:::\n::::\n:::::\n\n\n\nMinimum temperature (°C)\n\n\n\n\n\nCMIP6 downscaled future climate projection for 2061-2080 [model: CNRM-ESM2-1; ssp: “585”]\n\n\n\n\n\n\nWorldClim version 2.1 climate data for 1970-2000"
  },
  {
    "objectID": "slides/slides_session6.html#reductions",
    "href": "slides/slides_session6.html#reductions",
    "title": "Change detection part 1",
    "section": "Reductions",
    "text": "Reductions\nPossible reducers include min\", mean\", median\", max\", count\" (count non-missing values), sum\", var\" (variance), and sd\" (standard deviation). Reducer expressions are always given as a string, starting with the reducer name, followed by the band name in parentheses. Note that it is possible to mix reducers and bands.\nCounting\nTo get an idea of the data, we can compute simple summary statistics using a ‘reducer’ function over dimensions. For example, using the count reducer, we can learn about the temporal coverage for each aggregated pixel, giving us an initial understanding of the temporal quality of the dataset.\n\nncdf_cube(\"../data/MOF_10_2.nc\") |&gt;\n  reduce_time(\"count(B04)\") |&gt; \n  plot(key.pos = 1,  col = viridis::viridis, nbreaks = 10)\n\nWe can see that most time series contain valid observations of around 60 months, which should be sufficient for our example. Similarly, it is also possible to reduce over space, leading to summary time series.\nBelow you will find various examples dealing with the common NDVI and the kNDVI Indices.\nkNDVI\nBelow, we derive mean monthly kNdvi values over all pixel time series.\n\nncdf_cube(\"../data/MOF_10_2.nc\") |&gt;\n    apply_pixel(\"tanh(((B08-B04)/(B08+B04))^2)\", \"kNDVI\") |&gt;\n  reduce_time(\"mean(kNDVI)\") |&gt;\n  plot(key.pos = 1,  col = ndvi.col, nbreaks = 12)"
  },
  {
    "objectID": "slides/slides_session6.html#ndvi",
    "href": "slides/slides_session6.html#ndvi",
    "title": "Change detection part 1",
    "section": "NDVI",
    "text": "NDVI\nBelow, we derive mean monthly Ndvi values over all pixel time series.\n\nncdf_cube(\"../data/MOF_10_2.nc\") |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  reduce_time(\"mean(NDVI)\") |&gt;\n  plot(key.pos = 1, zlim=c(-0.2,1), col = ndvi.col, nbreaks = 12)\n\nZonal Statistics\n\nzstats = ncdf_cube(\"../data/MOF_10_2.nc\") |&gt;\n  apply_pixel(\"(B08-B04)/(B08+B04)\", \"NDVI\") |&gt;\n  extract_geom(forest_32632,FUN=median) \n\nforest_32632$FID = rownames(forest_32632)\nx = merge(forest_32632, zstats, by = \"FID\")\nmapview(x[x$time == \"2020-07\", \"NDVI\"],col.regions = ndvi.col)\n\nTimeseries\n\nncdf_cube(\"../data/MOF_10.nc\") |&gt;\n  apply_pixel(\"tanh(((B08-B04)/(B08+B04))^2)\", \"kNDVI\") |&gt;\n  reduce_space(\"min(kNDVI)\", \"max(kNDVI)\", \"mean(kNDVI)\") |&gt;\n  plot(join.timeseries = TRUE)\n\nCalculate the annual NDVI difference - using functions\nThe following function will iterate through 3 years of NDVI data and calculating NDVI difference maps for each pair of years.\n\ngdalcubes_options(parallel =  12)\n\n# ndvi operand\nndvi_type = \"median(kNDVI)\"\n\n# time slots as tibble\nts = tibble(t0 = c(\"2018-04-01\",\"2019-04-01\",\"2020-04-01\",\"2021-04-01\"), \n            t1= c(\"2018-09-01\",\"2019-09-01\",\"2020-09-01\",\"2021-09-01\"))\n\n# names of the time slots\nnames_ndvi = paste(ts$t0,ts$t1,sep = \" to \")\n\nbasic_kndvi &lt;- function(fncube=NULL, t0=NULL, t1=NULL) {\n  ncdf_cube(fncube) |&gt; \n    select_bands(c(\"B04\", \"B08\")) |&gt;\n    select_time(c(t0,t1)) |&gt;\n  apply_pixel(\"tanh(((B08-B04)/(B08+B04))^2)\", \"kNDVI\") |&gt;\n    reduce_time(ndvi_type)\n}\n\n# (stac-search -&gt; stac and cloud filter image collection -&gt; create cube -&gt; call user function)\nndvi = list()\nfor (i in 1:nrow(ts)){\n  # call user function\n  basic_kndvi(fncube = \"../data/MOF_10.nc\", t0 = ts$t0[i], t1 = ts$t1[i]) %&gt;%\n    st_as_stars() -&gt; ndvi[[i]]\n  \n}\n\n# now we may create a mask according to the NDVI extent and resolution\nstars::write_stars(ndvi[[1]] * 0, paste0(tutorialDir,\"forest/only_forestmask.tif\"))\ngi=link2GI::linkGDAL(searchLocation = \"/usr/bin/\")\ncmd=gi$bin[[1]]$gdal_bin[14]\nsystem(paste(cmd ,'-burn 1.0 -tr 10.0 10.0 -a_nodata 0.0 -te ', st_bbox(ndvi[[1]])$xmin,' ',st_bbox(ndvi[[1]])$ymin,' ',st_bbox(ndvi[[1]])$xmax,' ',st_bbox(ndvi[[1]])$ymax,' -ot Float32 -of GTiff', paste0(tutorialDir,\"forest/MOF_mask_pr.shp \"),paste0(tutorialDir,\"forest/only_forestmask.tif\")))\nmask &lt;- raster(paste0(tutorialDir,\"forest/only_forestmask.tif\"))\nplot(mask)                                \n\nmask[mask == 0] = NA\n\n# mapping the results\ntmap_mode(\"plot\")\ntm_ndvi = lapply(seq(2:length(names_ndvi) -1),function(i){\n  m = tm_shape(osm_forest) + tm_rgb() +\n    tm_shape((ndvi[[i]] - ndvi[[i+1]]) * st_as_stars(mask )) +\n    tm_raster(title = ndvi_type,pal =diverging_hcl(11, \"rg\")) +\n    tm_layout(panel.labels = paste(\"Difference \",names_ndvi[[i]],\"/\",names_ndvi[[i+1]]),\n              legend.show = TRUE,\n              panel.label.color = \"darkblue\",\n              panel.label.size =0.5,\n              panel.label.height=1.2,\n              legend.text.size = 0.3,\n              legend.outside = TRUE) +\n    tm_grid()\n})\n\n\n# tmap_arrange(tm_ndvi,nrow = 1,asp = NA,widths = c(0.33,0.33,0.33),outer.margins = 0.001)\n\nResulting maps"
  },
  {
    "objectID": "slides/slides_session6.html#use-case-spatial-identification-of-magnitudes-and-time-periods-of-kndvi-changes",
    "href": "slides/slides_session6.html#use-case-spatial-identification-of-magnitudes-and-time-periods-of-kndvi-changes",
    "title": "Change detection part 1",
    "section": "Use case: Spatial identification of magnitudes and time periods of kNDVI changes",
    "text": "Use case: Spatial identification of magnitudes and time periods of kNDVI changes\nTo apply a more complex time series method such as bfastmonitor(), the data cube operations below allow to provide custom user-defined R functions instead of string expressions, which translate to built-in reducers. It is very important that these functions receive arrays as input and must return arrays as output, too. Depending on the operation, the dimensionality of the arrays is different:\n\n\n\n\n\n\n\n\nOperator\nInput\nOutput\n\n\n\n\napply_pixel\nVector of band values for one pixel\nVector of band values of one pixel\n\n\nreduce_time\nMulti-band time series as a matrix\nVector of band values\n\n\nreduce_space\nThree-dimensional array with dimensions bands, x, and y\nVector of band values\n\n\napply_time\nMulti-band time series as a matrix\nMulti-band time series as a matrix\n\n\n\nThere is almost no limit of what R function we use, but we must take care of a few things: 1. The reducer function is executed in a new R process without access to the current workspace. It is not possible to access variables defined outside of the function and packages must be loaded within the function. 2. The reducer function must always return a vector with the same length (for all time series). 3. It is a good idea to think about NA values, i.e. you should check whether the complete time series is NA, and that missing values do not produce errors.\nAnother possibility to apply R functions to data cubes is of course to convert data cubes to stars objects and use the stars package for further analysis.\nApplying bfastmonitor as a user-defined reducer function\nIn our example, bfastmonitor returns change date and change magnitude values per time series so we can use reduce_time(). The script below (1) calculates the kNDVI, (2) applies bfastmonitor(), and properly handles errors e.g. due to missing data with tryCatch(), and (3) finally writes out the resulting change dates and magnitudes of change for all pixels of the time series as a netCDF file. The results shows the changes starting at 1/2019 until 12/2021 and identify pretty well the dynamical impacts of drought and bark beetle in the Marburg University Forest (MOF).\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(tmaptools)\nlibrary(gdalcubes)\nlibrary(stars)\nlibrary(colorspace)\nndvi.col = function(n) {\n  rev(sequential_hcl(n, \"Green-Yellow\"))\n}\ntutorialDir = path.expand(\"~/edu/agis/doc/data/tutorial/\")\n\nfigtrim &lt;- function(path) {\n  img &lt;- magick::image_trim(magick::image_read(path))\n  magick::image_write(img, path)\n  path\n}\ngdalcubes_options(parallel = 12)\n## start analysis\nsystem.time(\n  ncdf_cube(\"../data/MOF_10.nc\") |&gt;\n    reduce_time(names = c(\"change_date\", \"change_magnitude\",\"kndvi\"), FUN = function(x) {\n      kndvi = tanh(((x['B08',]-x['B04',])/(x['B08',]+x['B04',]))^2)\n      if (all(is.na(kndvi))) {\n        return(c(NA,NA))\n      }\n      kndvi_ts = ts(kndvi, start = c(2017, 1), frequency = 12)\n      library(bfast)\n      tryCatch({\n        result = bfastmonitor(kndvi_ts, start = c(2020,1), \n                              history = \"all\", level = 0.01)\n        return(c(result$breakpoint, result$magnitude))\n      }, error = function(x) {\n        return(c(NA,NA))\n      })\n    }) |&gt;\n    write_ncdf(paste0(tutorialDir,\"bf_results.nc\"),overwrite = TRUE))\n\nNow we can use the netCDF file and map the results with any preferred visualisation tool. In this case tmap.\n\nlibrary(tmap)\n\nmask &lt;- raster(paste0(tutorialDir,\"forest/only_forestmask.tif\"))\n# plotting it from the local ncdf  \ntmap_mode(\"view\")\ngdalcubes::ncdf_cube(paste0(tutorialDir,\"bf_results.nc\")) |&gt;\n  stars::st_as_stars() -&gt; x\ntm_shape(osm_forest) + tm_rgb() +\n  tm_shape(x[1] * st_as_stars(mask )) + \n  tm_raster(n = 6)  +\n  tm_layout(\n    legend.show = TRUE,\n    panel.label.height=0.6,\n    panel.label.size=0.6,\n    legend.text.size = 0.4,\n    legend.outside = TRUE) +\n  tm_grid()\n\ntm_shape(osm_forest) + tm_rgb() +\n  tm_shape(x[2]* st_as_stars(mask ))  + tm_raster() +\n  tm_layout(legend.title.size = 1,\n            panel.label.height=0.6,\n            panel.label.size=0.6,\n            legend.text.size = 0.4,\n            legend.outside = TRUE) +\n  tm_grid()\n\n\n\n\nMagnitude Change Map\n\n\n\n\n\nPeriod of Change Map\n\nRunning bfastmonitor() is computationally expensive. However, since in common (not in our case) the data should be located in the cloud, it would be obvious to launch one of the (payed) more powerful machine instance types with many processors. Parallelization within one instance can be controlled entirely by gdalcubes using gdalcubes_options() which is extremely simple.\nFrom a scientific point of view we need to tune some aspects. The kNDVI Index together with the bfastmonitor approach is somewhat sophisticated and need a validation strategy. Also the correlation to the different tree species could be promising. So there is certainly a need for more analysis to understand the processes and to identify false results."
  },
  {
    "objectID": "slides/slides_session6.html#minimum-temperature-c",
    "href": "slides/slides_session6.html#minimum-temperature-c",
    "title": "Change detection part 1",
    "section": "Minimum temperature (°C)",
    "text": "Minimum temperature (°C)\n\n\n\n\n\nCMIP6 downscaled future climate projection for 2061-2080 [model: CNRM-ESM2-1; ssp: “585”]\n\n\n\n\n\n\nWorldClim version 2.1 climate data for 1970-2000"
  },
  {
    "objectID": "reader/mc_2025_1.html",
    "href": "reader/mc_2025_1.html",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "",
    "text": "The PipeModel is a deliberately idealized yet physically plausible valley scenario. It distills terrain to the essentials (parabolic cross-valley profile) and optional features (left-side hill, right-side pond or hollow), so that dominant microclimate drivers become visible and testable:\nYou can sample synthetic stations, train interpolators (IDW, Kriging variants, RF, GAM), and assess them with spatial LBO-CV.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "reader/mc_2025_1.html#what-the-pipemodel-is",
    "href": "reader/mc_2025_1.html#what-the-pipemodel-is",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "What the pipemodel is",
    "text": "What the pipemodel is\nA didactic, reproducible pipeline for micro-scale spatial prediction with process-aware features and scale tuning. It simulates or ingests a domain (“scenario”), learns from station points, validates with leave-block-out CV, infers a characteristic scale (R*) from data, re-trains at that scale, and produces tuned maps, diagnostics, and optional exports.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "reader/mc_2025_1.html#architecture",
    "href": "reader/mc_2025_1.html#architecture",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "Architecture",
    "text": "Architecture\n\nMain (orchestrator): main_ultra.R is intentionally thin: it wires pieces together, runs the stages in order, shows live previews, and—optionally—saves outputs at the end. No heavy lifting.\n\n\n\nCode\n```{r}\n#| eval: false\n\n  # =====================================================================\n  # main_ultra.R — minimal: run + (optional) save-at-end\n  #\n  # Purpose:\n  #   1) Source packages + your function library + scenario registry\n  #   2) Build scenario + station sets from a chosen scenario \"make()\" factory\n  #   3) Live preview: domain/land-cover/terrain + 2x2 overview + scenario preview\n  #   4) Baseline: leave-block-out CV (LBO-CV) and prediction maps (T14, T05)\n  #   5) Scale inference: empirical variogram -&gt; (L50,L95) -&gt; U-curve -&gt; R*\n  #   6) Tuned CV & maps at R*\n  #   7) OPTIONAL: save all plots/tables/rasters at the end\n  #\n  # Design notes:\n  #   - This script stays \"thin\": all heavy lifting lives in fun_pipemodel.R\n  #     and the scenario files. This keeps the main pipe reproducible and\n  #     testable.\n  #   - Keep side effects (saving files) to the very end; set `export &lt;- FALSE`\n  #     if you just want to run and eyeball plots interactively.\n  # =====================================================================\n  \n  message(\"=== pipe main (ultra) ===\")\n  \n  # Toggle: set to FALSE to only run/plot without saving anything\n  export &lt;- TRUE\n  \n  # ---------------------------------------------------------------------\n  # 0) Setup & packages (centralized in your packages.R)\n  #    - Loads CRAN pkgs (terra, sf, ggplot2, mgcv, gstat, suncalc, ...)\n  #    - Sets knitr options (if used in a notebook context)\n  #    - We also disable spherical geometry in sf to keep planar ops robust\n  #      for projected domains (UTM in our scenarios).\n  # ---------------------------------------------------------------------\n  source(here::here(\"block4_5/src/packages.R\"))\n  sf::sf_use_s2(FALSE)\n  set.seed(42)  # one seed here (scenarios may set more where needed)\n  \n  # ---------------------------------------------------------------------\n  # 1) Functions + Scenario registry\n  #    - fun_pipemodel.R: your full function library (NO side effects)\n  #    - registry.R: maps scenario names to files; exposes source_scenario()\n  #      which returns a `make()` function to build the object.\n  # ---------------------------------------------------------------------\n  source(here::here(\"block4_5/src/fun_pipemodel.R\"))\n  source(here::here(\"block4_5/src/fun_learn_predict_core.R\"))\n  source(here::here(\"block4_5/scenarios/registry.R\"))\n  \n  # ---------------------------------------------------------------------\n  # 2) Pick a scenario\n  #    - Choose via env var SCEN (e.g., export SCEN=scen_scaled_demo)\n  #    - Defaults to \"lake_bump_dense\" which is a realistic didactic scene\n  #      (valley, lake, bump hill, dense micro-relief, LC = forest/water/bare/baseline meadow).\n  # ---------------------------------------------------------------------\n  #scen_name &lt;- Sys.getenv(\"SCEN\", \"lake_bump_dense\")\n  scen_name &lt;- Sys.getenv(\"SCEN\", \"scen_scaled_demo\")\n  make_fun  &lt;- source_scenario(scen_name)  # returns a function make(overrides=list(), do_cv=FALSE)\n  \n  # ---------------------------------------------------------------------\n  # 3) Build the object (domain + scenario + stations + params)\n  #    - `obj` is a list with a stable contract used downstream:\n  #        scen: list of rasters (E, R14, R05, I14, I05, lc, sun, ...)\n  #        stn_sf_14 / stn_sf_05: station sf at 14/05 UTC (features + temp)\n  #        block_size: integer (meters) used by LBO-CV\n  #        params$models: character vector of model names to run\n  # ---------------------------------------------------------------------\n  obj  &lt;- make_fun()\n  scen &lt;- obj$scen\n  st14 &lt;- obj$stn_sf_14\n  st05 &lt;- obj$stn_sf_05\n  bs   &lt;- obj$block_size\n  mods &lt;- obj$params$models\n  \n  # --- Safety checks that catch common wiring issues early ----------------------\n  stopifnot(inherits(st14, \"sf\"), inherits(st05, \"sf\"))\n  stopifnot(all(c(\"E\",\"R14\",\"R05\") %in% names(scen)))\n  \n  # Sun geometry must be present for the R*-tuning (cosine-of-incidence features)\n  if (is.null(scen$sun) || is.null(scen$sun$T14) || is.null(scen$sun$T05)) {\n    stop(\"Scenario did not populate scen$sun$T14 / scen$sun$T05 (alt/az). \",\n         \"Fix in the scenario builder (build_scenario) before tuning.\")\n  }\n  if (any(is.null(c(scen$sun$T14$alt, scen$sun$T14$az,\n                    scen$sun$T05$alt, scen$sun$T05$az)))) {\n    stop(\"Sun angles (alt/az) are NULL. Scenario must supply numeric alt/az for T14/T05.\")\n  }\n  \n  # ---------------------------------------------------------------------\n  # 4) Live preview: plots during the run (no side effects)\n  #    Why plot first?\n  #      - Instant sanity checks: station placement, LC, illumination maps\n  #      - Early visual cues if something is off (e.g., CRS mismatch)\n  # ---------------------------------------------------------------------\n  print(plot_landcover_terrain(scen, stations = st14, layout = \"vertical\"))\n  print(plot_block_overview_2x2_en(scen, pts_sf = st14))\n  # preview_scenario() may show truth fields, histograms, thumbnails, etc.\n  print(preview_scenario(obj))  # accepts obj or obj$scen in the robust version\n  \n    # ---------------------------------------------------------------------\n  # 5) Baseline LBO-CV & prediction maps\n  #    - For each time slice (T14 day / T05 pre-dawn):\n  #      1) Run leave-block-out CV on station data\n  #      2) Produce truth vs predicted raster maps (model ensemble)\n  #    - Diagnostics printed/printed:\n  #      * Metrics table (RMSE/MAE/R2 per model)\n  #      * Blocks plot (spatial CV blocks)\n  #      * Pred-vs-obs scatter plot\n  #      * Truth and prediction maps\n  # ---------------------------------------------------------------------\n  run14 &lt;- run_for_time(st14, scen$R14, \"T14\", scen_local = scen, block_m = bs, models = mods)\n  run05 &lt;- run_for_time(st05, scen$R05, \"T05\", scen_local = scen, block_m = bs, models = mods)\n  \n  cat(\"\\n== Metrics T14 ==\\n\"); print(run14$res$metrics)\n  cat(\"\\n== Metrics T05 ==\\n\"); print(run05$res$metrics)\n  \n  print(run14$res$blocks_plot); print(run14$res$diag_plot)\n  print(run05$res$blocks_plot); print(run05$res$diag_plot)\n  print(run14$maps$p_truth);    print(run14$maps$p_pred)\n  print(run05$maps$p_truth);    print(run05$maps$p_pred)\n  \n  # =====================================================================\n  # 6) SCALE → R* tuning → tuned CV + maps\n  #\n  # Pipeline rationale:\n  #   (a) Variogram reveals correlation ranges in the point field.\n  #   (b) U-curve scans DEM-smoothing radii ~ [L50, L95] to find data-driven R*.\n  #       Each radius implies a different \"macro-signal\" (E*, slope*, cosi*).\n  #       We refit CV at each R and pick the RMSE-minimizer (R*).\n  #   (c) With R* in hand, we derive feature rasters at that scale: E*, slp*, cosi*.\n  #   (d) Re-extract station features at R* to ensure training/prediction consistency.\n  #   (e) Run LBO-CV again (tuned) using E* as the reference raster for blocks/domain.\n  #   (f) Predict tuned maps by injecting the feature rasters.\n  #   (g) Build compact multi-model panels with residual diagnostics.\n  #\n  # Performance tips if tuning feels slow:\n  #   - Reduce n_grid in the U-curve (e.g., 5 instead of 9).\n  #     n_grid sets how many candidate smoothing radii are tested in the U-curve search for R*\n  #   - Trim `mods` to a smaller set while teaching the concept.\n  #   - Increase block size slightly (fewer blocks → fewer CV folds).\n  # =====================================================================\n  \n  # --- (a) Variogram → L50/L95 -------------------------------------------------\n  Ls14 &lt;- compute_Ls_from_points(st14, value_col = \"temp\")\n  Ls05 &lt;- compute_Ls_from_points(st05, value_col = \"temp\")\n  \n  p_vg14 &lt;- plot_variogram_with_scales(Ls14$vg, Ls14$L50, Ls14$L95, Ls14$sill,\n                                       \"T14 — empirical variogram\")\n  p_vg05 &lt;- plot_variogram_with_scales(Ls05$vg, Ls05$L50, Ls05$L95, Ls05$sill,\n                                       \"T05 — empirical variogram\")\n  print(p_vg14); print(p_vg05)\n  \n  # --- (b) U-curve → R* --------------------------------------------------------\n  # We pass *explicit* sun angles so tune_Rstar_ucurve() can build cosi@R\n  # consistently with the scenario's solar geometry.\n  tune14 &lt;- tune_Rstar_ucurve(\n    stn_sf = st14,\n    E      = scen$E,\n    alt    = scen$sun$T14$alt,\n    az     = scen$sun$T14$az,\n    L50    = Ls14$L50,\n    L95    = Ls14$L95,\n    block_fallback = bs,\n    n_grid = 6\n  )\n  \n  tune05 &lt;- tune_Rstar_ucurve(\n    stn_sf = st05,\n    E      = scen$E,\n    alt    = scen$sun$T05$alt,\n    az     = scen$sun$T05$az,\n    L50    = Ls05$L50,\n    L95    = Ls05$L95,\n    block_fallback = bs,\n    n_grid = 6\n  )\n  \n  # Plot the U-curves and report chosen R* (rounded for readability).\n  p_uc14 &lt;- plot_ucurve(tune14$grid, tune14$R_star, \"T14 — U-curve\")\n  p_uc05 &lt;- plot_ucurve(tune05$grid, tune05$R_star, \"T05 — U-curve\")\n  print(p_uc14); print(p_uc05)\n  \n  # IMPORTANT: use %.0f (not %d) because R* is numeric (may be non-integer).\n  message(sprintf(\"Chosen R* — T14: %.0f m | blocks ≈ %d m\", tune14$R_star, tune14$block_m))\n  message(sprintf(\"Chosen R* — T05: %.0f m | blocks ≈ %d m\", tune05$R_star, tune05$block_m))\n  \n  # --- (c) Feature rasters @R* -------------------------------------------------\n  # Smooth DEM at R* and derive slope/incident-cosine given the scenario sun angles.\n  fr14 &lt;- smooth_dem_and_derive(\n    scen$E, scen$sun$T14$alt, scen$sun$T14$az, radius_m = tune14$R_star\n  )\n  fr05 &lt;- smooth_dem_and_derive(\n    scen$E, scen$sun$T05$alt, scen$sun$T05$az, radius_m = tune05$R_star\n  )\n  \n  # --- (d) Station features @R* ------------------------------------------------\n  # Re-extract E*, slope*, cosi* (plus consistent LC factors) at station points.\n  # This keeps training features aligned with the tuned raster features.\n  st14_R &lt;- add_drifts_at_R(\n    st14, scen$E, scen$sun$T14$alt, scen$sun$T14$az, tune14$R_star,\n    lc = scen$lc, lc_levels = scen$lc_levels,\n    na_action = \"fill\"   # or \"drop\" if you prefer to omit affected stations\n  )\n  st05_R &lt;- add_drifts_at_R(\n    st05, scen$E, scen$sun$T05$alt, scen$sun$T05$az, tune05$R_star,\n    lc = scen$lc, lc_levels = scen$lc_levels,\n    na_action = \"fill\"   # or \"drop\" if you prefer to omit affected stations\n  )\n  \n  # --- (e) LBO-CV @R* ----------------------------------------------------------\n  # Use the tuned smoothed DEM (E*) as the reference for CV blocks and domain\n  # geometry so the CV respects the working resolution/scale of the model.\n  bench14 &lt;- run_lbo_cv(st14_R, E = scen$E, block_size = bs, models = mods)\n  bench05 &lt;- run_lbo_cv(st05_R, E = scen$E, block_size = bs, models = mods)\n  print(bench14$metrics); print(bench05$metrics)\n  \n  # --- (f) Tuned maps ----------------------------------------------------------\n  # Inject the tuned feature rasters so model predictions operate at R* scale.\n  maps14_tuned &lt;- predict_maps(\n    stn_sf = st14_R, truth_raster = scen$R14, which_time = \"T14\",\n    scen = scen, models = mods, lc_levels = scen$lc_levels,\n    feature_rasters = list(E = fr14$Es, slp = fr14$slp, cosi = fr14$cosi)\n  )\n  maps05_tuned &lt;- predict_maps(\n    stn_sf = st05_R, truth_raster = scen$R05, which_time = \"T05\",\n    scen = scen, models = mods, lc_levels = scen$lc_levels,\n    feature_rasters = list(E = fr05$Es, slp = fr05$slp, cosi = fr05$cosi)\n  )\n  \n  # --- (g) Panels: truth | predictions | residual diagnostics ------------------\n  panel_T14 &lt;- build_panels_truth_preds_errors_paged(\n    maps = maps14_tuned, truth_raster = scen$R14, cv_tbl = bench14$cv,\n    which_time = \"T14\", models_per_page = 7, scatter_next_to_truth = TRUE\n  )\n  panel_T05 &lt;- build_panels_truth_preds_errors_paged(\n    maps = maps05_tuned, truth_raster = scen$R05, cv_tbl = bench05$cv,\n    which_time = \"T05\", models_per_page = 7, scatter_next_to_truth = TRUE\n  )\n  print(panel_T14[[1]]); print(panel_T05[[1]])\n  \n  \n  # Sensor noise (°C) – from specs or co-location\n  sigma_inst &lt;- 0.5\n  \n  # α from residual variogram (microscale share) – T14 and T05\n  alpha14 &lt;- nugget_fraction_from_cv(bench14$cv, model = \"RF\", crs_ref = st14)\n  alpha05 &lt;- nugget_fraction_from_cv(bench05$cv, model = \"RF\", crs_ref = st05)\n  \n  # Fallbacks if the fit fails\n  if (!is.finite(alpha14)) alpha14 &lt;- 0.6\n  if (!is.finite(alpha05)) alpha05 &lt;- 0.6\n  \n  # Fehlerbudgets berechnen (Base = runXX$res, Tuned = benchXX)\n  eb14_base  &lt;- simple_error_budget(run14$res, sigma_inst, alpha14) |&gt;\n    dplyr::mutate(Time = \"T14\", Mode = \"Base\")\n  eb05_base  &lt;- simple_error_budget(run05$res, sigma_inst, alpha05) |&gt;\n    dplyr::mutate(Time = \"T05\", Mode = \"Base\")\n  eb14_tuned &lt;- simple_error_budget(bench14,   sigma_inst, alpha14) |&gt;\n    dplyr::mutate(Time = \"T14\", Mode = \"Tuned\")\n  eb05_tuned &lt;- simple_error_budget(bench05,   sigma_inst, alpha05) |&gt;\n    dplyr::mutate(Time = \"T05\", Mode = \"Tuned\")\n  \n  eb_all &lt;- dplyr::bind_rows(eb14_base, eb05_base, eb14_tuned, eb05_tuned) |&gt;\n    dplyr::relocate(Time, Mode)\n  \n  print(eb_all)\n  \n  # einfache Stacked-Bar-Plot-Funktion\n  plot_error_budget &lt;- function(df) {\n    d &lt;- df |&gt;\n      dplyr::filter(Component %in% c(\"Instrument var\",\"Microscale var\",\"Mesoscale var\"))\n    ggplot2::ggplot(d,\n                    ggplot2::aes(x = interaction(Time, Mode, sep = \" \"), y = Value, fill = Component)\n    ) +\n      ggplot2::geom_col(position = \"stack\") +\n      ggplot2::theme_minimal() +\n      ggplot2::labs(x = NULL, y = \"Variance (°C²)\", title = \"Error budget by time & mode\")\n  }\n  p_eb &lt;- plot_error_budget(eb_all)\n  print(p_eb)\n  # =====================================================================\n  # 7) Optional: save everything at the end (plots + tables + rasters)\n  #    - Change `export &lt;- FALSE` at the top to only run/plot interactively\n  #    - We wrap saves in try() so a single failed save does not abort the run.\n  # =====================================================================\n  if (export) {\n    # ---------- Ausgabe-Verzeichnis: results_&lt;scen-name&gt; ----------\n    out_root &lt;- here::here(\"block4_5\")\n    out_dir  &lt;- file.path(out_root, sprintf(\"results_%s\", scen_name))\n    fig_dir  &lt;- file.path(out_dir, \"fig\")\n    tab_dir  &lt;- file.path(out_dir, \"tab\")\n    ras_dir  &lt;- file.path(out_dir, \"ras\")\n    # ohne Rückfrage, rekursiv, ohne Warnungen\n    dir.create(fig_dir, recursive = TRUE, showWarnings = FALSE)\n    dir.create(tab_dir, recursive = TRUE, showWarnings = FALSE)\n    dir.create(ras_dir, recursive = TRUE, showWarnings = FALSE)\n    \n  \n    \n    # ---------- Baseline: Previews & CV-Plots ----------------------\n    save_plot_min(plot_landcover_terrain(scen, stations = st14, layout = \"vertical\"),\n                  fn_fig(\"landcover_terrain\"))\n    save_plot_min(plot_block_overview_2x2_en(scen, pts_sf = st14), fn_fig(\"overview_2x2\"))\n    save_plot_min(run14$res$blocks_plot, fn_fig(\"T14_blocks\"))\n    save_plot_min(run14$res$diag_plot,   fn_fig(\"T14_diag\"))\n    save_plot_min(run05$res$blocks_plot, fn_fig(\"T05_blocks\"))\n    save_plot_min(run05$res$diag_plot,   fn_fig(\"T05_diag\"))\n    save_plot_min(run14$maps$p_truth,    fn_fig(\"T14_truth\"))\n    save_plot_min(run14$maps$p_pred,     fn_fig(\"T14_pred\"))\n    save_plot_min(run05$maps$p_truth,    fn_fig(\"T05_truth\"))\n    save_plot_min(run05$maps$p_pred,     fn_fig(\"T05_pred\"))\n    \n    # --- Tuned Panels ---\n    save_plot_min(panel_T14[[1]], fn_fig(\"T14_panel_tuned\"))\n    save_plot_min(panel_T05[[1]], fn_fig(\"T05_panel_tuned\"))\n    \n    # --- Raster ---\n    save_raster_min(scen$E,   fn_ras(\"E_dem\"))\n    save_raster_min(scen$R14, fn_ras(\"R14_truth\"))\n    save_raster_min(scen$R05, fn_ras(\"R05_truth\"))\n    if (\"lc\" %in% names(scen)) save_raster_min(scen$lc, fn_ras(\"landcover\"))\n    # ---------- Scale inference + tuned panels ----------------------\n    safe_save_plot(p_vg14, fn_fig(\"T14_variogram\"))\n    safe_save_plot(p_vg05, fn_fig(\"T05_variogram\"))\n    safe_save_plot(p_uc14, fn_fig(\"T14_ucurve\"))\n    safe_save_plot(p_uc05, fn_fig(\"T05_ucurve\"))\n    safe_save_plot(panel_T14[[1]], fn_fig(\"T14_panel_tuned\"))\n    safe_save_plot(panel_T05[[1]], fn_fig(\"T05_panel_tuned\"))\n    \n    save_table_readable(bench14$metrics, \"metrics_T14_tuned\", \"Tuned metrics — T14\")\n    save_table_readable(bench05$metrics, \"metrics_T05_tuned\", \"Tuned metrics — T05\")\n    save_table_readable(tune14$grid,     \"Ucurve_T14\",       \"U-curve grid — T14\")\n    save_table_readable(tune05$grid,     \"Ucurve_T05\",       \"U-curve grid — T05\")\n    save_table_readable(data.frame(L50 = Ls14$L50, L95 = Ls14$L95, R_star = tune14$R_star),\n                        \"scales_T14\", \"Scales — T14 (L50/L95/R*)\")\n    save_table_readable(data.frame(L50 = Ls05$L50, L95 = Ls05$L95, R_star = tune05$R_star),\n                        \"scales_T05\", \"Scales — T05 (L50/L95/R*)\")\n    save_table_readable(run14$res$metrics, file.path(tab_dir, sprintf(\"metrics_T14_%s\", scen_name)))\n    save_table_readable(run05$res$metrics, file.path(tab_dir, sprintf(\"metrics_T05_%s\", scen_name)))\n    save_table_readable(bench14$metrics,   file.path(tab_dir, sprintf(\"metrics_T14_tuned_%s\", scen_name)))\n    save_table_readable(bench05$metrics,   file.path(tab_dir, sprintf(\"metrics_T05_tuned_%s\", scen_name)))\n    save_table_readable(eb_all,            file.path(tab_dir, sprintf(\"error_budget_%s\", scen_name)))\n    #\n    # ---------- Raster mit Szenario-Präfix --------------------------\n    try(terra::writeRaster(scen$E,   fn_ras(\"E_dem\")),     silent = TRUE)\n    try(terra::writeRaster(scen$R14, fn_ras(\"R14_truth\")), silent = TRUE)\n    try(terra::writeRaster(scen$R05, fn_ras(\"R05_truth\")), silent = TRUE)\n    if (\"lc\" %in% names(scen))\n      try(terra::writeRaster(scen$lc, fn_ras(\"landcover\")), silent = TRUE)\n    \n    # ---------- Sessioninfo -----------------------------------------\n    try(saveRDS(sessionInfo(), file.path(out_dir, sprintf(\"%s_sessionInfo.rds\", scen_name))),\n        silent = TRUE)\n    \n    message(\"✔ Exports written to: \", normalizePath(out_dir, winslash = \"/\"))\n  }\n```\n\n\n\nHelpers / Core library:\n\npackages.R: centralized package loading and global run options (e.g., sf_use_s2(FALSE), seeds).\n\n\n\n\nCode\n```{r}\n#| eval: false\n# --- Paketliste an EINER Stelle pflegen ---------------------------------\n.req_pkgs &lt;- list(\n  core      = c(\"terra\",\"sf\",\"suncalc\",\"gstat\"),\n  modeling  = c(\"randomForest\",\"mgcv\"),\n  wrangling = c(\"dplyr\",\"tibble\",\"tidyr\"),\n  viz       = c(\"ggplot2\",\"scales\",\"patchwork\",\"RColorBrewer\"),\n  report    = c(\"knitr\",\"kableExtra\",\"here\",\"zoo\", \"gt\", \"openxlsx\", \"writexl\")\n\n)\n\nensure_packages &lt;- function(pkgs = unlist(.req_pkgs)) {\n  inst &lt;- rownames(installed.packages())\n  missing &lt;- setdiff(pkgs, inst)\n  if (length(missing)) install.packages(missing, dependencies = TRUE)\n  invisible(lapply(pkgs, require, character.only = TRUE))\n}\n\nafter_load &lt;- function() {\n  if (requireNamespace(\"sf\", quietly = TRUE)) sf::sf_use_s2(FALSE)  # wie bisher\n}\n\n# Aufruf:\nensure_packages()\nafter_load()\n\n# ---- Pfade ------------------------------------------------------------\nbase_dir &lt;- tryCatch(here::here(), error = function(e) getwd())\nsrc_dir  &lt;- file.path(base_dir, \"block4_5\", \"src\")\nout_dir  &lt;- file.path(base_dir, \"block4_5\", \"exports\")\nfig_dir  &lt;- file.path(out_dir, \"figs\")\ntab_dir  &lt;- file.path(out_dir, \"tables\")\nras_dir  &lt;- file.path(out_dir, \"rasters\")\ndat_dir  &lt;- file.path(out_dir, \"data\")\n\ndir.create(out_dir, showWarnings = FALSE, recursive = TRUE)\nfor (d in c(fig_dir, tab_dir, ras_dir, dat_dir)) dir.create(d, showWarnings = FALSE, recursive = TRUE)\n```\n\n\n-   `fun_pipemodel.R`: domain modeling utilities (plots, feature\n    derivation, variogram utilities, U-curve tuning, panels, saving\n    helpers).\n\n\nCode\n```{r}\n#| eval: false\n\n## ======================================================================\n## pipemodel_functions.R  —  nur Funktionen, keine Seiteneffekte\n## ======================================================================\n# ========================= I/O helpers (tables & plots) ======================\n\n\n# ---- Export-Helper (einfügen NACH out_dir/fig_dir/tab_dir/ras_dir) ----\nfn_fig &lt;- function(stem, ext = \"png\") file.path(fig_dir, sprintf(\"%s.%s\", stem, ext))\nfn_ras &lt;- function(stem, ext = \"tif\") file.path(ras_dir, sprintf(\"%s.%s\", stem, ext))\n\nsave_plot_min &lt;- function(p, file, width = 9, height = 6, dpi = 150, bg = \"white\") {\n  # Speichert ggplot ODER \"druckbare\" Plot-Objekte\n  dir.create(dirname(file), recursive = TRUE, showWarnings = FALSE)\n  if (inherits(p, \"ggplot\")) {\n    ggplot2::ggsave(filename = file, plot = p, width = width, height = height, dpi = dpi, bg = bg)\n  } else {\n    grDevices::png(filename = file, width = width, height = height, units = \"in\", res = dpi, bg = bg)\n    print(p)\n    grDevices::dev.off()\n  }\n  invisible(normalizePath(file, winslash = \"/\"))\n}\n\nsafe_save_plot &lt;- function(p, file, width = 9, height = 6, dpi = 150, bg = \"white\") {\n  try(save_plot_min(p, file, width, height, dpi, bg), silent = TRUE)\n}\n\nsave_raster_min &lt;- function(r, file, overwrite = TRUE) {\n  dir.create(dirname(file), recursive = TRUE, showWarnings = FALSE)\n  terra::writeRaster(r, file, overwrite = overwrite)\n  invisible(normalizePath(file, winslash = \"/\"))\n}\n\n\n# Save a table in CSV (+ optional HTML via gt, XLSX via openxlsx/writexl)\n# Robust to tibbles, list cols (ignored), and mistaken positional args.\n# Save a table as CSV (always), HTML (if gt is installed), and XLSX\n# file_stem: full path without extension, e.g. fn_tab(\"metrics_T14_base\")\nsave_table_readable &lt;- function(df, file_stem,\n                                title = NULL,\n                                digits = 3,\n                                make_dirs = TRUE,\n                                verbose = FALSE) {\n  if (!inherits(df, \"data.frame\")) df &lt;- as.data.frame(df)\n  \n  # Drop list-cols so write.csv/openxlsx don't choke\n  is_listcol &lt;- vapply(df, is.list, logical(1))\n  if (any(is_listcol)) df &lt;- df[ , !is_listcol, drop = FALSE]\n  \n  if (isTRUE(make_dirs)) dir.create(dirname(file_stem), showWarnings = FALSE, recursive = TRUE)\n  \n  # Round numeric columns safely\n  numcols &lt;- vapply(df, is.numeric, TRUE)\n  if (any(numcols)) {\n    for (nm in names(df)[numcols]) df[[nm]] &lt;- round(df[[nm]], digits)\n  }\n  \n  paths &lt;- list()\n  \n  ## CSV\n  csv_path &lt;- paste0(file_stem, \".csv\")\n  utils::write.csv(df, csv_path, row.names = FALSE)\n  paths$csv &lt;- csv_path\n  \n  ## HTML via gt (optional)\n  if (requireNamespace(\"gt\", quietly = TRUE)) {\n    gt_tbl &lt;- gt::gt(df)\n    if (!is.null(title)) gt_tbl &lt;- gt::tab_header(gt_tbl, title = title)\n    gt::gtsave(gt_tbl, paste0(file_stem, \".html\"))\n    paths$html &lt;- paste0(file_stem, \".html\")\n  } else if (verbose) {\n    message(\"[save_table_readable] Package 'gt' not installed → skipping HTML.\")\n  }\n  \n  ## XLSX via openxlsx (preferred) or writexl (fallback)\n  xlsx_path &lt;- paste0(file_stem, \".xlsx\")\n  if (requireNamespace(\"openxlsx\", quietly = TRUE)) {\n    wb &lt;- openxlsx::createWorkbook()\n    openxlsx::addWorksheet(wb, \"table\")\n    \n    # Optional title in A1, style it a bit\n    start_row &lt;- 1L\n    if (!is.null(title)) {\n      openxlsx::writeData(wb, \"table\", title, startRow = start_row, startCol = 1)\n      # bold, bigger font for title\n      st &lt;- openxlsx::createStyle(textDecoration = \"bold\", fontSize = 14)\n      openxlsx::addStyle(wb, \"table\", st, rows = start_row, cols = 1, gridExpand = TRUE, stack = TRUE)\n      start_row &lt;- start_row + 2L  # blank row after title\n    }\n    \n    openxlsx::writeData(wb, \"table\", df, startRow = start_row, startCol = 1)\n    openxlsx::saveWorkbook(wb, xlsx_path, overwrite = TRUE)\n    paths$xlsx &lt;- xlsx_path\n  } else if (requireNamespace(\"writexl\", quietly = TRUE)) {\n    writexl::write_xlsx(df, xlsx_path)\n    paths$xlsx &lt;- xlsx_path\n  } else if (verbose) {\n    message(\"[save_table_readable] Neither 'openxlsx' nor 'writexl' installed → skipping XLSX.\")\n  }\n  \n  invisible(paths)\n}\n\n\n\n#' Save a ggplot/patchwork safely (no-op if not a plot)\n#'\n#' @param p A ggplot/patchwork object.\n#' @param file Output path (with extension, e.g. \\code{.png}).\n#' @param width,height Figure size in inches.\n#' @param dpi Resolution in dots per inch.\n#' @param bg Background color (default \\code{\"white\"}).\n#' @keywords io export plot\nsave_plot_safe &lt;- function(p, file, width = 9, height = 6, dpi = 300, bg = \"white\") {\n  if (inherits(p, c(\"gg\", \"ggplot\", \"patchwork\"))) {\n    dir.create(dirname(file), showWarnings = FALSE, recursive = TRUE)\n    try(ggplot2::ggsave(file, p, width = width, height = height, dpi = dpi, bg = bg),\n        silent = TRUE)\n  }\n}\n# =============================================================================\n\norder_models_by_median_rmse &lt;- function(cv_tbl) {\n  bm &lt;- block_metrics_long(cv_tbl)\n  bm |&gt;\n    dplyr::filter(Metric == \"RMSE\") |&gt;\n    dplyr::group_by(model) |&gt;\n    dplyr::summarise(med = stats::median(Value, na.rm = TRUE), .groups = \"drop\") |&gt;\n    dplyr::arrange(med) |&gt;\n    dplyr::pull(model)\n}\n\n# Block-wise metrics (RMSE, MAE)\nblock_metrics_long &lt;- function(cv_tbl) {\n  stopifnot(all(c(\"model\",\"block_id\",\"obs\",\"pred\") %in% names(cv_tbl)))\n  cv_tbl |&gt;\n    dplyr::group_by(model, block_id) |&gt;\n    dplyr::summarise(\n      RMSE = sqrt(mean((obs - pred)^2, na.rm = TRUE)),\n      MAE  = mean(abs(obs - pred), na.rm = TRUE),\n      .groups = \"drop\"\n    ) |&gt;\n    tidyr::pivot_longer(c(RMSE, MAE), names_to = \"Metric\", values_to = \"Value\")\n}\nmake_block_metric_box &lt;- function(cv_tbl, which_time = \"T14\", tail_cap = 0.995) {\n  bm &lt;- block_metrics_long(cv_tbl) |&gt;\n    dplyr::filter(is.finite(Value))\n  if (!is.null(tail_cap)) {\n    ymax &lt;- stats::quantile(bm$Value, tail_cap, na.rm = TRUE)\n  }\n  lev &lt;- order_models_by_median_rmse(cv_tbl)\n  bm$model &lt;- factor(bm$model, levels = lev)\n  \n  ggplot2::ggplot(bm, ggplot2::aes(model, Value)) +\n    ggplot2::geom_boxplot(outlier.alpha = 0.35, width = 0.7) +\n    ggplot2::stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3,\n                          fill = \"white\", colour = \"black\", stroke = 0.5) +\n    ggplot2::coord_cartesian(ylim = c(0, ymax)) +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(title = sprintf(\"%s — Block-wise errors (LBO-CV)\", which_time),\n                  subtitle = \"Box = IQR · line = median · ◆ = mean\",\n                  x = \"Model\", y = \"Error\") +\n    ggplot2::facet_wrap(~ Metric, scales = \"free_y\")\n}\n\nmake_abs_error_box &lt;- function(cv_tbl, which_time = \"T14\", tail_cap = 0.995) {\n  df &lt;- cv_tbl |&gt;\n    dplyr::mutate(abs_err = abs(pred - obs)) |&gt;\n    dplyr::filter(is.finite(abs_err))\n  ymax &lt;- if (!is.null(tail_cap)) stats::quantile(df$abs_err, tail_cap, na.rm = TRUE) else max(df$abs_err, na.rm = TRUE)\n  lev &lt;- df |&gt;\n    dplyr::group_by(model) |&gt;\n    dplyr::summarise(med = stats::median(abs_err, na.rm = TRUE), .groups = \"drop\") |&gt;\n    dplyr::arrange(med) |&gt;\n    dplyr::pull(model)\n  df$model &lt;- factor(df$model, levels = lev)\n  \n  ggplot2::ggplot(df, ggplot2::aes(model, abs_err)) +\n    ggplot2::geom_boxplot(outlier.alpha = 0.3, width = 0.7) +\n    ggplot2::stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3,\n                          fill = \"white\", colour = \"black\", stroke = 0.5) +\n    ggplot2::coord_cartesian(ylim = c(0, ymax)) +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(title = sprintf(\"%s — Absolute errors per station (LBO-CV)\", which_time),\n                  subtitle = \"Box = IQR · line = median · ◆ = mean\",\n                  x = \"Model\", y = \"|pred − obs|\")\n}\n\n\nmake_obs_pred_scatter &lt;- function(cv_tbl, which_time = \"T14\") {\n  lab &lt;- .make_labeller(cv_tbl)\n  ggplot(cv_tbl, aes(obs, pred)) +\n    geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n    geom_point(alpha = 0.7, shape = 16) +\n    coord_equal() + theme_minimal() +\n    labs(title = sprintf(\"%s — Observed vs Predicted (LBO-CV)\", which_time), x = \"Observed\", y = \"Predicted\") +\n    facet_wrap(~ model, ncol = 3, labeller = ggplot2::as_labeller(lab))\n}\n\nmake_residual_density &lt;- function(cv_tbl, which_time = \"T14\") {\n  cv_tbl |&gt; dplyr::mutate(resid = pred - obs) |&gt; ggplot2::ggplot(ggplot2::aes(resid, fill = model)) +\n    ggplot2::geom_density(alpha = 0.4) + ggplot2::theme_minimal() +\n    ggplot2::labs(title = sprintf(\"%s — Residual density\", which_time), x = \"Residual (°C)\", y = \"Density\")\n}\n\n# Prediction maps & error panels ---------------------------------------\n.make_labeller &lt;- function(cv_tbl) {\n  m &lt;- cv_tbl |&gt;\n    dplyr::group_by(model) |&gt;\n    dplyr::summarise(RMSE = sqrt(mean((obs - pred)^2, na.rm = TRUE)), MAE  = mean(abs(obs - pred), na.rm = TRUE), .groups = \"drop\")\n  setNames(sprintf(\"%s  (RMSE=%.2f · MAE=%.2f)\", m$model, m$RMSE, m$MAE), m$model)\n}\n.plot_raster_gg &lt;- function(r, title = \"\", palette = temp_palette, q = c(0.02,0.98), lims = NULL) {\n  stopifnot(terra::nlyr(r) == 1)\n  df &lt;- as.data.frame(r, xy = TRUE, na.rm = FALSE)\n  nm &lt;- names(df)[3]\n  if (is.null(lims)) {\n    vv &lt;- terra::values(r, na.rm = TRUE)\n    lims &lt;- stats::quantile(vv, probs = q, na.rm = TRUE, names = FALSE)\n  }\n  ggplot2::ggplot(df, ggplot2::aes(.data$x, .data$y, fill = .data[[nm]])) +\n    ggplot2::geom_raster() +\n    ggplot2::coord_equal() +\n    ggplot2::scale_fill_gradientn(colours = palette, limits = lims, oob = scales::squish) +\n    ggplot2::labs(title = title, x = NULL, y = NULL, fill = \"°C\") +\n    ggplot2::theme_minimal(base_size = 11) +\n    ggplot2::theme(legend.position = \"right\",\n                   plot.title = ggplot2::element_text(face = \"bold\"))\n}\n\n.get_preds_from_maps &lt;- function(maps) {\n  # 1) SpatRaster direkt\n  if (inherits(maps, \"SpatRaster\")) {\n    ul &lt;- terra::unstack(maps)\n    names(ul) &lt;- names(maps)\n    return(ul)\n  }\n  # 2) Liste mit typischen Feldern\n  if (is.list(maps)) {\n    if (!is.null(maps$preds))          return(maps$preds)\n    if (!is.null(maps$pred_rasters))   return(maps$pred_rasters)\n    if (!is.null(maps$pred_stack) && inherits(maps$pred_stack, \"SpatRaster\")) {\n      ul &lt;- terra::unstack(maps$pred_stack); names(ul) &lt;- names(maps$pred_stack); return(ul)\n    }\n    if (!is.null(maps$stack) && inherits(maps$stack, \"SpatRaster\")) {\n      ul &lt;- terra::unstack(maps$stack); names(ul) &lt;- names(maps$stack); return(ul)\n    }\n    if (!is.null(maps$maps) && inherits(maps$maps, \"SpatRaster\")) {\n      ul &lt;- terra::unstack(maps$maps); names(ul) &lt;- names(maps$maps); return(ul)\n    }\n    # 3) Liste, die bereits einzelne SpatRaster oder ggplots enthält\n    cand &lt;- maps[ vapply(maps, function(x) inherits(x, \"SpatRaster\") || inherits(x, \"ggplot\"), logical(1)) ]\n    if (length(cand) &gt; 0) return(cand)\n  }\n  stop(\"build_panels_with_errors(): In 'maps' keine Vorhersagen gefunden.\")\n}\n# --- Kartenplot mit optionalen Achsenticks/labels ----------------------\n.plot_map_axes &lt;- function(r, title, cols, lims, q = c(0.02,0.98),\n                           base_size = 14, tick_n = 5,\n                           show_axis_labels = FALSE, show_axis_ticks = TRUE) {\n  stopifnot(terra::nlyr(r) == 1)\n  df &lt;- as.data.frame(r, xy = TRUE, na.rm = FALSE)\n  nm &lt;- names(df)[3]\n  \n  if (is.null(lims) || !all(is.finite(lims)) || lims[1] &gt;= lims[2]) {\n    vv &lt;- terra::values(r, na.rm = TRUE)\n    lims &lt;- stats::quantile(vv, probs = q, na.rm = TRUE, names = FALSE)\n    if (!all(is.finite(lims)) || lims[1] == lims[2]) lims &lt;- range(vv, na.rm = TRUE) + c(-1e-6, 1e-6)\n  }\n  if (is.function(cols)) cols &lt;- cols(256)\n  if (!is.atomic(cols) || length(cols) &lt; 2) cols &lt;- grDevices::hcl.colors(256, \"YlOrRd\")\n  \n  ggplot2::ggplot(df, ggplot2::aes(x, y, fill = .data[[nm]])) +\n    ggplot2::geom_raster() +\n    ggplot2::coord_equal(expand = FALSE) +\n    ggplot2::scale_x_continuous(expand = c(0,0), breaks = scales::breaks_pretty(n = tick_n)) +\n    ggplot2::scale_y_continuous(expand = c(0,0), breaks = scales::breaks_pretty(n = tick_n)) +\n    ggplot2::scale_fill_gradientn(colours = cols, limits = lims, oob = scales::squish) +\n    ggplot2::labs(title = title, x = NULL, y = NULL, fill = \"°C\") +\n    ggplot2::theme_minimal(base_size = base_size) +\n    ggplot2::theme(\n      legend.position = \"right\",\n      plot.title = ggplot2::element_text(face = \"bold\"),\n      axis.title   = ggplot2::element_blank(),\n      axis.text    = if (show_axis_labels) ggplot2::element_text(size = base_size - 3) else ggplot2::element_blank(),\n      axis.ticks   = if (show_axis_ticks)  ggplot2::element_line(linewidth = 0.25) else ggplot2::element_blank(),\n      panel.border = ggplot2::element_rect(fill = NA, colour = \"grey40\", linewidth = .4)\n    )\n}\nbuild_panels_truth_preds_errors_paged &lt;- function(\n    maps, truth_raster, cv_tbl, which_time,\n    models_per_page = 4,\n    model_order      = NULL,\n    temp_pal         = temp_palette,     # Vektor ODER Funktion -&gt; wird zu Vektor\n    stretch_q        = c(0.02, 0.98),\n    errors_height    = 1.2,\n    scatter_next_to_truth = TRUE,        # Scatter rechts von Truth?\n    top_widths       = c(1.1, 0.9),      # Breitenverhältnis Truth | Scatter\n    show_second_legend = FALSE           # zweite °C-Legende bei den Preds unterdrücken\n) {\n  stopifnot(length(stretch_q) == 2)\n  if (is.function(temp_pal)) temp_pal &lt;- temp_pal(256)\n  stopifnot(is.atomic(temp_pal), length(temp_pal) &gt;= 2)\n  \n  # Vorhersagen einsammeln / Reihenfolge\n  preds_raw  &lt;- .get_preds_from_maps(maps)\n  pred_names &lt;- names(preds_raw) %||% paste0(\"model_\", seq_along(preds_raw))\n  if (!is.null(model_order)) {\n    keep &lt;- intersect(model_order, pred_names)\n    if (!length(keep)) stop(\"model_order enthält keine gültigen Modellnamen.\")\n    preds_raw  &lt;- preds_raw[keep]\n    pred_names &lt;- keep\n  }\n  \n  # Gemeinsame Farbskala\n  all_vals &lt;- c(terra::values(truth_raster, na.rm = TRUE))\n  for (p in preds_raw) if (inherits(p, \"SpatRaster\")) all_vals &lt;- c(all_vals, terra::values(p, na.rm = TRUE))\n  lims &lt;- stats::quantile(all_vals, probs = stretch_q, na.rm = TRUE, names = FALSE)\n  if (all(is.finite(lims)) && lims[1] == lims[2]) {\n    eps &lt;- .Machine$double.eps * max(1, abs(lims[1])); lims &lt;- lims + c(-eps, eps)\n  }\n  \n  # Helfer: Raster -&gt; ggplot\n  make_tile &lt;- function(obj, title_txt, show_legend = TRUE) {\n    if (inherits(obj, \"SpatRaster\")) {\n      g &lt;- .plot_raster_gg(obj, title = title_txt, palette = temp_pal, q = stretch_q, lims = lims)\n      if (!show_legend) g &lt;- g + ggplot2::theme(legend.position = \"none\")\n      g\n    } else if (inherits(obj, \"ggplot\")) {\n      obj + ggplot2::labs(title = title_txt)\n    } else stop(\"Nicht unterstützter Prediction-Typ: \", class(obj)[1])\n  }\n  \n  # Truth (+ optional Scatter daneben)\n  p_truth   &lt;- .plot_raster_gg(truth_raster, title = paste0(which_time, \" — truth\"),\n                               palette = temp_pal, q = stretch_q, lims = lims)\n  p_scatter &lt;- make_obs_pred_scatter(cv_tbl, which_time = which_time)\n  \n  # Prediction-Kacheln bauen (nur erste mit °C-Legende falls gewünscht)\n  pred_tiles &lt;- lapply(seq_along(preds_raw), function(i) {\n    show_leg &lt;- if (isTRUE(show_second_legend)) TRUE else (i == 1L)\n    make_tile(preds_raw[[i]], pred_names[i], show_legend = show_leg)\n  })\n  \n  # Paginierung\n  n &lt;- length(pred_tiles)\n  idx_split &lt;- split(seq_len(n), ceiling(seq_len(n) / models_per_page))\n  \n  pages &lt;- lapply(idx_split, function(idx) {\n    preds_row &lt;- patchwork::wrap_plots(pred_tiles[idx], nrow = 1, ncol = length(idx))\n    \n    top_row &lt;- if (isTRUE(scatter_next_to_truth)) {\n      (p_truth | (p_scatter + ggplot2::theme(legend.position = \"none\"))) +\n        patchwork::plot_layout(widths = top_widths)\n    } else {\n      p_truth\n    }\n    \n    # Errors unten: wenn Scatter schon oben, unten nur Dichte\n    p_box_rmse &lt;- make_block_metric_box(cv_tbl, which_time = which_time, tail_cap = 0.995)\n    p_box_ae   &lt;- make_abs_error_box  (cv_tbl, which_time = which_time, tail_cap = 0.995)\n    p_dens     &lt;- make_residual_density(cv_tbl, which_time = which_time)\n    p_errors   &lt;- (p_box_rmse | p_box_ae) / p_dens\n    \n    (top_row / preds_row / p_errors) +\n      patchwork::plot_layout(heights = c(1, 1, errors_height), guides = \"collect\") &\n      ggplot2::theme(legend.position = \"right\")\n  })\n  \n  pages\n}\n.pm_verbose &lt;- function(v = NULL) {\n  if (!is.null(v)) return(isTRUE(v))\n  isTRUE(getOption(\"pipemodel.verbose\", FALSE))\n}\npm_say &lt;- function(fmt, ..., v = NULL) {\n  if (.pm_verbose(v)) message(sprintf(fmt, ...))\n}\n.k_for_xy &lt;- function(n, n_xy) max(3, min(60, n_xy - 1L, floor(n * 0.8)))\n.kcap_unique &lt;- function(x, kmax) {\n  ux &lt;- unique(x[is.finite(x)])\n  nu &lt;- length(ux)\n  if (nu &lt;= 3) return(0L)                # treat as constant/near-constant\n  max(4L, min(kmax, nu - 1L))\n}\n\n`%||%` &lt;- function(a, b) if (!is.null(a)) a else b\n# --- Sun helper (self-contained in the lib) --------------------------\nsun_pos_utc &lt;- function(y, m, d, h, lat, lon) {\n  t  &lt;- as.POSIXct(sprintf(\"%04d-%02d-%02d %02d:00:00\", y, m, d, h), tz = \"UTC\")\n  sp &lt;- suncalc::getSunlightPosition(date = t, lat = lat, lon = lon)\n  list(\n    alt = sp$altitude,\n    az  = (sp$azimuth + pi) %% (2*pi)  # convert to [0, 2π) from north\n  )\n}\n\n# Sun helper: pull sun14/sun05 from scen; else compute; else fallback\n.get_sun &lt;- function(scen, which = c(\"T14\",\"T05\")) {\n  which &lt;- match.arg(which)\n  key   &lt;- if (which == \"T14\") \"sun14\" else \"sun05\"\n  \n  # 1) stored in scen?\n  s &lt;- scen[[key]]\n  if (is.list(s) && is.finite(s$alt) && is.finite(s$az)) return(s)\n  \n  # 2) compute from lat/lon/sun_date if available\n  if (all(c(\"lat\",\"lon\",\"sun_date\") %in% names(scen))) {\n    hour &lt;- if (which == \"T14\") 14L else 5L\n    return(sun_pos_utc(scen$sun_date, hour, scen$lat, scen$lon))\n  }\n  \n  # 3) hard fallback\n  list(alt = if (which == \"T14\") 0.75 else 0.10, az = 0.0)\n}\n\n# -------------------------- Defaults -----------------------------------\nlc_levels_default &lt;- c(\"forest\",\"water\",\"bare soil\",\"meadows\")\nlc_levels &lt;- getOption(\"pipemodel.lc_levels\", lc_levels_default)\n\nlc_colors_default &lt;- c(\n  \"forest\"   = \"#2E8B57\",\n  \"water\"    = \"#5DADE2\",\n  \"bare soil\"= \"#C49A6C\",\n  \"meadows\"  = \"#7FBF7B\"\n)\ntemp_palette &lt;- grDevices::colorRampPalette(c(\"#0000FF\",\"#FF0000\"))\nstretch_q    &lt;- c(0.02, 0.98)\n# Normalize any CRS input to a non-empty character string\nnorm_crs &lt;- function(crs, fallback = \"EPSG:32632\") {\n  # allow sf::crs, numeric EPSG, character EPSG/WKT\n  if (inherits(crs, \"crs\")) {\n    out &lt;- sf::st_crs(crs)$wkt\n  } else if (is.numeric(crs) && length(crs) == 1 && is.finite(crs)) {\n    out &lt;- sprintf(\"EPSG:%d\", as.integer(crs))\n  } else if (is.character(crs) && length(crs) &gt;= 1) {\n    out &lt;- crs[1]\n  } else {\n    out &lt;- NA_character_\n  }\n  if (!length(out) || is.na(out) || identical(out, \"\")) out &lt;- fallback\n  out\n}\n\n# -------------------------- Domain/Template -----------------------------\nmake_domain &lt;- function(center_E, center_N, len_x, len_y, res, crs = \"EPSG:32632\") {\n  crs &lt;- norm_crs(crs)\n  ext &lt;- terra::ext(center_E - len_x/2, center_E + len_x/2,\n                    center_N - len_y/2, center_N + len_y/2)\n  Rtemplate &lt;- terra::rast(ext, resolution = res, crs = crs)\n  list(\n    xmin = terra::xmin(ext), xmax = terra::xmax(ext),\n    ymin = terra::ymin(ext), ymax = terra::ymax(ext),\n    x0 = center_E, y0 = center_N,\n    res = as.numeric(res), crs = crs,\n    Rtemplate = Rtemplate\n  )\n}\n\n\ncompute_block_size &lt;- function(len_x, len_y, n_st,\n                               target_st_per_block = 3,\n                               min_blocks_axis = 3,\n                               round_to = 50) {\n  area &lt;- len_x * len_y\n  B_target &lt;- max(min_blocks_axis^2, round(n_st / target_st_per_block))\n  bs &lt;- sqrt(area / B_target)\n  bs &lt;- min(bs, len_x / min_blocks_axis, len_y / min_blocks_axis)\n  bs &lt;- max(round_to, round(bs / round_to) * round_to)\n  as.integer(bs)\n}\n\n# -------------------------- Sonne/Geometrie -----------------------------\n\n# Cosine of incidence on a slope/aspect for a given sun (radians)\ncosi_fun &lt;- function(alt, az, slp_r, asp_r) {\n  zen &lt;- (pi/2 - alt)\n  ci  &lt;- cos(slp_r)*cos(zen) + sin(slp_r)*sin(zen)*cos(az - asp_r)\n  terra::ifel(ci &lt; 0, 0, ci)\n}\n\n# Sun position at a given UTC date + hour (numeric hour), return radians\nsun_pos_utc &lt;- function(sun_date, hour_utc, lat, lon) {\n  stopifnot(inherits(sun_date, \"Date\"), length(hour_utc) == 1)\n  t  &lt;- as.POSIXct(sprintf(\"%s %02d:00:00\", format(sun_date, \"%Y-%m-%d\"), as.integer(hour_utc)), tz = \"UTC\")\n  sp &lt;- suncalc::getSunlightPosition(date = t, lat = lat, lon = lon)\n  list(\n    alt = as.numeric(sp$altitude),                   # radians\n    az  = as.numeric((sp$azimuth + pi) %% (2*pi))    # convert to [0..2π) from north\n  )\n}\n\n\n# -------------------------- Rauschen ------------------------------------\nmake_noise_pair &lt;- function(template, sd14 = 0.3, sd05 = 0.3,\n                            seed14 = 1001, seed05 = 1002) {\n  set.seed(seed14)\n  n14 &lt;- terra::setValues(terra::rast(template), rnorm(terra::ncell(template), 0, sd14))\n  set.seed(seed05)\n  n05 &lt;- terra::setValues(terra::rast(template), rnorm(terra::ncell(template), 0, sd05))\n  list(noise14 = n14, noise05 = n05)\n}\n\n# -------------------------- Topographie ---------------------------------\nbuild_topography &lt;- function(domain,\n                             lake_mode = c(\"none\",\"water\",\"hollow\"),\n                             hill_mode = c(\"none\",\"bump\"),\n                             lake_diam_m  = 50,  lake_depth_m = 10, smooth_edges = FALSE,\n                             hill_diam_m  = 80,  hill_height_m = 50, hill_smooth  = FALSE) {\n  lake_mode &lt;- match.arg(lake_mode); hill_mode &lt;- match.arg(hill_mode)\n  Rtemplate &lt;- domain$Rtemplate\n  x0 &lt;- domain$x0; y0 &lt;- domain$y0\n  xmin &lt;- domain$xmin; xmax &lt;- domain$xmax\n  len_x &lt;- xmax - xmin; y_hc &lt;- y0\n  x_hc &lt;- xmin + len_x/3; x_lc &lt;- xmin + 2*len_x/3; y_lc &lt;- y0\n  \n  XY &lt;- as.data.frame(terra::xyFromCell(Rtemplate, 1:terra::ncell(Rtemplate)))\n  names(XY) &lt;- c(\"x\",\"y\")\n  dy &lt;- XY$y - y0\n  a  &lt;- 100 / (( (domain$ymax - domain$ymin)/2 )^2)\n  elev &lt;- 500 + a * dy^2\n  \n  # See/Grube\n  rl &lt;- sqrt((XY$x - x_lc)^2 + (XY$y - y_lc)^2); lr &lt;- lake_diam_m/2\n  if (lake_mode %in% c(\"water\",\"hollow\")) {\n    w_l &lt;- if (smooth_edges) pmax(0, 1 - (rl/lr)^2) else as.numeric(rl &lt;= lr)\n    elev &lt;- elev - lake_depth_m * w_l\n  } else w_l &lt;- 0\n  \n  # Haupt-Hügel\n  if (hill_mode == \"bump\") {\n    rh &lt;- sqrt((XY$x - x_hc)^2 + (XY$y - y_hc)^2); hr &lt;- max(1e-6, hill_diam_m/2)\n    w_h &lt;- if (hill_smooth) exp(-(rh/hr)^2) else as.numeric(rh &lt;= hr)\n    elev &lt;- elev + hill_height_m * w_h\n  } else w_h &lt;- 0\n  \n  E &lt;- Rtemplate; terra::values(E) &lt;- elev; names(E) &lt;- \"elev\"\n  lakeR &lt;- Rtemplate; terra::values(lakeR) &lt;- if (lake_mode==\"water\") as.numeric(w_l&gt;0) else 0; names(lakeR) &lt;- \"lake\"\n  hillW &lt;- Rtemplate; terra::values(hillW) &lt;- w_h; names(hillW) &lt;- \"hillW\"\n  \n  slp  &lt;- terra::terrain(E, v=\"slope\",  unit=\"radians\")\n  asp  &lt;- terra::terrain(E, v=\"aspect\", unit=\"radians\")\n  \n  list(E = E, lake = lakeR, hillW = hillW,\n       slp = terra::ifel(is.na(slp), 0, slp),\n       asp = terra::ifel(is.na(asp), 0, asp))\n}\n# --- Sun helpers (UTC) -------------------------------------------------\nsun_pos_utc &lt;- function(date, hour, lat, lon) {\n  t  &lt;- as.POSIXct(sprintf(\"%s %02d:00:00\", as.Date(date), hour), tz = \"UTC\")\n  sp &lt;- suncalc::getSunlightPosition(date = t, lat = lat, lon = lon)\n  # Azimut: 0 = Nord, positiv im Uhrzeigersinn\n  list(alt = sp$altitude, az = (sp$azimuth + pi) %% (2*pi))\n}\n\n# -------------------------- Physikfelder --------------------------------\nbuild_physics_fields &lt;- function(topography, landcover,\n                                 noise14, noise05,\n                                 alpha_I_by_lc = c(\"forest\" = 3.5, \"water\" = 1.5, \"bare soil\" = 6.0, \"meadows\" = 4.0),\n                                 shade_fac_by_lc = c(\"forest\" = 0.60, \"water\" = 1.00, \"bare soil\" = 1.00, \"meadows\" = 0.95),\n                                 dawn_bias_by_lc = c(\"forest\" = 0.30, \"water\" = 1.20, \"bare soil\" = -0.50, \"meadows\" = 0.05),\n                                 pool_fac_by_lc  = c(\"forest\" = 0.70, \"water\" = 0.80, \"bare soil\" = 1.10, \"meadows\" = 1.05),\n                                 pool_block_gain = 0.4,\n                                 sun14 = list(alt = 0.75, az = 0.0),\n                                 sun05 = list(alt = 0.10, az = 0.0))\n                                 {\n  E    &lt;- topography$E\n  slp0 &lt;- topography$slp\n  asp0 &lt;- topography$asp\n  hillW&lt;- topography$hillW\n  \n  # Sonnen-Inzidenz\n  I14 &lt;- cosi_fun(sun14$alt, sun14$az, slp0, asp0)\n  I05 &lt;- cosi_fun(sun05$alt, sun05$az, slp0, asp0)\n  \n  lc &lt;- if (inherits(landcover, \"SpatRaster\")) landcover else landcover$lc\n  stopifnot(inherits(lc, \"SpatRaster\"))\n  \n  v &lt;- as.integer(terra::values(lc))\n  v[!is.finite(v)] &lt;- 1L\n  v &lt;- pmax(1L, pmin(v, length(lc_levels_default)))\n  lc_char &lt;- factor(lc_levels_default[v], levels = lc_levels_default)\n  \n  to_r &lt;- function(x) terra::setValues(terra::rast(E), x)\n  alpha_I &lt;- to_r(as.numeric(alpha_I_by_lc[lc_char]))\n  shade_f &lt;- to_r(as.numeric(shade_fac_by_lc[lc_char]))\n  dawn_b  &lt;- to_r(as.numeric(dawn_bias_by_lc[lc_char]))\n  pool_f  &lt;- to_r(as.numeric(pool_fac_by_lc[lc_char]))\n  \n  I14_eff &lt;- I14 * shade_f\n  \n  E_mean &lt;- terra::global(E, \"mean\", na.rm = TRUE)[1,1]\n  Y &lt;- terra::init(E, \"y\")\n  dist2ax &lt;- abs(Y - (terra::ymax(E)+terra::ymin(E))/2)\n  w_pool &lt;- 70\n  pool_base &lt;- 4.0 * exp(- (dist2ax / w_pool)^2)\n  pool_mod  &lt;- pool_base * (1 - pool_block_gain * hillW) * pool_f\n  \n  T0_14 &lt;- 26.0; lapse_14 &lt;- -0.0065\n  R14 &lt;- T0_14 + lapse_14 * (E - E_mean) + alpha_I * I14_eff + noise14; names(R14) &lt;- \"T14\"\n  \n  T0_05 &lt;- 8.5; inv_05 &lt;- 0.003; eta_slope &lt;- 0.6\n  R05 &lt;- T0_05 + inv_05 * (E - E_mean) + eta_slope * slp0 - pool_mod + dawn_b + noise05; names(R05) &lt;- \"T05\"\n  \n  list(R14 = R14, R05 = R05, I14 = I14, I05 = I05)\n}\n\n# --- Sun + cos(i) helpers (safe to keep once in your lib) --------------------\ncosi_fun &lt;- function(alt, az, slp_r, asp_r) {\n  zen &lt;- (pi/2 - alt)\n  ci  &lt;- cos(slp_r)*cos(zen) + sin(slp_r)*sin(zen)*cos(az - asp_r)\n  terra::ifel(ci &lt; 0, 0, ci)\n}\n\nsun_pos_utc &lt;- function(sun_date, hour_utc, lat, lon) {\n  stopifnot(inherits(sun_date, \"Date\"))\n  t  &lt;- as.POSIXct(sprintf(\"%s %02d:00:00\",\n                           format(sun_date, \"%Y-%m-%d\"),\n                           as.integer(hour_utc)), tz = \"UTC\")\n  sp &lt;- suncalc::getSunlightPosition(date = t, lat = lat, lon = lon)\n  list(\n    alt = as.numeric(sp$altitude),                  # radians\n    az  = as.numeric((sp$azimuth + pi) %% (2*pi))   # [0..2π) from north\n  )\n}\n\nbuild_scenario &lt;- function(\n    domain,\n    lake_mode = c(\"none\",\"water\",\"hollow\"),\n    hill_mode = c(\"none\",\"bump\"),\n    # main hill / lake geometry (meters)\n    lake_diam_m  = 50,  lake_depth_m = 10, smooth_edges = FALSE,\n    hill_diam_m  = 80,  hill_height_m = 50, hill_smooth  = FALSE,\n    # micro-relief (meters)\n    random_hills        = 0,\n    micro_hill_diam_m   = 30,\n    micro_hill_height_m = 50,\n    micro_hill_smooth   = TRUE,\n    micro_seed          = NULL,\n    # sun / geo\n    lat = 51.8, lon = 10.6, sun_date = as.Date(\"2024-06-21\"),\n    # optional noise\n    noise14 = NULL, noise05 = NULL\n) {\n  lake_mode &lt;- match.arg(lake_mode)\n  hill_mode &lt;- match.arg(hill_mode)\n  \n  # --- 0) Template & CRS guard (must be meters) -----------------------\n  ext &lt;- terra::ext(domain$xmin, domain$xmax, domain$ymin, domain$ymax)\n  Rtemplate &lt;- terra::rast(ext, resolution = domain$res, crs = domain$crs)\n  \n  crs_sf &lt;- sf::st_crs(terra::crs(Rtemplate, proj=TRUE))\n  if (isTRUE(sf::st_is_longlat(crs_sf))) {\n    stop(\n      \"build_scenario(): Domain CRS is geographic (degrees). \",\n      \"All geometry is in meters. Use a projected CRS (e.g. UTM / EPSG:32632).\"\n    )\n  }\n  \n  xmin &lt;- terra::xmin(ext); xmax &lt;- terra::xmax(ext)\n  ymin &lt;- terra::ymin(ext); ymax &lt;- terra::ymax(ext)\n  len_x &lt;- xmax - xmin;     len_y &lt;- ymax - ymin\n  x0 &lt;- (xmin + xmax)/2;    y0 &lt;- (ymin + ymax)/2\n  \n  # coordinate rasters\n  X &lt;- terra::init(Rtemplate, \"x\")\n  Y &lt;- terra::init(Rtemplate, \"y\")\n  \n  # quick sanity for scale\n  px &lt;- mean(terra::res(Rtemplate))\n  lr_px &lt;- (lake_diam_m/2) / px\n  hr_px &lt;- (hill_diam_m/2) / px\n  message(sprintf(\"[build_scenario] pixel=%.2f m; lake r=%.1f px; hill r=%.1f px\", px, lr_px, hr_px))\n  \n  # --- 1) Base valley --------------------------------------------------\n  a  &lt;- 100 / ((len_y/2)^2)\n  E  &lt;- 500 + a * (Y - y0)^2\n  names(E) &lt;- \"elev\"\n  \n  # --- 2) Lake (mirror of hill, negative) ------------------------------\n  x_lc &lt;- xmin + 2*len_x/3;  y_lc &lt;- y0\n  lr   &lt;- max(1e-6, lake_diam_m/2)\n  rl   &lt;- sqrt((X - x_lc)^2 + (Y - y_lc)^2)\n  \n  w_l &lt;- if (isTRUE(smooth_edges)) {\n    exp(-(rl/lr)^2)            # Gaussian \"bump\"\n  } else {\n    terra::ifel(rl &lt;= lr, 1, 0) # hard disc\n  }\n  \n  if (lake_mode %in% c(\"water\",\"hollow\")) {\n    E &lt;- E - as.numeric(lake_depth_m) * w_l\n  }\n  lakeR &lt;- if (identical(lake_mode, \"water\")) terra::ifel(w_l &gt; 1e-6, 1L, 0L)\n  else terra::setValues(terra::rast(Rtemplate), 0L)\n  names(lakeR) &lt;- \"lake\"\n  \n  # --- 3) Main hill ----------------------------------------------------\n  x_hc &lt;- xmin + len_x/3;  y_hc &lt;- y0\n  hr   &lt;- max(1e-6, hill_diam_m/2)\n  rh   &lt;- sqrt((X - x_hc)^2 + (Y - y_hc)^2)\n  \n  w_h_main &lt;- if (hill_mode == \"bump\") {\n    if (isTRUE(hill_smooth)) exp(-(rh/hr)^2) else terra::ifel(rh &lt;= hr, 1, 0)\n  } else {\n    0 * X\n  }\n  E &lt;- E + as.numeric(hill_height_m) * w_h_main\n  \n  # --- 4) Micro hills (additive, clamped to 1) ------------------------\n  w_h_micro &lt;- 0 * X\n  if (random_hills &gt; 0) {\n    if (!is.null(micro_seed)) set.seed(micro_seed)\n    margin &lt;- micro_hill_diam_m/2 + 5\n    hrm &lt;- max(1e-6, micro_hill_diam_m/2)\n    for (i in seq_len(random_hills)) {\n      cx &lt;- runif(1, xmin + margin, xmax - margin)\n      cy &lt;- runif(1, ymin + margin, ymax - margin)\n      r  &lt;- sqrt((X - cx)^2 + (Y - cy)^2)\n      wi &lt;- if (isTRUE(micro_hill_smooth)) exp(-(r/hrm)^2) else terra::ifel(r &lt;= hrm, 1, 0)\n      sum_i &lt;- w_h_micro + wi\n      w_h_micro &lt;- terra::ifel(sum_i &gt; 1, 1, sum_i)  # clamp without pmin()\n    }\n    E &lt;- E + as.numeric(micro_hill_height_m) * w_h_micro\n  }\n  \n  hillW &lt;- w_h_main + w_h_micro\n  hillW &lt;- terra::ifel(hillW &gt; 1, 1, hillW); names(hillW) &lt;- \"hillW\"\n  \n  # --- 5) Derivatives --------------------------------------------------\n  slp &lt;- terra::terrain(E, v = \"slope\",  unit = \"radians\")\n  asp &lt;- terra::terrain(E, v = \"aspect\", unit = \"radians\")\n  \n  # --- 6) Sun & cos(i) -------------------------------------------------\n  sun14 &lt;- sun_pos_utc(sun_date, 14L, lat, lon)\n  sun05 &lt;- sun_pos_utc(sun_date,  5L, lat, lon)\n  I14   &lt;- cosi_fun(sun14$alt, sun14$az, slp, asp); names(I14) &lt;- \"I14\"\n  I05   &lt;- cosi_fun(sun05$alt, sun05$az, slp, asp); names(I05) &lt;- \"I05\"\n  \n  # --- 7) Land cover (1 forest, 2 water, 3 bare, 4 meadows) -----------\n  lc &lt;- terra::setValues(terra::rast(Rtemplate), 4L)  # meadows\n  lc &lt;- terra::ifel(lakeR &gt; 0, 2L, lc)                # water overrides\n  forest_mask &lt;- terra::ifel((hillW &gt; 0.2) | ((slp &gt; 0.15) & (Y &gt; y0)), 1, 0)\n  lc &lt;- terra::ifel((forest_mask == 1) & (lakeR &lt;= 0), 1L, lc)\n  v_slp   &lt;- terra::values(slp)\n  thr_slp &lt;- stats::quantile(v_slp[is.finite(v_slp)], 0.90, na.rm = TRUE)\n  bare_mask &lt;- terra::ifel((slp &gt;= thr_slp) & (lakeR &lt;= 0) & (forest_mask == 0), 1, 0)\n  lc &lt;- terra::ifel(bare_mask == 1, 3L, lc)\n  lc &lt;- terra::clamp(lc, 1L, 4L); names(lc) &lt;- \"lc\"\n  \n  lc_levels &lt;- c(\"forest\",\"water\",\"bare soil\",\"meadows\")\n  lc_colors &lt;- c(\"forest\"=\"#2E8B57\",\"water\"=\"#5DADE2\",\"bare soil\"=\"#C49A6C\",\"meadows\"=\"#7FBF7B\")\n  \n  # --- 8) Noise --------------------------------------------------------\n  if (is.null(noise14)) {\n    set.seed(1001)\n    noise14 &lt;- terra::setValues(terra::rast(E), rnorm(terra::ncell(E), 0, 0.3))\n  }\n  if (is.null(noise05)) {\n    set.seed(1002)\n    noise05 &lt;- terra::setValues(terra::rast(E), rnorm(terra::ncell(E), 0, 0.3))\n  }\n  \n  # --- 9) Physics fields ----------------------------------------------\n  topo &lt;- list(E = E, slp = slp, asp = asp, hillW = hillW)\n  phys &lt;- build_physics_fields(\n    topography = topo, landcover = lc,\n    noise14 = noise14, noise05 = noise05,\n    sun14 = sun14, sun05 = sun05\n  )\n  R14 &lt;- phys$R14; R05 &lt;- phys$R05\n  \n  # --- 10) Return ------------------------------------------------------\n  list(\n    E = E, slp = slp, asp = asp,\n    I14 = I14, I05 = I05,\n    R14 = R14, R05 = R05,\n    lake = lakeR, hillW = hillW,\n    lc = lc, lc_levels = lc_levels, lc_colors = lc_colors,\n    sun = list(T14 = sun14, T05 = sun05)\n  )\n}\n\n\n\n\n# -------------------------- Stationen/Features -------------------------\nmake_stations &lt;- function(domain, n_st = 60,\n                          station_mode = c(\"random\",\"ns_transect\",\"ew_transect\"),\n                          transect_margin_m = 10, ns_offset_m = 0, ew_offset_m = 0,\n                          crs = sf::st_crs(domain$Rtemplate)) {\n  station_mode &lt;- match.arg(station_mode)\n  with(domain, {\n    if (station_mode == \"random\") {\n      pts &lt;- tibble::tibble(\n        id = 1:n_st,\n        x  = runif(n_st, xmin + transect_margin_m, xmax - transect_margin_m),\n        y  = runif(n_st, ymin + transect_margin_m, ymax - transect_margin_m)\n      )\n    } else if (station_mode == \"ns_transect\") {\n      x_const &lt;- min(max(x0 + ns_offset_m, xmin + transect_margin_m), xmax - transect_margin_m)\n      y_seq   &lt;- seq(ymin + transect_margin_m, ymax - transect_margin_m, length.out = n_st)\n      pts &lt;- tibble::tibble(id = 1:n_st, x = x_const, y = y_seq)\n    } else {\n      y_const &lt;- min(max(y0 + ew_offset_m, ymin + transect_margin_m), ymax - transect_margin_m)\n      x_seq   &lt;- seq(xmin + transect_margin_m, xmax - transect_margin_m, length.out = n_st)\n      pts &lt;- tibble::tibble(id = 1:n_st, x = x_seq, y = y_const)\n    }\n    sf::st_as_sf(pts, coords = c(\"x\",\"y\"), crs = crs, remove = FALSE)\n  })\n}\n\nstations_from_scenario &lt;- function(scen, pts_sf) {\n  vpts &lt;- terra::vect(pts_sf)\n  df &lt;- tibble::as_tibble(pts_sf) %&gt;%\n    dplyr::mutate(\n      z_surf = as.numeric(terra::extract(scen$E,   vpts, ID = FALSE)[,1]),\n      slp    = as.numeric(terra::extract(scen$slp, vpts, ID = FALSE)[,1]),\n      I14    = as.numeric(terra::extract(scen$I14, vpts, ID = FALSE)[,1]),\n      I05    = as.numeric(terra::extract(scen$I05, vpts, ID = FALSE)[,1]),\n      lc     = as.integer(terra::extract(scen$lc,  vpts, ID = FALSE)[,1]),\n      T14    = as.numeric(terra::extract(scen$R14, vpts, ID = FALSE)[,1]),\n      T05    = as.numeric(terra::extract(scen$R05, vpts, ID = FALSE)[,1])\n    )\n  lc_levels &lt;- scen$lc_levels\n  pts14 &lt;- df[stats::complete.cases(df[, c(\"x\",\"y\",\"z_surf\",\"slp\",\"I14\",\"lc\",\"T14\")]), ]\n  pts05 &lt;- df[stats::complete.cases(df[, c(\"x\",\"y\",\"z_surf\",\"slp\",\"I05\",\"lc\",\"T05\")]), ]\n  stn_sf_14 &lt;- pts14 %&gt;%\n    dplyr::transmute(id, x, y,\n                     z_surf = as.numeric(z_surf), slp = as.numeric(slp), cosi = as.numeric(I14),\n                     lc = factor(lc_levels[pmax(1, pmin(lc, length(lc_levels)))], levels = lc_levels),\n                     temp = as.numeric(T14)) %&gt;%\n    sf::st_as_sf(coords = c(\"x\",\"y\"), crs = sf::st_crs(pts_sf), remove = FALSE)\n  stn_sf_05 &lt;- pts05 %&gt;%\n    dplyr::transmute(id, x, y,\n                     z_surf = as.numeric(z_surf), slp = as.numeric(slp), cosi = as.numeric(I05),\n                     lc = factor(lc_levels[pmax(1, pmin(lc, length(lc_levels)))], levels = lc_levels),\n                     temp = as.numeric(T05)) %&gt;%\n    sf::st_as_sf(coords = c(\"x\",\"y\"), crs = sf::st_crs(pts_sf), remove = FALSE)\n  list(T14 = stn_sf_14, T05 = stn_sf_05)\n}\n\n# -------------------------- Plots: Übersicht ---------------------------\n\n# -------------------------- Preview: Domain ----------------------------\n# Zeigt Extent, optional ein Grid, und annotiert Kern-Parameter.\npreview_domain &lt;- function(domain, grid = TRUE, grid_step = NULL, annotate = TRUE) {\n  stopifnot(is.list(domain), !is.null(domain$Rtemplate))\n  crs &lt;- sf::st_crs(domain$Rtemplate)\n  bb  &lt;- sf::st_as_sfc(sf::st_bbox(c(\n    xmin = domain$xmin, ymin = domain$ymin,\n    xmax = domain$xmax, ymax = domain$ymax\n  ), crs = crs))\n\n  p &lt;- ggplot2::ggplot() +\n    ggplot2::geom_sf(data = bb, fill = NA, color = \"black\", linewidth = 0.7) +\n    ggplot2::coord_sf(crs = crs, datum = NA, expand = FALSE) +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(title = \"Domain preview\", x = \"Easting (m)\", y = \"Northing (m)\")\n\n  if (isTRUE(grid)) {\n    len_x &lt;- domain$xmax - domain$xmin\n    len_y &lt;- domain$ymax - domain$ymin\n    step  &lt;- if (is.null(grid_step)) max(100, round(min(len_x, len_y)/6, -2)) else grid_step\n    gr    &lt;- sf::st_make_grid(bb, cellsize = c(step, step), what = \"polygons\")\n    gr_ln &lt;- sf::st_as_sf(sf::st_boundary(gr))\n    p &lt;- p + ggplot2::geom_sf(data = gr_ln, color = \"grey70\", linewidth = 0.2)\n  }\n\n  if (isTRUE(annotate)) {\n    r   &lt;- terra::res(domain$Rtemplate)\n    txt &lt;- sprintf(\n      \"Center: (%.0f, %.0f)\\nSize: %.0f × %.0f m\\nRes: %.0f × %.0f m\\nCRS: %s\",\n      domain$x0, domain$y0,\n      domain$xmax - domain$xmin, domain$ymax - domain$ymin,\n      r[1], r[2], as.character(terra::crs(domain$Rtemplate))\n    )\n    p &lt;- p + ggplot2::annotate(\n      \"label\",\n      x = domain$xmin + 0.02 * (domain$xmax - domain$xmin),\n      y = domain$ymin + 0.06 * (domain$ymax - domain$ymin),\n      label = txt, hjust = 0, vjust = 0, size = 3\n    )\n  }\n\n  print(p)\n  invisible(p)\n}\n\n# -------------------------- Preview: Szenario-Raster -------------------\n# Visualisiert die vorhandenen Raster im Szenario (ohne Modelle/CV).\npreview_scenario &lt;- function(x,\n                             which = c(\"lc\",\"E\",\"slp\",\"R14\",\"R05\"),\n                             stations = NULL,\n                             show_contours = TRUE,\n                             layout = c(\"grid\",\"vertical\")) {\n  layout &lt;- match.arg(layout)\n  \n  # ---- Szenario + Stationen erkennen ---------------------------------\n  scen &lt;- if (is.list(x) && !is.null(x$scen)) x$scen else x\n  if (is.null(stations) && is.list(x)) {\n    stations &lt;- x$pts_sf %||% x$stn_sf_14 %||% x$stn_sf_05\n  }\n  \n  # ---- Verfügbare Ebenen sammeln -------------------------------------\n  layers &lt;- list(\n    lc  = scen$lc,\n    E   = scen$E,\n    slp = scen$slp,\n    R14 = scen$R14,\n    R05 = scen$R05\n  )\n  keep &lt;- names(layers) %in% which & vapply(layers, function(r) inherits(r,\"SpatRaster\"), TRUE)\n  layers &lt;- layers[keep]\n  if (!length(layers)) stop(\"preview_scenario(): Keine der angefragten Ebenen im Szenario vorhanden.\")\n  \n  # ---- optionale Konturen vorbereiten --------------------------------\n  add_contours &lt;- function(p) p\n  if (isTRUE(show_contours)) {\n    adders &lt;- list()\n    \n    has_lake &lt;- !is.null(scen$lake) && inherits(scen$lake, \"SpatRaster\")\n    if (has_lake) {\n      lake_df &lt;- as.data.frame(scen$lake, xy = TRUE); names(lake_df) &lt;- c(\"x\",\"y\",\"lake\")\n      adders[[length(adders)+1]] &lt;- ggplot2::geom_contour(\n        data = lake_df,\n        mapping = ggplot2::aes(x = x, y = y, z = lake),\n        breaks = 0.5, colour = \"black\", linewidth = 0.35,\n        inherit.aes = FALSE\n      )\n    }\n    \n    has_hill &lt;- !is.null(scen$hillW) && inherits(scen$hillW, \"SpatRaster\")\n    if (has_hill) {\n      hill_df &lt;- as.data.frame(scen$hillW, xy = TRUE); names(hill_df) &lt;- c(\"x\",\"y\",\"hillW\")\n      adders[[length(adders)+1]] &lt;- ggplot2::geom_contour(\n        data = hill_df,\n        mapping = ggplot2::aes(x = x, y = y, z = hillW),\n        breaks = 0.5, colour = \"black\", linetype = \"22\", linewidth = 0.3,\n        inherit.aes = FALSE\n      )\n    }\n    \n    add_contours &lt;- function(p) {\n      if (length(adders)) for (a in adders) p &lt;- p + a\n      p\n    }\n  }\n  \n  # ---- Einzelplots bauen ----------------------------------------------\n  make_plot &lt;- function(name, r) {\n    df &lt;- as.data.frame(r, xy = TRUE); names(df) &lt;- c(\"x\",\"y\",\"val\")\n    \n    if (name == \"lc\") {\n      levs &lt;- scen$lc_levels %||% c(\"1\",\"2\",\"3\",\"4\")\n      pal  &lt;- scen$lc_colors %||% setNames(scales::hue_pal()(length(levs)), levs)\n      df$val &lt;- factor(df$val, levels = seq_along(levs), labels = levs)\n      p &lt;- ggplot2::ggplot(df, ggplot2::aes(x, y, fill = val)) +\n        ggplot2::geom_raster() +\n        ggplot2::scale_fill_manual(\n          values = pal[levels(df$val)],  # &lt;-- sicher auf Levels abbilden\n          na.value = \"grey90\", name = \"Landuse\"\n        ) +\n        ggplot2::coord_equal() + ggplot2::theme_minimal() +\n        ggplot2::labs(title = \"Landuse\", x = \"Easting\", y = \"Northing\")\n    } else {\n      p &lt;- ggplot2::ggplot(df, ggplot2::aes(x, y, fill = val)) +\n        ggplot2::geom_raster() +\n        ggplot2::coord_equal() + ggplot2::theme_minimal() +\n        ggplot2::labs(\n          title = switch(name,\n                         E   = \"Elevation (m)\",\n                         slp = \"Slope (rad)\",\n                         R14 = \"T 14 UTC\",\n                         R05 = \"T 05 UTC\",\n                         name),\n          x = \"Easting\", y = \"Northing\"\n        )\n      if (name %in% c(\"E\",\"slp\")) {\n        p &lt;- p + ggplot2::scale_fill_viridis_c()\n      } else {\n        p &lt;- p + ggplot2::scale_fill_gradientn(colors = temp_palette(256), name = \"Temp\")\n      }\n    }\n    \n    # Konturen (nur falls vorhanden)\n    p &lt;- add_contours(p)\n    \n    # Stationen optional\n    if (!is.null(stations) && inherits(stations, \"sf\")) {\n      # stilles CRS-Align (Fehlermeldungen unterdrücken)\n      suppressWarnings({\n        stations_plot &lt;- try(sf::st_transform(stations, sf::st_crs(scen$lc %||% scen$E)), silent = TRUE)\n        if (inherits(stations_plot, \"try-error\")) stations_plot &lt;- stations\n        p &lt;- p + ggplot2::geom_sf(\n          data = stations_plot, colour = \"black\", fill = \"white\",\n          shape = 21, size = 1.6, stroke = 0.25, inherit.aes = FALSE\n        )\n      })\n    }\n    p\n  }\n  \n  plots &lt;- Map(make_plot, names(layers), layers)\n  \n  # ---- kombinieren ----------------------------------------------------\n  if (length(plots) == 1) {\n    p_out &lt;- plots[[1]]\n  } else if (layout == \"vertical\") {\n    p_out &lt;- patchwork::wrap_plots(plots, ncol = 1)\n  } else {\n    p_out &lt;- patchwork::wrap_plots(plots, ncol = min(3, length(plots)))\n  }\n  \n  print(p_out)\n  invisible(p_out)\n}\n\n\n\n\nplot_landcover_terrain &lt;- function(scen, stations = NULL, show_contours = TRUE,\n                                   layout = c(\"grid\",\"vertical\")) {\n  layout &lt;- match.arg(layout)\n  lc_df  &lt;- as.data.frame(scen$lc,  xy = TRUE); names(lc_df)  &lt;- c(\"x\",\"y\",\"lc\")\n  E_df   &lt;- as.data.frame(scen$E,   xy = TRUE); names(E_df)   &lt;- c(\"x\",\"y\",\"elev\")\n  slp_df &lt;- as.data.frame(scen$slp, xy = TRUE); names(slp_df) &lt;- c(\"x\",\"y\",\"slp\")\n  lc_df$lc &lt;- factor(lc_df$lc, levels = seq_along(scen$lc_levels), labels = scen$lc_levels)\n  \n  p_lc &lt;- ggplot2::ggplot() +\n    ggplot2::geom_raster(data = lc_df, ggplot2::aes(x, y, fill = lc)) +\n    ggplot2::scale_fill_manual(values = scen$lc_colors, na.value = \"grey90\", name = \"Landuse\") +\n    ggplot2::coord_equal() + ggplot2::theme_minimal() +\n    ggplot2::labs(title = \"Landuse\", x = \"Easting\", y = \"Northing\")\n  \n  p_elev &lt;- ggplot2::ggplot() +\n    ggplot2::geom_raster(data = E_df, ggplot2::aes(x, y, fill = elev)) +\n    ggplot2::scale_fill_viridis_c(name = \"Altitude [m]\") +\n    ggplot2::coord_equal() + ggplot2::theme_minimal() +\n    ggplot2::labs(title = \"Altitude\", x = \"Easting\", y = \"Northing\")\n  \n  p_slp &lt;- ggplot2::ggplot() +\n    ggplot2::geom_raster(data = slp_df, ggplot2::aes(x, y, fill = slp)) +\n    ggplot2::scale_fill_viridis_c(name = \"Slope [rad]\") +\n    ggplot2::coord_equal() + ggplot2::theme_minimal() +\n    ggplot2::labs(title = \"Slope\", x = \"Easting\", y = \"Northing\")\n  \n  if (isTRUE(show_contours)) {\n    lake_df &lt;- as.data.frame(scen$lake, xy = TRUE); names(lake_df) &lt;- c(\"x\",\"y\",\"lake\")\n    hill_df &lt;- as.data.frame(scen$hillW, xy = TRUE); names(hill_df) &lt;- c(\"x\",\"y\",\"hillW\")\n    p_lc  &lt;- p_lc  +\n      ggplot2::geom_contour(data = lake_df, ggplot2::aes(x, y, z = lake),\n                            breaks = 0.5, colour = \"black\", linewidth = 0.35) +\n      ggplot2::geom_contour(data = hill_df, ggplot2::aes(x, y, z = hillW),\n                            breaks = 0.5, colour = \"black\", linetype = \"22\", linewidth = 0.3)\n    p_slp &lt;- p_slp +\n      ggplot2::geom_contour(data = lake_df, ggplot2::aes(x, y, z = lake),\n                            breaks = 0.5, colour = \"black\", linewidth = 0.35) +\n      ggplot2::geom_contour(data = hill_df, ggplot2::aes(x, y, z = hillW),\n                            breaks = 0.5, colour = \"black\", linetype = \"22\", linewidth = 0.3)\n  }\n  if (!is.null(stations)) {\n    add_st &lt;- list(ggplot2::geom_sf(data = stations, colour = \"black\", fill = \"white\",\n                                    shape = 21, size = 1.6, stroke = 0.25, inherit.aes = FALSE))\n    p_lc   &lt;- p_lc   + add_st\n    p_elev &lt;- p_elev + add_st\n    p_slp  &lt;- p_slp  + add_st\n  }\n  if (layout == \"vertical\") {\n    (p_lc / p_elev / p_slp) + patchwork::plot_layout(guides = \"keep\")\n  } else {\n    (p_lc | (p_elev | p_slp)) + patchwork::plot_layout(guides = \"keep\")\n  }\n}\n\nplot_block_overview_2x2_en &lt;- function(scen, pts_sf = NULL) {\n  Rstack &lt;- c(scen$E, scen$slp, scen$I14, scen$I05)\n  df &lt;- terra::as.data.frame(Rstack, xy = TRUE, na.rm = FALSE)\n  names(df) &lt;- c(\"x\",\"y\",\"elev\",\"slope\",\"I14\",\"I05\")\n  theme_base &lt;- ggplot2::theme_minimal(base_size = 11)\n  pal_terrain &lt;- grDevices::hcl.colors(256, \"Terrain\")\n  pal_slope   &lt;- grDevices::hcl.colors(256, \"Viridis\")\n  pal_hot     &lt;- grDevices::hcl.colors(256, \"YlOrRd\")\n  pal_cool    &lt;- grDevices::hcl.colors(256, \"PuBuGn\")\n  p_elev &lt;- ggplot2::ggplot(df, ggplot2::aes(x, y, fill = elev)) +\n    ggplot2::geom_raster() + ggplot2::coord_equal() +\n    ggplot2::scale_fill_gradientn(colours = pal_terrain, name = \"m\") +\n    ggplot2::labs(title = \"Terrain (Elevation)\") + theme_base\n  p_slope &lt;- ggplot2::ggplot(df, ggplot2::aes(x, y, fill = slope)) +\n    ggplot2::geom_raster() + ggplot2::coord_equal() +\n    ggplot2::scale_fill_gradientn(colours = pal_slope, name = \"rad\") +\n    ggplot2::labs(title = \"Slope (radians)\") + theme_base\n  p_I14 &lt;- ggplot2::ggplot(df, ggplot2::aes(x, y, fill = I14)) +\n    ggplot2::geom_raster() + ggplot2::coord_equal() +\n    ggplot2::scale_fill_gradientn(colours = pal_hot, name = \"\") +\n    ggplot2::labs(title = \"Insolation 14 UTC (cos i)\") + theme_base\n  p_I05 &lt;- ggplot2::ggplot(df, ggplot2::aes(x, y, fill = I05)) +\n    ggplot2::geom_raster() + ggplot2::coord_equal() +\n    ggplot2::scale_fill_gradientn(colours = pal_cool, name = \"\") +\n    ggplot2::labs(title = \"Insolation 05 UTC (cos i)\") + theme_base\n  if (!is.null(pts_sf)) {\n    pts_df &lt;- sf::st_drop_geometry(pts_sf)\n    add_pts &lt;- function(p)\n      p + ggplot2::geom_point(data = pts_df, ggplot2::aes(x = x, y = y),\n                              inherit.aes = FALSE, size = 0.7, alpha = 0.7,\n                              colour = \"black\")\n    p_elev  &lt;- add_pts(p_elev); p_slope &lt;- add_pts(p_slope)\n    p_I14   &lt;- add_pts(p_I14);  p_I05   &lt;- add_pts(p_I05)\n  }\n  (p_elev | (p_slope)) / (p_I14 | p_I05) + patchwork::plot_layout(guides = \"collect\")\n}\n\n# -------------------------- Geostat/Models -----------------------------\n# helpers\n.align_factor_to_model &lt;- function(x, lev_model) {\n  xs &lt;- as.character(x)\n  if (!length(lev_model)) return(factor(rep(NA_character_, length(xs))))\n  y &lt;- factor(xs, levels = lev_model)\n  if (anyNA(y)) { xs[is.na(y)] &lt;- lev_model[1]; y &lt;- factor(xs, levels = lev_model) }\n  y\n}\n.default_vgm &lt;- function(values, model = \"Exp\", range = 100) {\n  psill &lt;- stats::var(values, na.rm = TRUE); nug &lt;- 0.1 * psill\n  gstat::vgm(psill = psill, model = model, range = range, nugget = nug)\n}\nsafe_r2 &lt;- function(obs, pred) {\n  idx &lt;- is.finite(obs) & is.finite(pred)\n  if (sum(idx) &lt; 2) return(NA_real_)\n  x &lt;- obs[idx]; y &lt;- pred[idx]\n  sx &lt;- stats::sd(x); sy &lt;- stats::sd(y)\n  if (!is.finite(sx) || !is.finite(sy) || sx == 0 || sy == 0) return(NA_real_)\n  stats::cor(x, y)^2\n}\nsafe_gam_formula &lt;- function(d, include_lc = FALSE) {\n  stopifnot(all(c(\"temp\",\"x\",\"y\") %in% names(d)))\n  d &lt;- d[stats::complete.cases(d[, c(\"temp\",\"x\",\"y\")]), , drop = FALSE]\n  n    &lt;- nrow(d)\n  n_xy &lt;- dplyr::n_distinct(paste0(round(d$x,3), \"_\", round(d$y,3)))\n  k_xy &lt;- max(3, min(60, n_xy - 1L, floor(n * 0.8)))\n  base &lt;- if (n_xy &gt;= 4) sprintf(\"temp ~ s(x,y,bs='tp',k=%d)\", k_xy) else \"temp ~ x + y\"\n  add &lt;- character(0)\n  kcap &lt;- function(x, kmax) {\n    ux &lt;- unique(x[is.finite(x)]); nu &lt;- length(ux)\n    if (nu &lt;= 3) return(0L); max(4L, min(kmax, nu - 1L))\n  }\n  if (\"z_surf\" %in% names(d) && dplyr::n_distinct(d$z_surf) &gt; 3) add &lt;- c(add, sprintf(\"s(z_surf,bs='tp',k=%d)\", kcap(d$z_surf, 20)))\n  if (\"slp\"    %in% names(d) && dplyr::n_distinct(d$slp)    &gt; 3) add &lt;- c(add, sprintf(\"s(slp,bs='tp',k=%d)\",    kcap(d$slp, 12)))\n  if (\"cosi\"   %in% names(d) && dplyr::n_distinct(d$cosi)   &gt; 3) add &lt;- c(add, sprintf(\"s(cosi,bs='tp',k=%d)\",   kcap(d$cosi, 12)))\n  if (include_lc && \"lc\" %in% names(d)) { d$lc &lt;- droplevels(factor(d$lc)); if (nlevels(d$lc) &gt;= 2) add &lt;- c(add, \"lc\") }\n  stats::as.formula(paste(base, paste(add, collapse = \" + \"), sep = if (length(add)) \" + \" else \"\"))\n}\n# learners\n# NOTE:\n# The following learner functions have been moved to a dedicated file\n# (e.g., learners_geostat_core.R):\n#   - pred_Voronoi\n#   - pred_IDW\n#   - pred_OK\n#   - pred_KED\n#   - pred_RF\n#   - pred_GAM\n#\n# Source that file alongside your helpers BEFORE any code that calls them.\n# -------------------------- Block-CV -----------------------------------\nmake_blocks_and_assign &lt;- function(pts_sf, E, block_size = 100) {\n  bb &lt;- sf::st_as_sfc(sf::st_bbox(c(xmin = terra::xmin(E), ymin = terra::ymin(E),\n                                    xmax = terra::xmax(E), ymax = terra::ymax(E)),\n                                  crs = sf::st_crs(pts_sf)))\n  gr &lt;- sf::st_make_grid(bb, cellsize = c(block_size, block_size), what = \"polygons\")\n  blocks &lt;- sf::st_sf(block_id = seq_along(gr), geometry = gr)\n  pts_blk &lt;- sf::st_join(pts_sf, blocks, join = sf::st_intersects, left = TRUE)\n  if (any(is.na(pts_blk$block_id))) {\n    nearest &lt;- sf::st_nearest_feature(pts_blk[is.na(pts_blk$block_id), ], blocks)\n    pts_blk$block_id[is.na(pts_blk$block_id)] &lt;- blocks$block_id[nearest]\n  }\n  list(blocks = blocks, pts = pts_blk)\n}\nplot_blocks_grid &lt;- function(blocks, pts_blk, title = \"Blocks & stations\") {\n  crs_plot &lt;- sf::st_crs(pts_blk)\n  bb       &lt;- sf::st_bbox(blocks)\n  n_blocks &lt;- dplyr::n_distinct(pts_blk$block_id)\n  cols     &lt;- scales::hue_pal()(max(1, n_blocks))\n  ggplot2::ggplot() +\n    ggplot2::geom_sf(data = blocks, fill = NA, color = \"grey50\", linewidth = 0.25) +\n    ggplot2::geom_sf(data = pts_blk, ggplot2::aes(color = factor(block_id)), size = 2, alpha = 0.95) +\n    ggplot2::scale_color_manual(values = cols, name = \"Block\") +\n    ggplot2::coord_sf(crs  = crs_plot, datum = NA,\n                      xlim = c(bb[\"xmin\"], bb[\"xmax\"]),\n                      ylim = c(bb[\"ymin\"], bb[\"ymax\"]), expand = FALSE) +\n    ggplot2::theme_minimal() + ggplot2::labs(title = title, x = \"Easting (m)\", y = \"Northing (m)\")\n}\nrun_lbo_cv &lt;- function(stn_sf, E, block_size = 100, models = c(\"Voronoi\",\"IDW\",\"OK\",\"KED\",\"RF\",\"GAM\")) {\n  if (!all(c(\"x\",\"y\") %in% names(stn_sf))) { xy &lt;- sf::st_coordinates(stn_sf); stn_sf$x &lt;- xy[,1]; stn_sf$y &lt;- xy[,2] }\n  blk &lt;- make_blocks_and_assign(stn_sf, E, block_size = block_size)\n  blocks_sf &lt;- blk$blocks; stn_blk &lt;- blk$pts\n  for (nm in c(\"temp\",\"z_surf\",\"slp\",\"cosi\",\"lc\",\"x\",\"y\")) if (!(nm %in% names(stn_blk))) stn_blk[[nm]] &lt;- stn_sf[[nm]][match(stn_blk$id, stn_sf$id)]\n  \n  block_ids &lt;- sort(unique(stn_blk$block_id))\n  out_list &lt;- vector(\"list\", length(block_ids))\n  for (k in seq_along(block_ids)) {\n    b &lt;- block_ids[k]\n    test_idx  &lt;- which(stn_blk$block_id == b)\n    train_idx &lt;- which(stn_blk$block_id != b)\n    train_sf &lt;- stn_blk[train_idx, ]; test_sf &lt;- stn_blk[test_idx, ]\n    pred_tbl &lt;- lapply(models, function(m) {\n      p &lt;- switch(m,\n                  \"Voronoi\" = pred_Voronoi(train_sf, test_sf),\n                  \"IDW\"     = pred_IDW    (train_sf, test_sf),\n                  \"OK\"      = pred_OK     (train_sf, test_sf),\n                  \"KED\"     = pred_KED    (train_sf, test_sf, E = E),\n                  \"RF\"      = pred_RF     (train_sf, test_sf),\n                  \"GAM\"     = pred_GAM    (train_sf, test_sf),\n                  stop(\"Unknown model: \", m)\n      )\n      tibble::tibble(model = m, id = test_sf$id, obs = test_sf$temp, pred = p, block_id = b)\n    })\n    out_list[[k]] &lt;- dplyr::bind_rows(pred_tbl)\n  }\n  \n  cv_tbl &lt;- dplyr::bind_rows(out_list)\n  metrics &lt;- cv_tbl %&gt;%\n    dplyr::group_by(model) %&gt;%\n    dplyr::summarise(\n      n    = dplyr::n(),\n      n_ok = sum(is.finite(obs) & is.finite(pred)),\n      MAE  = {i &lt;- is.finite(obs) & is.finite(pred); if (any(i)) mean(abs(pred[i]-obs[i])) else NA_real_},\n      RMSE = {i &lt;- is.finite(obs) & is.finite(pred); if (any(i)) sqrt(mean((pred[i]-obs[i])^2)) else NA_real_},\n      Bias = {i &lt;- is.finite(obs) & is.finite(pred); if (any(i)) mean(pred[i]-obs[i]) else NA_real_},\n      R2   = safe_r2(obs, pred),\n      .groups = \"drop\"\n    ) |&gt;\n    dplyr::arrange(RMSE)\n  \n  diag_plot &lt;- ggplot2::ggplot(cv_tbl, ggplot2::aes(obs, pred)) +\n    ggplot2::geom_abline(slope=1, intercept=0, linetype=\"dashed\") +\n    ggplot2::geom_point(alpha=0.7) +\n    ggplot2::coord_equal() + ggplot2::theme_minimal() +\n    ggplot2::labs(title = sprintf(\"LBO-CV (block = %dm) — Observed vs Predicted\", block_size), x = \"Observed\", y = \"Predicted\") +\n    ggplot2::facet_wrap(~ model)\n  \n  blocks_plot &lt;- plot_blocks_grid(blocks_sf, stn_blk, title = sprintf(\"Blocks (%.0f m) & stations\", block_size))\n  list(cv = cv_tbl, metrics = metrics, diag_plot = diag_plot, blocks_plot = blocks_plot)\n}\n\n# -------------------------- „run_for_time“ Wrapper ---------------------\nrun_for_time &lt;- function(stn_sf, truth_r, label,\n                         scen_local,\n                         block_m,\n                         models = c(\"Voronoi\",\"IDW\",\"OK\",\"KED\",\"RF\",\"GAM\"),\n                         layout = c(\"horizontal\",\"vertical\")) {\n  layout &lt;- match.arg(layout)\n  res   &lt;- run_lbo_cv(stn_sf, scen_local$E, block_size = block_m, models = models)\n  maps  &lt;- predict_maps(stn_sf, truth_r, which_time = label,\n                        scen = scen_local, models = models,\n                        lc_levels = scen_local$lc_levels)\n  list(res = res, maps = maps)\n}\n\n# -------------------------- Skalen & Tuning ----------------------------\nplot_variogram_with_scales &lt;- function(vg, L50, L95, sill, title = \"Empirical variogram\") {\n  df &lt;- as.data.frame(vg)\n  ggplot2::ggplot(df, ggplot2::aes(dist, gamma)) +\n    ggplot2::geom_point(size = 1.4) +\n    ggplot2::geom_line(alpha = 0.5) +\n    ggplot2::geom_hline(yintercept = sill, linetype = \"dotted\", linewidth = 0.4) +\n    ggplot2::geom_vline(xintercept = L50, colour = \"#2b8cbe\", linetype = \"dashed\") +\n    ggplot2::geom_vline(xintercept = L95, colour = \"#de2d26\", linetype = \"dashed\") +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(title = title, x = \"Distance (m)\", y = \"Semivariance\")\n}\n.mean_kernel_for_R &lt;- function(r, R_m) {\n  px &lt;- mean(terra::res(r))\n  half &lt;- max(1L, ceiling(R_m / px))\n  k &lt;- 2L * half + 1L\n  W &lt;- matrix(1, nrow = k, ncol = k)\n  W / sum(W)\n}\nsmooth_mean_R &lt;- function(r, R_m) {\n  W &lt;- .mean_kernel_for_R(r, R_m)\n  terra::focal(r, w = W, fun = \"mean\", na.policy = \"omit\", pad = TRUE, normalize = FALSE)\n}\ngaussian_focal &lt;- function(r, radius_m, sigma_m = NULL) {\n  resx &lt;- terra::res(r)[1]\n  if (is.null(sigma_m)) sigma_m &lt;- radius_m / 2\n  rad_px   &lt;- max(1L, round(radius_m / resx))\n  sigma_px &lt;- max(0.5, sigma_m / resx)\n  xs &lt;- -rad_px:rad_px\n  k1 &lt;- exp(-0.5 * (xs / sigma_px)^2); k1 &lt;- k1 / sum(k1)\n  K  &lt;- outer(k1, k1); K / sum(K)\n}\nsmooth_dem_and_derive &lt;- function(E, alt, az, radius_m) {\n  resx &lt;- terra::res(E)[1]\n  pad_cells &lt;- ceiling(radius_m / resx) + 2L\n  \n  E_pad &lt;- terra::extend(E, pad_cells)\n  \n  K &lt;- gaussian_focal(E_pad, radius_m)\n  Es_pad  &lt;- terra::focal(E_pad, w = K, fun = mean, na.policy = \"omit\", pad = TRUE)\n  \n  slp_pad &lt;- terra::terrain(Es_pad, v = \"slope\",  unit = \"radians\")\n  asp_pad &lt;- terra::terrain(Es_pad, v = \"aspect\", unit = \"radians\")\n  \n  ci_pad  &lt;- cosi_fun(alt, az, slp_pad, asp_pad)\n  \n  list(\n    Es   = terra::crop(Es_pad,  E),\n    slp  = terra::crop(slp_pad, E),\n    cosi = terra::crop(ci_pad,  E)\n  )\n}\n\n .extract_to_pts &lt;- function(r, pts_sf) {\n  out &lt;- try(terra::extract(r, terra::vect(pts_sf), ID = FALSE)[,1], silent = TRUE)\n  if (inherits(out, \"try-error\") || length(out) == 0L) rep(NA_real_, nrow(pts_sf)) else out\n}\n cv_gam_with_R &lt;- function(stn_sf, E, alt = NULL, az = NULL, R, block_size_m = NULL, verbose = TRUE,...) \n   {\n   t0 &lt;- proc.time()\n   # robust check whether to compute cos(i)\n   use_cosi &lt;- isTRUE(!is.null(alt) && !is.null(az) &&\n                        is.finite(alt) && is.finite(az)) \n   \n   # --- 0) Block size guard\n   bs &lt;- suppressWarnings(as.numeric(block_size_m)[1])\n   if (!is.finite(bs) || bs &lt;= 0) {\n     bs &lt;- suppressWarnings(as.numeric(get0(\"block_size\", ifnotfound = NA_real_)))\n   }\n   if (!is.finite(bs) || bs &lt;= 0)\n     stop(\"cv_gam_with_R(): no valid block size (block_size_m or global block_size).\")\n   \n   # --- 1) DEM smoothing + derived fields\n   zR   &lt;- smooth_mean_R(E, R)\n   slpR &lt;- terra::terrain(zR, v = \"slope\",  unit = \"radians\")\n   aspR &lt;- terra::terrain(zR, v = \"aspect\", unit = \"radians\")\n   \n   # Only compute cos(i) if BOTH angles are clean, finite scalars\n   use_cosi &lt;- isTRUE(is.numeric(alt) && is.numeric(az) &&\n                        length(alt) == 1L && length(az) == 1L &&\n                        is.finite(alt) && is.finite(az))\n   if (use_cosi) {\n     zen  &lt;- (pi/2 - alt)\n     ci   &lt;- cos(slpR)*cos(zen) + sin(slpR)*sin(zen)*cos(az - aspR)\n     cosiR &lt;- terra::ifel(ci &lt; 0, 0, ci)\n   } else {\n     cosiR &lt;- NULL\n   }\n   \n   # --- 2) Extract to stations (fill missing with medians)\n   if (!all(c(\"x\",\"y\") %in% names(stn_sf))) {\n     xy &lt;- sf::st_coordinates(stn_sf); stn_sf$x &lt;- xy[,1]; stn_sf$y &lt;- xy[,2]\n   }\n   fill_med &lt;- function(v) {\n     m &lt;- stats::median(v[is.finite(v)], na.rm = TRUE)\n     v[!is.finite(v)] &lt;- m\n     v\n   }\n   stn_sf$z_surf_R &lt;- fill_med(.extract_to_pts(zR,   stn_sf))\n   stn_sf$slp_R    &lt;- fill_med(.extract_to_pts(slpR, stn_sf))\n   if (use_cosi) {\n     stn_sf$cosi_R &lt;- fill_med(.extract_to_pts(cosiR, stn_sf))\n   } else {\n     stn_sf$cosi_R &lt;- NA_real_\n   }\n   \n   # --- 3) Build blocks\n   bb_poly &lt;- sf::st_as_sfc(sf::st_bbox(stn_sf), crs = sf::st_crs(stn_sf))\n   blocks  &lt;- sf::st_make_grid(bb_poly, cellsize = c(bs, bs), what = \"polygons\")\n   blocks  &lt;- sf::st_sf(block_id = seq_along(blocks), geometry = blocks)\n   \n   stn_blk &lt;- sf::st_join(stn_sf, blocks, join = sf::st_intersects, left = TRUE)\n   if (anyNA(stn_blk$block_id)) {\n     i &lt;- is.na(stn_blk$block_id)\n     stn_blk$block_id[i] &lt;- blocks$block_id[sf::st_nearest_feature(stn_blk[i,], blocks)]\n   }\n   if (!all(c(\"x\",\"y\") %in% names(stn_blk))) {\n     xy &lt;- sf::st_coordinates(stn_blk); stn_blk$x &lt;- xy[,1]; stn_blk$y &lt;- xy[,2]\n   }\n   \n   # --- 4) LBO-CV\n   bids  &lt;- sort(unique(stn_blk$block_id))\n   pm_say(\"[cv_gam_with_R] R=%.0f m | block=%.0f m | stations=%d | blocks=%d | cos(i)=%s\",\n          R, block_size_m, nrow(stn_sf), length(bids),\n          if (use_cosi) \"yes\" else \"no\", v = verbose)\n   preds &lt;- vector(\"list\", length(bids)); j &lt;- 0L\n   \n   for (b in bids) {\n     te &lt;- stn_blk[stn_blk$block_id == b, ]\n     tr &lt;- stn_blk[stn_blk$block_id != b, ]\n     pm_say(\"  - block %d: train=%d, test=%d\", b, nrow(tr), nrow(te), v = verbose)\n     \n     \n     dtr  &lt;- sf::st_drop_geometry(tr)\n     # include cosi_R only if it’s present with finite values\n     need &lt;- c(\"temp\",\"x\",\"y\",\"z_surf_R\",\"slp_R\")\n     inc_cosi &lt;- (\"cosi_R\" %in% names(dtr)) && any(is.finite(dtr$cosi_R))\n     if (inc_cosi) need &lt;- c(need, \"cosi_R\")\n     \n     dtr  &lt;- dtr[stats::complete.cases(dtr[, need, drop = FALSE]), need, drop = FALSE]\n     if (nrow(dtr) &lt; 10) next\n     \n     # dynamic k guards\n     n_xy &lt;- dplyr::n_distinct(paste0(round(dtr$x,3), \"_\", round(dtr$y,3)))\n     k_xy &lt;- .k_for_xy(nrow(dtr), n_xy)\n     k_z  &lt;- .kcap_unique(dtr$z_surf_R, 20)\n     k_sl &lt;- .kcap_unique(dtr$slp_R,    12)\n     if (inc_cosi) k_ci &lt;- .kcap_unique(dtr$cosi_R, 12)\n     \n     # assemble formula with only informative terms\n     terms &lt;- c()\n     terms &lt;- c(terms, if (n_xy &gt;= 4) sprintf(\"s(x,y,bs='tp',k=%d)\", k_xy) else \"x + y\")\n     terms &lt;- c(terms, if (k_z  &gt;= 4) sprintf(\"s(z_surf_R,bs='tp',k=%d)\", k_z)  else \"z_surf_R\")\n     if (length(unique(dtr$slp_R[is.finite(dtr$slp_R)])) &gt; 1)\n       terms &lt;- c(terms, if (k_sl &gt;= 4) sprintf(\"s(slp_R,bs='tp',k=%d)\", k_sl) else \"slp_R\")\n     if (inc_cosi && any(is.finite(dtr$cosi_R)) &&\n         length(unique(dtr$cosi_R[is.finite(dtr$cosi_R)])) &gt; 1)\n       terms &lt;- c(terms, if (k_ci &gt;= 4) sprintf(\"s(cosi_R,bs='tp',k=%d)\", k_ci) else \"cosi_R\")\n     \n     form &lt;- stats::as.formula(paste(\"temp ~\", paste(terms, collapse = \" + \")))\n     gm &lt;- mgcv::gam(form, data = dtr, method = \"REML\", select = TRUE)\n     \n     dte &lt;- sf::st_drop_geometry(te)\n     # restrict to variables actually in the model\n     vars_needed &lt;- setdiff(all.vars(form), \"temp\")\n     dte &lt;- dte[, vars_needed, drop = FALSE]\n     ph  &lt;- try(stats::predict(gm, newdata = dte, type = \"response\"), silent = TRUE)\n     if (inherits(ph, \"try-error\")) ph &lt;- rep(NA_real_, nrow(dte))\n     \n     j &lt;- j + 1L\n     preds[[j]] &lt;- tibble::tibble(id = te$id, obs = te$temp, pred = as.numeric(ph), block_id = b)\n   }\n   \n   preds &lt;- preds[seq_len(j)]\n   if (!length(preds)) {\n     return(list(cv = tibble::tibble(id = integer(), obs = numeric(), pred = numeric(), block_id = integer()),\n                 RMSE = NA_real_))\n   }\n   out  &lt;- dplyr::bind_rows(preds)\n   rmse &lt;- sqrt(mean((out$pred - out$obs)^2, na.rm = TRUE))\n   list(cv = out, RMSE = rmse)\n }\n \n \n \n\n tune_Rstar_ucurve &lt;- function(stn_sf, E, alt = NULL, az = NULL,\n                               L50, L95, block_fallback = 120,\n                               n_grid = 6, extra = c(0.8, 1.2),\n                               scen = NULL, which_time = c(\"T14\",\"T05\")) {\n   \n   which_time &lt;- match.arg(which_time)\n   \n   # fallback L50/L95 if broken\n   e &lt;- terra::ext(E)\n   dom_diag &lt;- sqrt((terra::xmax(e)-terra::xmin(e))^2 + (terra::ymax(e)-terra::ymin(e))^2)\n   if (!is.finite(L50) || !is.finite(L95) || L95 &lt;= L50) {\n     L50 &lt;- dom_diag/10; L95 &lt;- dom_diag/4\n   }\n   block_m &lt;- max(block_fallback, round(L50))\n   \n   # sun from scen if not given\n   if ((is.null(alt) || is.null(az)) && !is.null(scen)) {\n     s &lt;- .get_sun(scen, which_time)\n     alt &lt;- s$alt; az &lt;- s$az\n   }\n   \n   R_min &lt;- max(10, round(L50*extra[1])); R_max &lt;- round(L95*extra[2])\n   R_grid &lt;- unique(round(seq(R_min, R_max, length.out = n_grid)))\n   \n   rows &lt;- lapply(R_grid, function(R) {\n     z &lt;- cv_gam_with_R(stn_sf, E, alt = alt, az = az, R = R,\n                        block_size_m = block_m, scen = NULL, which_time = which_time)\n     data.frame(R = R, RMSE = z$RMSE)\n   })\n   df &lt;- do.call(rbind, rows)\n   R_star &lt;- df$R[which.min(df$RMSE)]\n   list(grid = df, R_star = as.numeric(R_star), block_m = block_m)\n }\n \n \n\nplot_ucurve &lt;- function(df, R_star, title = \"U-curve: tune R\") {\n  ggplot2::ggplot(df, ggplot2::aes(R, RMSE)) +\n    ggplot2::geom_line() + ggplot2::geom_point() +\n    ggplot2::geom_vline(xintercept = R_star, linetype = \"dashed\", colour = \"#de2d26\") +\n    ggplot2::theme_minimal() + ggplot2::labs(title = title, x = \"Drift radius R (m)\", y = \"RMSE (block-CV)\")\n}\n\n# Drop-in replacement\nadd_drifts_at_R &lt;- function(stn_sf, E, alt, az, R,\n                            lc = NULL, lc_levels = NULL,\n                            na_action = c(\"error\",\"fill\",\"drop\")) {\n  na_action &lt;- match.arg(na_action)\n  \n  # 0) Align CRS (key cause of NA extractions)\n  crs_r &lt;- sf::st_crs(E)\n  if (!isTRUE(sf::st_crs(stn_sf) == crs_r)) {\n    stn_sf &lt;- sf::st_transform(stn_sf, crs_r)\n  }\n  \n  # 1) Build @R* features (Es, slope, cosi) — your existing function\n  fr &lt;- smooth_dem_and_derive(E, alt, az, radius_m = R)\n  \n  # 2) Extract to points\n  v &lt;- terra::vect(stn_sf)\n  stn_sf$E_R    &lt;- as.numeric(terra::extract(fr$Es,   v, ID = FALSE)[, 1])\n  stn_sf$slp_R  &lt;- as.numeric(terra::extract(fr$slp,  v, ID = FALSE)[, 1])\n  stn_sf$cosi_R &lt;- as.numeric(terra::extract(fr$cosi, v, ID = FALSE)[, 1])\n  \n  # Optional LC (factor) — unchanged logic\n  if (!is.null(lc)) {\n    if (is.null(lc_levels)) lc_levels &lt;- lc_levels_default\n    lc_idx &lt;- as.integer(terra::extract(lc, v, ID = FALSE)[, 1])\n    lc_idx[!is.finite(lc_idx)] &lt;- 1L\n    lc_idx &lt;- pmax(1L, pmin(lc_idx, length(lc_levels)))\n    stn_sf$lc &lt;- factor(lc_levels[lc_idx], levels = lc_levels)\n  }\n  \n  # 3) Handle NA per policy\n  d &lt;- sf::st_drop_geometry(stn_sf)\n  miss &lt;- !stats::complete.cases(d[, c(\"E_R\",\"slp_R\",\"cosi_R\"), drop = FALSE])\n  \n  if (any(miss)) {\n    if (na_action == \"error\") {\n      stop(\"Station features at R* contain NA. Increase padding in smooth_dem_and_derive(), \",\n           \"reduce R*, or call add_drifts_at_R(..., na_action='fill'/'drop').\")\n    }\n    if (na_action == \"fill\") {\n      fill_med &lt;- function(x) { m &lt;- stats::median(x[is.finite(x)], na.rm = TRUE); x[!is.finite(x)] &lt;- m; x }\n      stn_sf$E_R    &lt;- fill_med(stn_sf$E_R)\n      stn_sf$slp_R  &lt;- fill_med(stn_sf$slp_R)\n      stn_sf$cosi_R &lt;- fill_med(stn_sf$cosi_R)\n    }\n    if (na_action == \"drop\") {\n      stn_sf &lt;- stn_sf[!miss, ]\n    }\n  }\n  \n  stn_sf\n}\n\ncompute_Ls_from_points &lt;- function(stn_sf, value_col = \"temp\",\n                                   maxdist = NULL, nlag = 18, smooth_k = 3) {\n  stopifnot(inherits(stn_sf, \"sf\"), value_col %in% names(stn_sf))\n  pts &lt;- stn_sf[is.finite(stn_sf[[value_col]]), ]\n  if (is.null(maxdist)) {\n    bb &lt;- sf::st_bbox(pts)\n    dom_diag &lt;- sqrt((bb[\"xmax\"]-bb[\"xmin\"])^2 + (bb[\"ymax\"]-bb[\"ymin\"])^2)\n    maxdist &lt;- dom_diag / 2\n  }\n  form &lt;- stats::as.formula(sprintf(\"%s ~ 1\", value_col))\n  vg  &lt;- gstat::variogram(form, data = pts, cutoff = maxdist, width = maxdist/nlag)\n  if (nrow(vg) &gt;= smooth_k) {\n    vg$gamma &lt;- stats::filter(vg$gamma, rep(1/smooth_k, smooth_k), sides = 2)\n    vg$gamma[!is.finite(vg$gamma)] &lt;- zoo::na.approx(vg$gamma, na.rm = FALSE)\n    vg$gamma &lt;- zoo::na.locf(zoo::na.locf(vg$gamma, fromLast = TRUE))\n  }\n  sill &lt;- max(vg$gamma, na.rm = TRUE)\n  if (!is.finite(sill) || sill &lt;= 0) sill &lt;- stats::median(vg$gamma, na.rm = TRUE)\n  L_at_q &lt;- function(q) {\n    thr &lt;- q * sill\n    i   &lt;- which(vg$gamma &gt;= thr)[1]\n    if (is.na(i)) return(NA_real_)\n    if (i == 1) return(vg$dist[1])\n    d0 &lt;- vg$dist[i-1]; d1 &lt;- vg$dist[i]\n    g0 &lt;- vg$gamma[i-1]; g1 &lt;- vg$gamma[i]\n    if (!is.finite(d0) || !is.finite(d1) || g1 == g0) return(d1)\n    d0 + (thr - g0) * (d1 - d0) / (g1 - g0)\n  }\n  list(vg = vg, sill = sill, L50 = L_at_q(0.5), L95 = L_at_q(0.95), cutoff = maxdist)\n}\n\n# -------------------------- Error-Budget --------------------------------\n\n# -------------------------- Error-Budget --------------------------------\nnugget_fraction_from_cv &lt;- function(cv_sf_or_df, model, crs_ref, x_col=\"x\", y_col=\"y\",\n                                    cutoff = NULL, width = NULL) {\n  stopifnot(!missing(model))\n  df &lt;- dplyr::filter(cv_sf_or_df, .data$model == !!model)\n  \n  # Ensure sf\n  sf &lt;- if (inherits(df, \"sf\")) df else sf::st_as_sf(df, coords = c(x_col, y_col), crs = sf::st_crs(crs_ref))\n  sf &lt;- sf::st_transform(sf, sf::st_crs(crs_ref))\n  sf$resid &lt;- sf$obs - sf$pred\n  \n  xy  &lt;- sf::st_coordinates(sf) %&gt;% as.data.frame()\n  dat &lt;- dplyr::bind_cols(sf::st_drop_geometry(sf), xy)\n  \n  if (is.null(cutoff)) cutoff &lt;- max(dist(xy)) * 0.5\n  if (is.null(width))  width  &lt;- cutoff / 12\n  \n  vg  &lt;- gstat::variogram(resid ~ 1, data = dat, locations = ~ X + Y,\n                          cutoff = cutoff, width = width)\n  fit &lt;- gstat::fit.variogram(vg, gstat::vgm(\"Mat\"))\n  nug &lt;- fit$psill[fit$model == \"Nug\"]; sill &lt;- sum(fit$psill)\n  if (length(nug) && is.finite(sill) && sill &gt; 0) nug / sill else NA_real_\n}\n\nsimple_error_budget &lt;- function(res_cv, sigma_inst = 0.5, alpha = 0.6) {\n  res &lt;- res_cv$cv\n  res &lt;- res[is.finite(res$obs) & is.finite(res$pred), , drop = FALSE]\n  RMSE &lt;- sqrt(mean((res$pred - res$obs)^2))\n  Bias &lt;- mean(res$pred - res$obs)\n  VarE &lt;- stats::var(res$pred - res$obs, na.rm = TRUE)\n  meas &lt;- sigma_inst^2\n  proc &lt;- max(0, VarE - meas)\n  micro &lt;- alpha * proc\n  meso  &lt;- (1 - alpha) * proc\n  tibble::tibble(Component = c(\"RMSE\",\"Bias\",\"Total var\",\"Instrument var\",\"Microscale var\",\"Mesoscale var\"),\n                 Value     = c(RMSE, Bias, VarE, meas, micro, meso))\n}\n## ======================================================================\n## Ende der Bibliothek\n## ======================================================================\n\n## ---------- Mini-Beispiel (nicht Teil der Bibliothek) -----------------\n## domain  &lt;- make_domain()\n## scen    &lt;- build_scenario(domain, lake_mode=\"water\", hill_mode=\"bump\",\n##                           random_hills = 100, micro_seed = 1)\n## pts_sf  &lt;- make_stations(domain, n_st = 60, station_mode = \"random\")\n## stns    &lt;- stations_from_scenario(scen, pts_sf)\n## bs      &lt;- compute_block_size(len_x = domain$xmax-domain$xmin,\n##                               len_y = domain$ymax-domain$ymin, n_st = nrow(pts_sf))\n## out14   &lt;- run_for_time(stns$T14, scen$R14, \"T14\", scen, bs)\n## out05   &lt;- run_for_time(stns$T05, scen$R05, \"T05\", scen, bs)\n## # Plot-Beispiele:\n## # print(plot_landcover_terrain(scen, stations = stns$T14))\n## # print(out14$res$blocks_plot); print(out14$res$diag_plot)\n```\n\n\n-   `fun_learn_predict_core.R`: learning/validation/prediction\n    routines (block CV, learners, map prediction, residual\n    diagnostics).\n\n\nCode\n```{r}\n#| eval: false\n#' Geostatistical Learners & Map Predictor (Core Only)\n#'\n#' @title Learners and Raster Predictor (no helpers inside)\n#' @description\n#' A compact set of **model-specific predictors** used in your teaching/\n#' pipeline code, plus a high-level `predict_maps()` convenience that\n#' evaluates multiple learners on a full grid.  \n#'\n#' This file intentionally contains **no helpers**. It assumes that common\n#' utilities and constants are sourced from your *helpers* module, including:\n#' - `%||%` — null-coalescing helper\n#' - `.default_vgm()` — conservative variogram fallback\n#' - `.align_factor_to_model()` — align factor levels at predict time\n#' - `safe_gam_formula()` — guarded GAM formula constructor\n#' - `lc_levels_default` — global land-cover levels\n#' - `temp_palette`, `stretch_q` — visualization defaults\n#'\n#' @details\n#' **Contract expected by all learners**:\n#' - `train_sf`, `test_sf` are `sf` objects with at least:\n#'   - `temp` (numeric): the response variable to be learned\n#'   - `x`, `y` (numeric): planar coordinates (will be derived from geometry\n#'     if absent)\n#'   - Drift/covariate columns depending on the learner (see each function)\n#' - Each learner returns a numeric vector of predictions aligned with\n#'   `nrow(test_sf)`.\n#'\n#' **Coordinate Reference System**: all learners assume that `x` and `y`\n#' are in a **projected CRS** with meter-like units (e.g., UTM).\n#'\n#' **Error handling**:\n#' - Learners are defensive; if inputs are insufficient (e.g., too few rows,\n#'   missing drift columns), they return `NA_real_` predictions of the correct\n#'   length instead of failing hard (except where a *hard requirement* is unmet\n#'   such as missing KED drifts in training).\n#'\n#' @section Dependencies:\n#' - **Packages**: `sf`, `gstat`, `mgcv`, `randomForest`, `terra`, `ggplot2`,\n#'   `tibble`, `dplyr`, `stats`, `scales`\n#' - **Helpers (sourced elsewhere)**: `%||%`, `.default_vgm`, `.align_factor_to_model`,\n#'   `safe_gam_formula`, `lc_levels_default`, `temp_palette`, `stretch_q`\n#'\n#' @seealso\n#' - Your helpers/utilities module for the functions noted above.\n#' - `gstat::krige`, `gstat::idw`, `gstat::variogram`, `gstat::fit.variogram`\n#' - `mgcv::gam`, `randomForest::randomForest`\n#'\n#' @keywords geostatistics interpolation kriging regression GAM randomForest\n#' @family learners\n\n\n#' Voronoi / Nearest-Station Predictor\n#'\n#' @description\n#' Assigns each prediction point the observed value from the **nearest**\n#' training station (a fast proxy for Voronoi interpolation).\n#'\n#' @param train_sf `sf` with at least `temp` and geometry.\n#' @param test_sf  `sf` with geometry to predict for.\n#'\n#' @return Numeric vector `length(nrow(test_sf))` with nearest-neighbor temps.\n#' @examples\n#' # y_hat &lt;- pred_Voronoi(train_sf, grid_sf)\npred_Voronoi &lt;- function(train_sf, test_sf) {\n  idx &lt;- sf::st_nearest_feature(test_sf, train_sf)\n  as.numeric(train_sf$temp)[idx]\n}\n\n\n#' Inverse Distance Weighting (IDW)\n#'\n#' @description\n#' Classic **IDW** using `gstat::idw`, predicting from training points to\n#' the test geometry.\n#'\n#' @param train_sf `sf` with `temp` and geometry.\n#' @param test_sf  `sf` with geometry.\n#' @param idp      Inverse distance power (default `2`).\n#'\n#' @return Numeric vector of predictions for `test_sf`.\n#' @examples\n#' # y_hat &lt;- pred_IDW(train_sf, grid_sf, idp = 2)\npred_IDW &lt;- function(train_sf, test_sf, idp = 2) {\n  pr &lt;- suppressWarnings(gstat::idw(temp ~ 1, locations = train_sf, newdata = test_sf, idp = idp))\n  as.numeric(pr$var1.pred)\n}\n\n\n#' Ordinary Kriging (OK)\n#'\n#' @description\n#' Univariate **OK** with an automatically fitted **exponential** variogram.\n#' Falls back to `.default_vgm()` if fitting fails (e.g., too few points).\n#'\n#' @param train_sf `sf` with `temp` and geometry.\n#' @param test_sf  `sf` with geometry.\n#'\n#' @return Numeric vector of kriged predictions.\n#' @examples\n#' # y_hat &lt;- pred_OK(train_sf, grid_sf)\npred_OK &lt;- function(train_sf, test_sf) {\n  vg      &lt;- suppressWarnings(gstat::variogram(temp ~ 1, data = train_sf))\n  vgm_fit &lt;- try(suppressWarnings(gstat::fit.variogram(vg, gstat::vgm(\"Exp\"))), silent = TRUE)\n  if (inherits(vgm_fit, \"try-error\")) vgm_fit &lt;- .default_vgm(train_sf$temp)\n  pr &lt;- suppressWarnings(gstat::krige(temp ~ 1, locations = train_sf, newdata = test_sf, model = vgm_fit))\n  as.numeric(pr$var1.pred)\n}\n\n\n#' Kriging with External Drift (KED)\n#'\n#' @description\n#' **KED** with additive drift terms. Requires drifts in *training*, fills\n#' non-finite values in *test* by median of training. If `lc` is present in\n#' both sets, it is included as a categorical drift with aligned levels.\n#'\n#' @details\n#' **Required drift columns** in `train_sf`: `z_surf`, `slp`, `cosi`.  \n#' If any are missing in training, this function errors (by design).\n#'\n#' @param train_sf `sf`; must contain `temp`, `z_surf`, `slp`, `cosi`, geometry,\n#'   and optionally `lc`.\n#' @param test_sf  `sf` with geometry and preferably the same drift columns\n#'   (non-finite values are median-filled).\n#' @param ...      Unused (placeholder for compatibility).\n#'\n#' @return Numeric vector of KED predictions, `length(nrow(test_sf))`.\n#' @examples\n#' # y_hat &lt;- pred_KED(train_sf, grid_sf)\npred_KED &lt;- function(train_sf, test_sf, ...) {\n  need &lt;- c(\"z_surf\",\"slp\",\"cosi\")\n  miss &lt;- setdiff(need, names(train_sf))\n  if (length(miss)) stop(\"pred_KED(): missing drifts in training: \", paste(miss, collapse = \", \"))\n  use_lc &lt;- \"lc\" %in% names(train_sf) && \"lc\" %in% names(test_sf)\n  tr &lt;- train_sf; te &lt;- test_sf\n  if (use_lc) {\n    tr$lc &lt;- droplevels(factor(tr$lc))\n    te$lc &lt;- factor(as.character(te$lc), levels = levels(tr$lc))\n    te$lc[is.na(te$lc)] &lt;- levels(tr$lc)[1]\n  }\n  for (nm in need) {\n    m &lt;- stats::median(tr[[nm]][is.finite(tr[[nm]])], na.rm = TRUE)\n    te[[nm]][!is.finite(te[[nm]])] &lt;- m\n  }\n  keep_tr &lt;- c(\"temp\", need, if (use_lc) \"lc\")\n  dtr &lt;- sf::st_drop_geometry(tr)[, keep_tr, drop = FALSE]\n  ok  &lt;- stats::complete.cases(dtr); tr &lt;- tr[ok, ]\n  if (nrow(tr) &lt; 5) return(rep(NA_real_, nrow(te)))\n  form &lt;- stats::as.formula(paste(\"temp ~\", paste(c(need, if (use_lc) \"lc\"), collapse = \" + \")))\n  vg      &lt;- suppressWarnings(gstat::variogram(form, data = tr))\n  vgm_fit &lt;- try(suppressWarnings(gstat::fit.variogram(vg, gstat::vgm(\"Exp\"))), silent = TRUE)\n  if (inherits(vgm_fit, \"try-error\")) {\n    ps &lt;- stats::var(sf::st_drop_geometry(tr)$temp, na.rm = TRUE)\n    vgm_fit &lt;- gstat::vgm(psill = ps, model = \"Exp\", range = max(vg$dist, na.rm = TRUE)/3, nugget = 0.1*ps)\n  }\n  pr &lt;- suppressWarnings(gstat::krige(form, locations = tr, newdata = te, model = vgm_fit))\n  as.numeric(pr$var1.pred)\n}\n\n\n#' Random Forest Regressor (RF)\n#'\n#' @description\n#' A **RandomForest** on spatial and drift features. If `lc` is absent, a\n#' harmless single-level factor is injected (levels provided by\n#' `lc_levels_default`). At prediction, factor levels are aligned using\n#' `.align_factor_to_model()`.\n#'\n#' @param train_sf `sf` with `temp`, `x`, `y`, `z_surf`, `slp`, `cosi`,\n#'   optionally `lc` (factor), and geometry.\n#' @param test_sf  `sf` with the same predictors (geometry required).\n#'\n#' @return Numeric vector of RF predictions.\n#' @examples\n#' # y_hat &lt;- pred_RF(train_sf, grid_sf)\npred_RF &lt;- function(train_sf, test_sf) {\n  dtr &lt;- sf::st_drop_geometry(train_sf)\n  if (!(\"lc\" %in% names(dtr))) dtr$lc &lt;- factor(lc_levels_default[1], levels = lc_levels_default)\n  dtr$lc &lt;- droplevels(factor(as.character(dtr$lc), levels = lc_levels_default))\n  dtr &lt;- stats::na.omit(dtr)\n  if (nrow(dtr) &lt; 5) return(rep(NA_real_, nrow(test_sf)))\n  rf  &lt;- randomForest::randomForest(temp ~ x + y + z_surf + slp + cosi + lc, data = dtr, na.action = na.omit)\n  dte &lt;- sf::st_drop_geometry(test_sf)\n  if (!(\"lc\" %in% names(dte))) dte$lc &lt;- factor(lc_levels_default[1], levels = lc_levels_default)\n  lev &lt;- levels(dtr$lc)\n  dte$lc &lt;- .align_factor_to_model(dte$lc, lev)\n  good &lt;- stats::complete.cases(dte[, c(\"x\",\"y\",\"z_surf\",\"slp\",\"cosi\",\"lc\")])\n  out  &lt;- rep(NA_real_, nrow(dte)); if (any(good)) out[good] &lt;- stats::predict(rf, dte[good, ])\n  out\n}\n\n\n#' Generalized Additive Model (GAM)\n#'\n#' @description\n#' A **GAM** (thin-plate splines) built with a protective formula from\n#' `safe_gam_formula()` that caps basis sizes and includes `lc` only if\n#' useful. Requires a minimal number of complete rows.\n#'\n#' @param train_sf `sf` with `temp`, `x`, `y`, `z_surf`, `slp`, `cosi`,\n#'   optionally `lc` (factor).\n#' @param test_sf  `sf` with matching predictors.\n#'\n#' @return Numeric vector of GAM predictions; `NA_real_` if the model could\n#'   not be trained.\n#' @examples\n#' # y_hat &lt;- pred_GAM(train_sf, grid_sf)\npred_GAM &lt;- function(train_sf, test_sf) {\n  dtr  &lt;- sf::st_drop_geometry(train_sf)\n  keep &lt;- intersect(c(\"temp\",\"x\",\"y\",\"z_surf\",\"slp\",\"cosi\",\"lc\"), names(dtr))\n  dtr  &lt;- dtr[stats::complete.cases(dtr[, keep, drop = FALSE]), keep, drop = FALSE]\n  if (!nrow(dtr)) return(rep(NA_real_, nrow(test_sf)))\n  if (\"lc\" %in% names(dtr)) dtr$lc &lt;- droplevels(factor(dtr$lc))\n  inc_lc &lt;- \"lc\" %in% names(dtr) && nlevels(dtr$lc) &gt;= 2\n  if (nrow(dtr) &lt; 10) return(rep(NA_real_, nrow(test_sf)))\n  gm &lt;- mgcv::gam(formula = safe_gam_formula(dtr, include_lc = inc_lc), data = dtr, method = \"REML\", select = TRUE)\n  dte &lt;- sf::st_drop_geometry(test_sf)\n  vars &lt;- c(\"x\",\"y\",\"z_surf\",\"slp\",\"cosi\", if (inc_lc) \"lc\"); vars &lt;- intersect(vars, names(dte))\n  if (inc_lc) {\n    lev &lt;- levels(model.frame(gm)$lc)\n    if (!(\"lc\" %in% names(dte))) dte$lc &lt;- lev[1]\n    dte$lc &lt;- .align_factor_to_model(dte$lc, lev)\n  }\n  good &lt;- stats::complete.cases(dte[, vars, drop = FALSE])\n  out  &lt;- rep(NA_real_, nrow(dte)); if (any(good)) out[good] &lt;- stats::predict(gm, dte[good, vars, drop = FALSE], type = \"response\")\n  out\n}\n\n\n#' Predict on a Raster Grid with Multiple Learners + Pretty Plots\n#'\n#' @description\n#' High-level utility that:\n#' 1. Ensures station covariates exist (E, slope, cos(i), optional LC).\n#' 2. Builds a **full-grid** data frame of covariates from rasters.\n#' 3. Runs selected learners (`Voronoi`, `IDW`, `OK`, `KED`, `RF`, `GAM`).\n#' 4. Returns both **prediction rasters** and **ggplot** panels.\n#'\n#' @param stn_sf `sf` training stations; must have `temp` and (if missing)\n#'   this function will derive `x`, `y` and extract missing covariates from\n#'   rasters.\n#' @param truth_raster `SpatRaster` (single-layer) used only for common\n#'   color scaling in plots (and optional “truth” visualization).\n#' @param which_time Character; `\"T14\"` or `\"T05\"` (plot titles only).\n#' @param scen A scenario list containing at least: `E`, `slp`, and either\n#'   `I14` or `I05` (for cos(i)) and optionally `lc` + `lc_levels`.\n#' @param models Character vector of learners to run.\n#' @param lc_levels Optional character vector of LC levels (defaults to\n#'   `scen$lc_levels`).\n#' @param feature_rasters Optional list with named rasters `E`, `slp`, `cosi`\n#'   to **override** the scenario’s baseline (e.g., when using tuned R*).\n#'\n#' @return A list with:\n#' \\describe{\n#'   \\item{pred_df}{Tidy `tibble` of predictions for all models & grid cells}\n#'   \\item{pred_rasters}{`list` of `SpatRaster` predictions, one per model}\n#'   \\item{p_pred}{`ggplot` facet showing all model maps}\n#'   \\item{p_truth}{`ggplot` of the truth raster (for reference)}\n#' }\n#'\n#' @note\n#' Requires helpers/constants: `%||%`, `temp_palette`, `stretch_q`, plus\n#' land-cover level alignment utilities.\n#'\n#' @examples\n#' # out &lt;- predict_maps(stn_sf, scen$R14, which_time = \"T14\", scen = scen)\n#' # print(out$p_truth); print(out$p_pred)\npredict_maps &lt;- function(stn_sf, truth_raster,\n                         which_time = c(\"T14\",\"T05\"),\n                         scen, models = c(\"Voronoi\",\"IDW\",\"OK\",\"KED\",\"RF\",\"GAM\"),\n                         lc_levels = NULL,\n                         feature_rasters = NULL) {\n  which_time &lt;- match.arg(which_time)\n  lc_levels  &lt;- lc_levels %||% scen$lc_levels\n  E      &lt;- feature_rasters$E   %||% scen$E\n  slp_r  &lt;- feature_rasters$slp %||% scen$slp\n  cosi_r &lt;- feature_rasters$cosi %||% if (which_time == \"T14\") scen$I14 else scen$I05\n  has_lc &lt;- (\"lc\" %in% names(scen)) && !is.null(scen$lc)\n  lc_r   &lt;- if (has_lc) scen$lc else NULL\n  \n  train_sf &lt;- stn_sf\n  if (!all(c(\"x\",\"y\") %in% names(train_sf))) {\n    xy &lt;- sf::st_coordinates(train_sf); train_sf$x &lt;- xy[,1]; train_sf$y &lt;- xy[,2]\n  }\n  if (!(\"z_surf\" %in% names(train_sf)))\n    train_sf$z_surf &lt;- as.numeric(terra::extract(E,      sf::st_coordinates(train_sf))[,1])\n  if (!(\"slp\" %in% names(train_sf)))\n    train_sf$slp    &lt;- as.numeric(terra::extract(slp_r,  sf::st_coordinates(train_sf))[,1])\n  if (!(\"cosi\" %in% names(train_sf)))\n    train_sf$cosi   &lt;- as.numeric(terra::extract(cosi_r, sf::st_coordinates(train_sf))[,1])\n  if (has_lc && !(\"lc\" %in% names(train_sf))) {\n    lc_codes &lt;- as.integer(terra::extract(lc_r, sf::st_coordinates(train_sf))[,1])\n    lc_codes[is.na(lc_codes)] &lt;- 1L\n    lc_codes &lt;- pmax(1L, pmin(lc_codes, length(lc_levels)))\n    train_sf$lc &lt;- factor(lc_levels[lc_codes], levels = lc_levels)\n  }\n  \n  xy &lt;- as.data.frame(terra::xyFromCell(E, 1:terra::ncell(E))); names(xy) &lt;- c(\"x\",\"y\")\n  grid_df &lt;- xy\n  grid_df$z_surf &lt;- as.numeric(terra::values(E))\n  grid_df$slp    &lt;- as.numeric(terra::values(slp_r))\n  grid_df$cosi   &lt;- as.numeric(terra::values(cosi_r))\n  if (has_lc) {\n    lc_codes &lt;- as.integer(terra::values(lc_r))\n    lc_codes[!is.finite(lc_codes)] &lt;- 1L\n    lc_codes &lt;- pmax(1L, pmin(lc_codes, length(lc_levels)))\n    grid_df$lc &lt;- factor(lc_levels[lc_codes], levels = lc_levels)\n  }\n  grid_sf &lt;- sf::st_as_sf(grid_df, coords = c(\"x\",\"y\"),\n                          crs = sf::st_crs(train_sf), remove = FALSE)\n  \n  use_lc &lt;- has_lc && (\"lc\" %in% names(train_sf)) && (\"lc\" %in% names(grid_sf))\n  if (use_lc) {\n    lev &lt;- levels(droplevels(factor(train_sf$lc)))\n    train_sf$lc &lt;- factor(as.character(train_sf$lc), levels = lev)\n    grid_sf$lc  &lt;- factor(as.character(grid_sf$lc),  levels = lev)\n    if (anyNA(train_sf$lc) || anyNA(grid_sf$lc)) {\n      use_lc &lt;- FALSE; train_sf$lc &lt;- NULL; grid_sf$lc &lt;- NULL\n    }\n  }\n  \n  pred_list &lt;- list()\n  if (\"Voronoi\" %in% models) pred_list$Voronoi &lt;- pred_Voronoi(train_sf, grid_sf)\n  if (\"IDW\"     %in% models) pred_list$IDW     &lt;- pred_IDW    (train_sf, grid_sf, idp = 2)\n  if (\"OK\"      %in% models) pred_list$OK      &lt;- pred_OK     (train_sf, grid_sf)\n  if (\"KED\"     %in% models) pred_list$KED     &lt;- pred_KED    (train_sf, grid_sf)\n  if (\"RF\"      %in% models) {\n    dtr &lt;- sf::st_drop_geometry(train_sf)\n    rf_vars &lt;- c(\"x\",\"y\",\"z_surf\",\"slp\",\"cosi\", if (use_lc) \"lc\")\n    dtr &lt;- stats::na.omit(dtr[, c(\"temp\", rf_vars), drop = FALSE])\n    pred_list$RF &lt;- if (nrow(dtr) &gt;= 5) {\n      rf &lt;- randomForest::randomForest(stats::as.formula(paste(\"temp ~\", paste(rf_vars, collapse = \" + \"))),\n                                       data = dtr, na.action = na.omit)\n      as.numeric(stats::predict(rf, sf::st_drop_geometry(grid_sf)[, rf_vars, drop = FALSE]))\n    } else rep(NA_real_, nrow(grid_sf))\n  }\n  if (\"GAM\"     %in% models) pred_list$GAM     &lt;- pred_GAM    (train_sf, grid_sf)\n  \n  pred_df &lt;- dplyr::bind_rows(lapply(names(pred_list), function(nm) {\n    tibble::tibble(model = nm, x = grid_df$x, y = grid_df$y, pred = pred_list[[nm]])\n  }))\n  \n  make_r &lt;- function(vals, template = E) { r &lt;- terra::rast(template); terra::values(r) &lt;- as.numeric(vals); r }\n  pred_rasters &lt;- lapply(pred_list, make_r)\n  \n  truth_df &lt;- as.data.frame(truth_raster, xy = TRUE, na.rm = FALSE)\n  names(truth_df) &lt;- c(\"x\",\"y\",\"truth\")\n  lims &lt;- stats::quantile(truth_df$truth, probs = stretch_q, na.rm = TRUE)\n  \n  p_pred &lt;- ggplot2::ggplot(pred_df, ggplot2::aes(x, y, fill = pred)) +\n    ggplot2::geom_raster() +\n    ggplot2::scale_fill_gradientn(colors = temp_palette(256), limits = lims,\n                                  oob = scales::squish, name = \"Temp\") +\n    ggplot2::coord_equal() + ggplot2::theme_minimal() +\n    ggplot2::labs(title = sprintf(\"%s — Predictions by model\", which_time),\n                  x = \"Easting\", y = \"Northing\") +\n    ggplot2::facet_wrap(~ model, ncol = 3)\n  \n  p_truth &lt;- ggplot2::ggplot(truth_df, ggplot2::aes(x, y, fill = truth)) +\n    ggplot2::geom_raster() +\n    ggplot2::scale_fill_gradientn(colors = temp_palette(256), limits = lims,\n                                  oob = scales::squish, name = \"Temp\") +\n    ggplot2::coord_equal() + ggplot2::theme_minimal() +\n    ggplot2::labs(title = sprintf(\"%s — Truth raster\", which_time),\n                  x = \"Easting\", y = \"Northing\")\n  \n  list(pred_df = pred_df, pred_rasters = pred_rasters, p_pred = p_pred, p_truth = p_truth)\n}\n```\n\n\n\nScenarios (data & geometry): scenarios/ + registry.R. Each scenario provides a make() factory returning a standard object contract (see below). registry.R maps a name to its builder; source_scenario(name) returns the make() function. Scenarios control domain rasters, station sets, and recommended block size.\n\n\n\nCode\n```{r}\n#| eval: false\n# =====================================================================\n# block4_5/scenarios/lake_bump_dense.R\n# Benötigt: source(\"block4_5/src/pipemodel_functions.R\") davor\n# Definiert: SCEN_NAME, SCEN_DESC, make(...)\n# =====================================================================\n\nSCEN_NAME &lt;- \"lake_bump_dense\"\nSCEN_DESC &lt;- \"Valley with lake (right), bump hill (left) + dense micro-hills; random stations.\"\n\n# Defaults so wie bisher genutzt (nur Settings/Parameter)\n.defaults &lt;- list(\n  # Domain\n  center_E = 600000, center_N = 5725000,\n  len_x = 600, len_y = 400, res = 10, crs = \"EPSG:32632\",\n  \n  # Topographie-Features\n  lake_mode = \"water\",   # \"none\" | \"water\" | \"hollow\"\n  hill_mode = \"bump\",    # \"none\" | \"bump\"\n  lake_diam_m  = 100, lake_depth_m = 500, smooth_edges = FALSE,\n  hill_diam_m  = 100, hill_height_m = 500, hill_smooth  = FALSE,\n  \n  # micro-relief\n  random_hills        = 50,\n  micro_hill_diam_m   = 30,\n  micro_hill_height_m = 50,\n  micro_hill_smooth   = FALSE,\n  micro_seed          = NULL,\n  \n\n  # Sonne/Geo\n  lat = 51.8, lon = 10.6, sun_date = as.Date(\"2024-06-21\"),\n  \n  # Stationen\n  station_mode = \"random\",  # \"random\" | \"ns_transect\" | \"ew_transect\"\n  n_st = 60,\n  transect_margin_m = 10,\n  ns_offset_m = 0,\n  ew_offset_m = 0,\n  \n  # Modelle + Block-CV\n  models = c(\"Voronoi\",\"IDW\",\"OK\",\"KED\",\"RF\",\"GAM\"),\n  block_size = NA_real_    # wenn NA -&gt; automatisch berechnet\n)\n\n# einfaches Mergen (ohne Seiteneffekte)\n.merge &lt;- function(a, b) { a[names(b)] &lt;- b; a }\n\n# ---------------------------------------------------------------------\n# make(overrides = list(), do_cv = FALSE)\n# baut Domain -&gt; Szenario -&gt; Stationen -&gt; (optional) CV\n# nutzt ausschließlich Funktionen aus pipemodel_functions.R\n# ---------------------------------------------------------------------\nmake &lt;- function(overrides = list(), do_cv = FALSE) {\n  p &lt;- .merge(.defaults, overrides)\n  \n  # 1) Domain\n  domain &lt;- make_domain(\n    center_E = p$center_E, center_N = p$center_N,\n    len_x = p$len_x, len_y = p$len_y, res = p$res, crs = p$crs\n  )\n  \n  # 2) Szenario (Topographie/Physikfelder)\n  scen &lt;- build_scenario(\n    domain       = domain,\n    lake_mode    = p$lake_mode,\n    hill_mode    = p$hill_mode,\n    \n    # &lt;&lt;&lt; diese Zeilen fehlten bisher\n    lake_diam_m  = p$lake_diam_m,\n    lake_depth_m = p$lake_depth_m,\n    smooth_edges = p$smooth_edges,\n    hill_diam_m  = p$hill_diam_m,\n    hill_height_m= p$hill_height_m,\n    hill_smooth  = p$hill_smooth,\n    # &gt;&gt;&gt;\n    \n    # micro-relief\n    random_hills        = p$random_hills,\n    micro_hill_diam_m   = p$micro_hill_diam_m,\n    micro_hill_height_m = p$micro_hill_height_m,\n    micro_hill_smooth   = p$micro_hill_smooth,\n    micro_seed          = p$micro_seed,\n    \n    # Sonne/Geo\n    lat = p$lat, lon = p$lon, sun_date = p$sun_date\n  )\n  \n  # 3) Stationen\n  pts_sf &lt;- make_stations(\n    domain,\n    n_st = p$n_st,\n    station_mode = p$station_mode,\n    transect_margin_m = p$transect_margin_m,\n    ns_offset_m = p$ns_offset_m,\n    ew_offset_m = p$ew_offset_m\n  )\n  \n  # 4) Station-Features/Targets extrahieren\n  stns &lt;- stations_from_scenario(scen, pts_sf)\n  stn_sf_14 &lt;- stns$T14\n  stn_sf_05 &lt;- stns$T05\n  \n  # 5) Blockgröße\n  block_size &lt;- if (is.finite(p$block_size)) {\n    as.numeric(p$block_size)\n  } else {\n    compute_block_size(\n      len_x = domain$xmax - domain$xmin,\n      len_y = domain$ymax - domain$ymin,\n      n_st  = p$n_st\n    )\n  }\n  \n  out &lt;- list(\n    name       = SCEN_NAME,\n    desc       = SCEN_DESC,\n    params     = p,\n    domain     = domain,\n    scen       = scen,\n    pts_sf     = pts_sf,\n    stn_sf_14  = stn_sf_14,\n    stn_sf_05  = stn_sf_05,\n    block_size = block_size\n  )\n  \n  # 6) Optional: Block-CV (nur wenn gewünscht)\n  if (isTRUE(do_cv)) {\n    out$cv &lt;- list(\n      T14 = run_for_time(stn_sf_14, scen$R14, \"T14\",\n                         scen_local = scen, block_m = block_size, models = p$models),\n      T05 = run_for_time(stn_sf_05, scen$R05, \"T05\",\n                         scen_local = scen, block_m = block_size, models = p$models)\n    )\n  }\n  \n  out\n}\n# =====================================================================\n```",
    "crumbs": [
      "FAQ",
      "Lectures",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "reader/mc_2025_1.html#pipeline-stages",
    "href": "reader/mc_2025_1.html#pipeline-stages",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "Pipeline stages",
    "text": "Pipeline stages\n\nSetup model\n\nSetup Source packages.R; disable s2 for robust planar ops in projected UTM domains; set seeds. Source core helpers and the scenario registry.\nScenario selection & build Select scen_name via Sys.getenv(\"SCEN\", \"&lt;default&gt;\"). make_fun &lt;- source_scenario(scen_name) → obj &lt;- make_fun().\nSanity checks Assert E, R14, R05 exist; sun$T14/sun$T05 include numeric alt,az.\nLive preview (no side effects) Quick plots to catch wiring/CRS issues early: land-cover + terrain overview, 2×2 domain panel, a scenario preview.\n\n\n\n\n\n\n\n\n\n\n\nOverview Setup\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep\nFunction/Call\nPurpose\nKey args (examples)\nOutput\n\n\n\n\nS1\nsource(here::here(\"block4_5/src/packages.R\"))\nLoad packages & global options\n–\nPackages loaded\n\n\nS2\nsf::sf_use_s2(FALSE)\nRobust planar ops in UTM\n–\ns2 disabled\n\n\nS3\nsource(here::here(\"block4_5/src/fun_pipemodel.R\"))\nPlot/feature/scale utilities\n–\nHelper funcs\n\n\nS4\nsource(here::here(\"block4_5/src/fun_learn_predict_core.R\"))\nLearn/validate/predict core\n–\nCore funcs\n\n\nS5\nsource(here::here(\"block4_5/scenarios/registry.R\"))\nScenario registry\n–\nsource_scenario()\n\n\nS6\nmake_fun &lt;- source_scenario(scen_name)\nSelect scenario\nscen_name\nmake() factory\n\n\nS7\nobj &lt;- make_fun()\nBuild scenario object\noverrides, do_cv (opt.)\nscen, stn_sf_14, stn_sf_05, block_size, params\n\n\n\nScenario object obj (returned by make()):\n\nscen: named list of rasters and metadata\n\nE: DEM (reference geometry)\nR14, R05: truth rasters for ~14 UTC and ~05 UTC\nlc: land-cover raster (optional; with scen$lc_levels)\nsun: list with T14 and T05, each having numeric alt and az (mandatory for R* tuning)\noptional helpers (illumination, indices) as needed by features/plots\n\nstn_sf_14, stn_sf_05: sf point layers with at least temp and covariate fields\nblock_size: integer (meters) used for leave-block-out CV\nparams$models: character vector of learner names to run\n\nTuned feature rasters @ R* (produced later):\n\nEs (smoothed DEM), slp (slope from Es), cosi (cosine of incidence given sun alt,az)\n\n\n\n\n\n\nLearning and Predicting\n\nBaseline learning & maps For each time slice (T14, T05):\n\nrun_for_time(st, R, \"Txx\", scen_local=scen, block_m=block_size, models=mods)\nProduces: CV metrics (MAE/RMSE/R²), blocks plot (spatial CV folds), diag plot (pred vs obs), truth & prediction maps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do we see?\nThe pipeline is behaving sensibly: block CV exposes how much of the error comes from sensors versus spatial structure. At daytime (T14), most error is already “accounted for” by instrumentation; what remains splits between fine-scale heterogeneity and broader structure. At pre-dawn (T05), fields are smoother at the choosen station spacing: after peeling off instrument noise, almost all leftover error is mesoscale—i.e., large-scale processes your mean/residual model doesn’t yet capture.\n\n\nDaytime (T14)\n\nWhat the errors mean: Sensors do a lot of the damage; once you discount that, the remainder is balanced: some is truly microscale (representativeness—canopy edges, roughness, small topographic breaks), and some is mesoscale (gentle gradients / structure not fully in the drift).\nWhat you likely see on maps: Predictions look coherent, but local pockets near environmental transitions (edge effects, slope/aspect shifts) show residual texture. Scatter plots are tight with mild spread; block-wise boxplots show similar medians across your better models.\nModel behavior: Simple geometric baselines (Voronoi/IDW) are adequate as references but can over- or under-smooth. KED/GAM/RF generally capture the daytime drift better; their residuals still show a bit of spatial correlation, which is consistent with the micro+meso split.\n\nImplication: Improvements now come from targeted features (multi-scale terrain/roughness/canopy, radiation states) and, if residuals are directional, anisotropic/cost-aware residual modeling. More raw smoothing won’t help much.\n\n\nPre-dawn (T05)\n\nWhat the errors mean: After instrument noise, the leftover error is overwhelmingly mesoscale. That points to organised nocturnal processes—cold-air drainage, pooling, gentle advection—operating at scales larger than your microstructure and not fully encoded by the current drift.\nWhat you likely see on maps: Smooth, broad residual swaths aligned with terrain and flow paths rather than speckled local noise. Scatter remains fair, but block-wise errors can vary between blocks that sit on/away from drainage lines.\nModel behavior: Purely geometric methods under-represent the organised night-time structure; RF/GAM help if fed flow-aware features, but without those drivers their residuals still carry structure. Kriging alone won’t fix it unless the metric (anisotropy, barriers, cost distance) matches the physics.\n\nImplication: Focus on process-scale cues: flow-aligned neighborhoods, barriers across ridgelines, cost/geodesic distances that penalise uphill motion, and drift terms that proxy stratification (e.g., cold-air pathways, sky-view, topographic indices at appropriate scales). If residual correlation persists after that, add residual kriging with an anisotropic/cost metric.\n\n\nPractical takeaways\n\nYou’re not bottlenecked by algorithm choice; you’re bottlenecked by process representation:\n\nT14: refine multi-scale features (terrain filters, canopy/roughness, radiation states) and allow directional residuals where wind/valley orientation matters.\nT05: elevate flow physics in the mean and residual metric; think valley graphs, cost distance, barriers.\n\nSampling/design: If T14 micro noise bothers you, micro-targeted placements (edges, small basins) or denser spacing help. For T05 meso, ensure transects across valley axes and basin outlets so the model “sees” the gradients it must learn.\nQC nudge: If any model shows physically implausible tails (e.g., extreme GAM values at night), tighten smoothness or clip to plausible ranges and revisit feature scaling.\n\nBottom line: baseline runs already tell a coherent story—daytime limits are representativeness + mild missing drift; night-time limits are larger-scale drainage/advection. Aim your next changes at process-aware features and matching the residual metric to the physics rather than chasing extra smoothing.\n\nScale inference & tuning\n\nVariogram → L50/L95: compute_Ls_from_points(st, value_col=\"temp\") returns empirical variogram and correlation scales; plotting highlights L50/L95.\nU-curve → R*: tune_Rstar_ucurve(st, E, alt, az, L50, L95, block_fallback, n_grid) scans smoothing radii around [L50, L95] to find the RMSE-minimizer R* via spatial CV.\nFeatures @ R*: smooth_dem_and_derive(E, alt, az, radius_m=R*) → Es, slp, cosi.\nRe-extract station features @ R*: add_drifts_at_R(st, E, alt, az, R*, lc=..., lc_levels=...) aligns training features with tuned rasters.\nTuned CV: run_lbo_cv(st_R, E=scen$E, block_size=block_size, models=mods) → updated metrics and CV table.\nTuned maps: predict_maps(st_R, truth_raster=Rxx, which_time=\"Txx\", scen, models, lc_levels, feature_rasters=list(E=Es, slp=slp, cosi=cosi)).\nPanels & diagnostics: build_panels_truth_preds_errors_paged(...) shows truth | predictions | residual diagnostics.\nError budgets (optional): simple_error_budget(...) aggregates instrument/micro/meso components; plot_error_budget() stacks them for base vs tuned.\n\n\n\n\n{=html} &lt;iframe width=\"780\" height=\"500\" src=\"results_lake_bump_dense/tab/metrics_T14_tuned_lake_bump_dense.html\" title=\"Voronoi\"&gt;&lt;/iframe&gt; {=html} &lt;iframe width=\"780\" height=\"500\" src=\"results_lake_bump_dense/tab/metrics_T05_tuned_lake_bump_dense.html\" title=\"Voronoi\"&gt;&lt;/iframe&gt; ::: {.callout-note title=“Modeling, Prediction & Tuning” bucon=“false” collapse=“true”}\n\n\n\n\n\n\n\n\n\n\n\n\nStep\nFunction/Call\nPurpose\nKey args (examples)\nOutput\n\n\n\n\n\n\nP1\nplot_landcover_terrain(scen, stations = st14, layout=\"vertical\")\nQuick domain sanity check\nscen, stations\nggplot\n\n\n\n\nP2\nplot_block_overview_2x2_en(scen, pts_sf = st14)\n2×2 overview\nscen, pts_sf\nggplot\n\n\n\n\nP3\npreview_scenario(obj)\nScenario preview\nobj\nPanels/plots\n\n\n\n\nP4\nrun_for_time(st14, scen$R14, \"T14\", scen_local = scen, block_m = bs, models = mods)\nBaseline LBO-CV + maps (T14)\nstations, truth raster, blocks, models\nres={metrics, blocks_plot, diag_plot}, maps={p_truth, p_pred}\n\n\n\n\nP5\nrun_for_time(st05, scen$R05, \"T05\", …)\nBaseline (T05)\nas above\nas above\n\n\n\n\nP6\nLs14 &lt;- compute_Ls_from_points(st14, value_col=\"temp\")\nVariogram & scales (T14)\nstations, value col\n{vg, L50, L95, sill}\n\n\n\n\nP7\nplot_variogram_with_scales(Ls14$vg, Ls14$L50, Ls14$L95, Ls14$sill, \"…\")\nVariogram plot\nvariogram + scales\nggplot\n\n\n\n\nP8\ntune_Rstar_ucurve(st14, scen$E, alt=scen$sun$T14$alt, az=scen$sun$T14$az, L50=Ls14$L50, L95=Ls14$L95, block_fallback=bs, n_grid=6)\nU-curve → R* (T14)\nstations, DEM, sun, L50/L95\n{grid, R_star, block_m}\n\n\n\n\nP9\nfr14 &lt;- smooth_dem_and_derive(scen$E, scen$sun$T14$alt, scen$sun$T14$az, radius_m=tune14$R_star)\nFeatures @ R*\nDEM, sun, R*\n{Es, slp, cosi}\n\n\n\n\nP10\nst14_R &lt;- add_drifts_at_R(st14, scen$E, scen$sun$T14$alt, scen$sun$T14$az, tune14$R_star, lc=scen$lc, lc_levels=scen$lc_levels)\nStation features @ R*\nstations, DEM, sun, R*, LC\nsf with drifts\n\n\n\n\nP11\nbench14 &lt;- run_lbo_cv(st14_R, E=scen$E, block_size=bs, models=mods)\nTuned LBO-CV (T14)\ntuned stations, DEM, blocks, models\n{metrics, cv, …}\n\n\n\n\nP12\nmaps14_tuned &lt;- predict_maps(st14_R, truth_raster=scen$R14, which_time=\"T14\", scen=scen, models=mods, lc_levels=scen$lc_levels, feature_rasters=list(E=fr14$Es, slp=fr14$slp, cosi=fr14$cosi))\nMaps @ R* (T14)\ntuned stations + features\nPred rasters/plots\n\n\n\n\nP13\npanel_T14 &lt;- build_panels_truth_preds_errors_paged(maps14_tuned, scen$R14, bench14$cv, \"T14\", models_per_page=7, scatter_next_to_truth=TRUE)\nTruth\npreds\nresiduals panel\nmaps, truth, CV\nlist of ggplot\n\n\nP14\nsimple_error_budget(run14$res, sigma_inst=0.5, alpha=0.6)\nError budget (baseline/tuned)\nCV results, params\ntidy table\n\n\n\n\n\n\nMirror P6–P14 for T05 with st05, R05, sun$T05.\n\n:::\n\n\nSaving Results\n\nExports (optional, end-only side effects) Controlled by export &lt;- TRUE/FALSE. When TRUE:\n\nCreate results_&lt;scen-name&gt;/{fig,tab,ras}.\nSave baseline and tuned plots (previews, CV blocks/diag, truth/pred, variograms, U-curves, panels).\nSave tables (metrics, U-curve grid, scales L50/L95/R*, error budgets).\nSave rasters (E, R14, R05, lc if present).\nWrite sessionInfo().\n\n\n\n\n\n\n\n\nDocumentation & Export\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep\nFunction/Call\nPurpose\nKey args (examples)\nOutput\n\n\n\n\nD1\nfn_fig(\"name\"), fn_ras(\"name\")\nBuild output paths\nstems, ext\nfile paths\n\n\nD2\nsave_plot_min(p, fn_fig(\"plot_name\"))\nSave plot safely\nggplot, size, dpi\nPNG\n\n\nD3\nsafe_save_plot(p, fn_fig(\"plot_name\"))\nTry-save without aborting\nplot, path\nPNG (best-effort)\n\n\nD4\nsave_table_readable(df, file_stem, title=..., digits=3)\nSave tables\ndata.frame, stem\nCSV/HTML/XLSX\n\n\nD5\nsave_raster_min(r, fn_ras(\"raster_name\"))\nSave raster (GeoTIFF)\nSpatRaster, path\nTIF\n\n\nD6\nterra::writeRaster(r, fn_ras(\"name\"), overwrite=TRUE)\nLow-level raster write\nraster, path\nTIF\n\n\nD7\nsaveRDS(sessionInfo(), file.path(out_dir, sprintf(\"%s_sessionInfo.rds\", scen_name)))\nSession record\nscen name\nRDS",
    "crumbs": [
      "FAQ",
      "Lectures",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "reader/mc_2025_1.html#headline",
    "href": "reader/mc_2025_1.html#headline",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "Headline",
    "text": "Headline\n\nAccuracy: Day (T14) ~0.45 °C RMSE, Pre-dawn (T05) ~0.60 °C RMSE — similar before/after tuning.\nError structure:\n\nT14: sensors dominate; leftover split between micro and meso.\nT05: sensors ~half; remainder is mostly mesoscale (smooth nocturnal structure).",
    "crumbs": [
      "FAQ",
      "Lectures",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "reader/mc_2025_1.html#daytime-t14-1",
    "href": "reader/mc_2025_1.html#daytime-t14-1",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "Daytime (T14)",
    "text": "Daytime (T14)\nBaseline (LBO-CV)\n\nRMSE: 0.452 °C (Total var 0.206 °C²)\nInstrument: 0.116 °C² (~56%)\nMicroscale: 0.0496 °C² (~24%)\nMesoscale: 0.0401 °C² (~20%) Reading: After removing sensor noise, remaining error is fairly balanced between sub-grid heterogeneity (representativeness) and broader, unmodelled structure.\n\nTuned (R* features)\n\nRMSE: essentially unchanged (≈ 0.45 °C).\nBudget: slices stay close to baseline (minor shifts only). Reading: Tuning mainly harmonizes feature scale (R*) and improves map coherence; it doesn’t move headline error much.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "reader/mc_2025_1.html#pre-dawn-t05-1",
    "href": "reader/mc_2025_1.html#pre-dawn-t05-1",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "Pre-dawn (T05)",
    "text": "Pre-dawn (T05)\nBaseline (LBO-CV)\n\nRMSE: 0.600 °C (Total var 0.362 °C²)\nInstrument: 0.185 °C² (~51%)\nMicroscale: ≈ 0 °C² (~0%)\nMesoscale: 0.177 °C² (~49%) Reading: Nearly all non-instrument error is mesoscale — consistent with smooth nocturnal fields (drainage/advection, stratification) at your station spacing.\n\nTuned (R* features)\n\nRMSE: essentially unchanged (≈ 0.60 °C).\nBudget: instrument remains ~51%; remainder still meso-heavy. Reading: Tuning doesn’t change the picture: add process-scale structure rather than more smoothing.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "reader/mc_2025_1.html#what-to-do-next-precise-minimal",
    "href": "reader/mc_2025_1.html#what-to-do-next-precise-minimal",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "What to do next (precise & minimal)",
    "text": "What to do next (precise & minimal)\n\nSensors (both times): sanity-check σ_inst using co-location/specs; improve shielding/siting if instrument slice looks high.\nT14 (micro + meso):\n\nMicro: enrich multi-scale terrain/roughness/canopy features; consider slightly smaller R* or multi-radius features.\nMeso: add/strengthen process covariates (radiation states, exposure), and allow anisotropy/cost in residuals where flow/wind channel influence.\n\nT05 (meso-dominated): favor flow-aligned neighborhoods, barriers across ridgelines, and advection/drainage proxies; if RF/GAM residuals remain correlated, add residual kriging (possibly anisotropic/cost-based).",
    "crumbs": [
      "FAQ",
      "Lectures",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "reader/mc_2025_1.html#how-to-present-clear-story",
    "href": "reader/mc_2025_1.html#how-to-present-clear-story",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "How to present (clear story)",
    "text": "How to present (clear story)\n\nPer time slice: metrics table → obs-pred scatter → block-wise error boxplots → truth vs prediction maps → stacked error-budget bars (Instrument / Micro / Meso).\nThen show the same row for tuned to highlight that scale harmonization improved map consistency while headline RMSE stayed stable.\n\nBottom line: Your models are already accurate at T14 and reasonable at T05. The limiting factor during the day is representativeness + mild missing structure; at night it’s large-scale process not yet in the mean/residual model. Focus on process-aware features and anisotropic/cost residuals, not more smoothing.\n\nError budget — idea, concept, and purpose\nIdea. Turn the overall CV error (\\(\\sigma_{\\text{cv}}^2 \\approx \\text{RMSE}^2\\)) into a story of where it comes from:\n\nInstrument (sensor noise you assume),\nMicroscale (sub-grid, representativeness),\nMesoscale (larger-scale, unmodelled structure).\n\nConcept.\n\nFirst peel off a fixed instrument part using your sensor SD (sigma_inst).\nSplit the leftover between micro and meso with α.\n\nPrefer α from the residual variogram’s nugget fraction (on CV residuals): nugget ≈ micro.\nOtherwise use a site heuristic (open 0.2–0.4; mixed 0.4–0.6; complex 0.6–0.8).\n\n\nWhat for.\n\nDiagnose limits: Is error dominated by instrument, micro (representativeness), or meso (missing process/scale)?\nGuide action:\n\nBig instrument → sensor QA, shielding, calibration.\nBig micro → finer scale (R*), denser stations, better canopy/roughness features.\nBig meso → add process covariates (drift), anisotropy/cost metrics, rethink neighborhood/blocks.\n\nCompare models/scenarios: Same RMSE can hide very different error structures.\nCommunicate uncertainty: Bars in °C² (optionally show SD by √).\n\nWhy σ_inst and α.\n\nσ_inst externalizes known noise (you choose it from specs/co-location).\nα makes the remaining variance interpretable by scale: micro (&lt; ~R*/2) vs meso (≳ ~R*).\n\nPractical workflow.\n\nCompute CV errors → overall variance.\nSet σ_inst.\nGet α from residual variogram nugget fraction (per time/model), fallback to heuristic.\nReport instrument / micro / meso components; act on the largest slice.\n\nHere’s the plain-English read of your results.\n\n\nT14 (daytime)\n\nTotal variance: 0.206 °C² (RMSE ≈ 0.452 °C).\nInstrument: 0.116 °C² → ~56% of total (SD ≈ 0.34 °C).\nMicroscale: 0.0496 °C² → ~24% (SD ≈ 0.22 °C).\nMesoscale: 0.0401 °C² → ~20% (SD ≈ 0.20 °C).\n\nInterpretation: Most error is explained by sensor noise. The remaining ~44% splits fairly evenly between sub-grid/representativeness (micro) and larger-scale unmodeled structure (meso). Both small-scale heterogeneity and some broader process signal are still in play.\n\n\nT05 (pre-dawn)\n\nTotal variance: 0.362 °C² (RMSE ≈ 0.600 °C).\nInstrument: 0.185 °C² → ~51% of total (SD ≈ 0.43 °C).\nMicroscale: ≈ 0.\nMesoscale: 0.177 °C² → ~49% (SD ≈ 0.42 °C).\n\nInterpretation: About half the error is sensor noise. Nearly all of the leftover is mesoscale, i.e., smoother, broader structure that the mean/residual model hasn’t captured (e.g., nocturnal drainage patterns, advection, or missing covariates). A near-zero micro slice is plausible at night when fields are smoother at the station spacing.\n\n\nNet takeaway\n\nT14: Sensor noise dominates; the rest is a balanced mix of micro and meso → both better micro-scale features (or finer R*) and some added process covariates could help.\nT05: Sensor ~half; remainder is meso-dominated → focus on process/scale (e.g., flow-aligned/anisotropic metrics, cost distance, additional nocturnal drivers).",
    "crumbs": [
      "FAQ",
      "Lectures",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "reader/cd-1.html",
    "href": "reader/cd-1.html",
    "title": "Change Detection - Sentinel",
    "section": "",
    "text": "In the geosciences, remote sensing is the only measurement technique that allows complete coverage of large spatial areas, up to the entire Earth’s surface. Its successful application requires both the use of existing methods and the adaptation and development of new ones.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial data classification"
    ]
  },
  {
    "objectID": "reader/cd-1.html#introduction",
    "href": "reader/cd-1.html#introduction",
    "title": "Change Detection - Sentinel",
    "section": "Introduction",
    "text": "Introduction\nIn geospatial or environmental informatics, the detection of changes to the Earth’s surface using satellite, aircraft or drone images, known as change detection analysis, is an important application. These results are often linked to biophysical, geophysical or anthropogenic processes in order to gain both a deeper understanding and the possibility of developing predictive models. Methods of image analysis are of outstanding importance for generating spatial information from the underlying processes. Since both the quantity and quality of this “image data” are playing an increasingly important role in environmental monitoring and modeling, it is becoming more and more necessary to integrate “big data” concepts into the analyses. This means performing reproducible analyses with large amounts of data (&gt;&gt; 1 TB). This is essential for both scientific knowledge gain and future societal challenges.\nAs already explained in the introduction, we start with a scalable change detection analysis of forest damage in low mountain ranges, which is a typical application-oriented task. Scalable means that we limit the analysis to a manageable area, the Nordwestharz, and to two time slices. However, the resulting algorithm can be applied to different or larger areas and to more time slices.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial data classification"
    ]
  },
  {
    "objectID": "reader/cd-1.html#goals",
    "href": "reader/cd-1.html#goals",
    "title": "Change Detection - Sentinel",
    "section": "Goals",
    "text": "Goals\nThis example shows how change detection methods can be applied conventionally to individual satellite scenes and in a modern way in cloud computing environments using rstac (Brazil Data Cube Team 2021) and gdalcubes (Appel und Pebesma 2019) or openeo (Lahn 2024). In addition to classical supervised classification methods such as Maximum Likelihood and Random Forest, the bfast (Verbesselt, Zeileis, und Herold 2012) is used, which includes an unsupervised method for detecting structural breaks in vegetation index time series.\nOther packages used in this tutorial include stars (Pebesma 2019), tmap (Tennekes 2018) and mapview (Appelhans u. a. 2019) for creating interactive maps, sf (Pebesma 2018) for processing vector data, and colorspace (Zeileis u. a. 2020) for visualizations with accessible colors.\nThis study employs a variety of approaches to time series and difference analyses with different indices, using the Harz Mountains as a case study for the period between 2018 and 2023. The objective is to analyze or classify the data.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial data classification"
    ]
  },
  {
    "objectID": "reader/cd-1.html#information-from-satellite-imagery",
    "href": "reader/cd-1.html#information-from-satellite-imagery",
    "title": "Change Detection - Sentinel",
    "section": "Information from satellite imagery",
    "text": "Information from satellite imagery\nUnprocessed satellite images are not necessarily informative. While our eyes can interpret a true-color image relatively conclusively and intuitively, a reliable and reproducible, i.e. scientifically sound, interpretation requires other approaches. A major advantage of typical image analysis methods over visual interpretation is the derivation of additional, so-called invisible information.\nTo obtain useful or meaningful information, e.g. about the land cover in an area, we have to analyze the data according to the question at hand. Probably the best known and most widely used approach is the supervised classification of image data into categories of interest.\nIn this unit, you will learn about the classification of satellite image data. This includes both data acquisition on the Copernicus portal and the various steps from digitising the training data to evaluating the quality of the classifications.\nWe will cover the following topics:\n\nTheoretical principles\nCase Study Harz Mountains\n\nPreparing the work environment\nRetrieving Sentinel and auxilliary data\nUnsupervised classification (k-means clustering)\nRecording training areas\nSupervised classification (Random Forest, Maximum Likelihood)\nEstimating model quality",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial data classification"
    ]
  },
  {
    "objectID": "reader/cd-1.html#theoretical-principles",
    "href": "reader/cd-1.html#theoretical-principles",
    "title": "Change Detection - Sentinel",
    "section": "Theoretical principles",
    "text": "Theoretical principles\nPlease note that all types of classification usually require extensive data pre-processing. The focus is then on model building and quality assessment, which can be seen as the technical basis for classification, in order to finally derive the interpretation of the results in terms of content in the data post-processing.\nWe will go through this process step by step.\n\nUnsupervised Classification - k-means clustering\nProbably the best-known unsupervised classification technique is K-means clustering, which is also referred to as the “simplest machine learning algorithm”.\nK-means clustering is a technique commonly used in satellite image classification to group pixels with similar spectral characteristics. Treating each pixel as an observation, the algorithm assigns pixels to clusters based on their spectral values, with each cluster having a mean (or centroid) that represents its central spectral signature. This results in the segmentation of the image into distinct regions (similar to Voronoi cells) corresponding to land cover types, such as water, vegetation or urban areas, facilitating further analysis. It is often used to obtain an initial overview of whether the raster data can be sufficiently separated in feature space.\n\nFigure: Convergence of k-means clustering from an unfavorable starting position (two initial cluster centers are fairly close). Chire [CC BY-SA 4.0] via wikipedia.org\n\n\nSupervised classification\nIn supervised land cover classification, a model is derived from a limited amount of training land cover data that predicts land cover for the entire data set. The land cover types are defined a priori, and the model attempts to predict these types based on the similarity between the characteristics of the training data and the rest of the data set.\n\nClassifiers (e.g. the maximum likelihood classifier) or machine learning algorithms (such as Random Forest) use the training data to determine descriptive models that represent statistical signatures, classification trees or other functions. Within the limits of the quality of the training data, such models are suitable and representative for making predictions for areas if the predictors from the model are available for the entire area.\nWe now want to predict the spatial characteristics of clear-felling/no forest using a maximum likelihood classification and random forest, and apply standard methods of random validation and model quality assessment.\nThe goal is to separate clearcuts from all other pixels and to quantify the differences between 2018 and 2022.\n\nMaximum Likelihood Classification\nMaximum likelihood classification assumes that the distribution of data for each class and in each channel is normally distributed. Under this assumption, the probability that a particular pixel belongs to a particular class is calculated. Since the probabilities can also be specified as a threshold, without this restriction, all pixels are assigned regardless of how unlikely they are. Each pixel is assigned to the class that has the highest probability (i.e., the maximum probability).\n\n\n\nRandom forest\nRandom forests can be used for both regression and classification tasks, with the latter being particularly relevant in environmental remote sensing. Like any machine learning method, the random forest model learns to recognize patterns and structures in the data itself. Since the random forest algorithm also requires training data, it is also a supervised learning method.\n\nFigure: Simplified illustration of data classification by random forest during training. Venkata Jagannath [CC BY-SA 4.0] via wikipedia.org\nFrom a pragmatic point of view, classification tasks generally require the following steps:\n\nCreation of a comprehensive input data set that contains one or more raster layers.\nSelection of training areas, i.e. subsets of the input data set for which the land cover type is known to the remote sensing expert. Knowledge of the land cover can be obtained, for example, from one’s own or third-party in situ observations, management information or other remote sensing products (e.g. high-resolution aerial photographs).\nTraining a model using the training sites. For validation purposes, the training sites are often subdivided into one or more test and training sites to evaluate the performance of the model algorithm.\nApplying the trained model to the entire data set, i.e. predicting the land cover type based on the similarity of the data at each site to the class characteristics of the training data set.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial data classification"
    ]
  },
  {
    "objectID": "reader/cd-1.html#change-detection-case-study-harz-mountains",
    "href": "reader/cd-1.html#change-detection-case-study-harz-mountains",
    "title": "Change Detection - Sentinel",
    "section": "Change detection case study: Harz Mountains",
    "text": "Change detection case study: Harz Mountains\nSince 2018, there has been a notable increase in the incidence of extensive forest dieback in the Harz Mountains. During this period, the combination of repeated years of drought, extreme heat waves and the resulting weakening of the spruce trees led to an exponential increase in the population of bark beetles. The combined impact of these factors resulted in the extensive mortality of spruce stands across an area of approximately 30,000 hectares over a period of approximately five to six years. This equates to approximately 35% of the total forest area of the Harz.\nIt is important to note that the prolonged drought in 2018, 2019 and 2020, which is considered one of the most severe in the region, has significantly exacerbated the damage in the Harz Mountains.\nIn this context, a time series analysis or a change detection analysis is an essential technique for quantifying and localising the damage and characterising its dynamics.\n\nSetting up the work environment\n\n# ---- 0 Project Setup ----\nrequire(\"pacman\")\n# Achtung: OpenStreetMap nicht laden (zieht rJava)\npacman::p_load(\n  mapview, mapedit, tmap, tmaptools,\n  raster, terra, stars, gdalcubes, sf,\n  webshot, dplyr, downloader, tidyverse,\n  RStoolbox, rprojroot, exactextractr,\n  randomForest, ranger, e1071, caret,\n  link2GI, rstac, colorspace, ows4R, httr\n)\n\nroot_folder &lt;- rprojroot::find_rstudio_root_file()\n\nndvi.col &lt;- function(n) rev(colorspace::sequential_hcl(n, \"Green-Yellow\"))\nano.col  &lt;- colorspace::diverging_hcl(7, palette = \"Red-Green\",  register = \"rg\")\n\nPlease add any missing or defective packages in the above setup script (if error messages occur). On the basis of the available Sentinel data, the first step should be to identify suitable data sets for a surface classification.\n\n\nDefining the Area of Interest\n\n# download training data (also used for extent)\nutils::download.file(\n  url      = \"https://github.com/gisma/gismaData/raw/master/geoinfo/train_areas_2019_2020.gpkg\",\n  destfile = file.path(root_folder, \"data/train_areas_2019_2020.gpkg\"),\n  mode     = \"wb\"\n)\ntrain_areas_2019_2020 &lt;- sf::st_read(file.path(root_folder, \"data/train_areas_2019_2020.gpkg\"), quiet = TRUE)\nbbox &lt;- sf::st_bbox(train_areas_2019_2020)\n\n# CORINE forest mask (optional step, cached)\nif (!file.exists(file.path(root_folder,\"data/corine_harz.tif\"))){\n  corine &lt;- terra::rast(file.path(root_folder,\"data/U2018_CLC2018_V2020_20u1.tif\"))\n  corine &lt;- terra::project(corine,\"EPSG:4326\")\n  corine_harz &lt;- terra::crop(corine, terra::vect(train_areas_2019_2020))\n  terra::writeRaster(corine_harz, file.path(root_folder,\"data/corine_harz.tif\"), overwrite=TRUE)\n}\ncorine_harz &lt;- terra::rast(file.path(root_folder,\"data/corine_harz.tif\"))\n\n# Forest mask codes: 22..25 -&gt; 1, else 0\nrclmat &lt;- matrix(c(-100,22,0, 22,26,1, 26,500,0), ncol = 3, byrow = TRUE)\nharz_forest_mask &lt;- terra::classify(corine_harz, rclmat, include.lowest = TRUE)\n\n# quick view (optional)\n# mapview::mapview(corine_harz) + mapview::mapview(train_areas_2019_2020, zcol=\"class\") + harz_forest_mask",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial data classification"
    ]
  },
  {
    "objectID": "reader/cd-1.html#step-1-retrieving-sentinel-data",
    "href": "reader/cd-1.html#step-1-retrieving-sentinel-data",
    "title": "Change Detection - Sentinel",
    "section": "Step 1: Retrieving Sentinel data",
    "text": "Step 1: Retrieving Sentinel data\n\nAlternative 1: Using gdalcubes with STAC\n\nSTAC & earth-search\n\n# search the data stack for the given period and area\ns &lt;- rstac::stac(\"https://earth-search.aws.element84.com/v0\")\n\nitems &lt;- s |&gt;\n  rstac::stac_search(\n    collections = \"sentinel-s2-l2a-cogs\",\n    bbox       = c(bbox[\"xmin\"], bbox[\"ymin\"], bbox[\"xmax\"], bbox[\"ymax\"]),\n    datetime   = \"2018-06-01/2022-09-01\",\n    limit      = 600\n  ) |&gt;\n  rstac::post_request()\n\nitems\n\n###Items\n- matched feature(s): 627\n- features (600 item(s) / 27 not fetched):\n  - S2A_32UNC_20220901_0_L2A\n  - S2A_32UNC_20220829_0_L2A\n  - S2B_32UNC_20220827_0_L2A\n  - S2B_32UNC_20220824_0_L2A\n  - S2A_32UNC_20220822_0_L2A\n  - S2A_32UNC_20220819_1_L2A\n  - S2B_32UNC_20220817_0_L2A\n  - S2B_32UNC_20220814_0_L2A\n  - S2A_32UNC_20220812_0_L2A\n  - S2A_32UNC_20220809_0_L2A\n  - ... with 590 more feature(s).\n- assets: \nAOT, B01, B02, B03, B04, B05, B06, B07, B08, B09, B11, B12, B8A, info, metadata, overview, SCL, thumbnail, visual, WVP\n- item's fields: \nassets, bbox, collection, geometry, id, links, properties, properties.sentinel:boa_offset_applied, stac_extensions, stac_version, type\n\n\n\ns2_collection &lt;- gdalcubes::stac_image_collection(\n  items$features,\n  asset_names = c(\"B01\",\"B02\",\"B03\",\"B04\",\"B05\",\"B06\",\"B07\",\"B08\",\"B8A\",\"B09\",\"B11\",\"SCL\"),\n  property_filter = function(x) { x[[\"eo:cloud_cover\"]] &lt; 5 }\n)\ns2_collection\n\nImage collection object, referencing 78 images with 12 bands\nImages:\n                      name     left      top   bottom    right\n1 S2B_32UNC_20220824_0_L2A 9.494450 52.34699 51.35264 10.61137\n2 S2A_32UNC_20220812_0_L2A 8.999721 52.35046 51.35264 10.61137\n3 S2A_32UNC_20220623_0_L2A 8.999721 52.35046 51.35264 10.61137\n4 S2B_32UNC_20220618_0_L2A 8.999721 52.35046 51.35264 10.61137\n5 S2B_32UNC_20220615_0_L2A 9.471038 52.34716 51.35264 10.61137\n6 S2B_32UNC_20220519_1_L2A 8.999721 52.35046 51.35264 10.61137\n             datetime        srs\n1 2022-08-24T10:26:32 EPSG:32632\n2 2022-08-12T10:36:42 EPSG:32632\n3 2022-06-23T10:36:42 EPSG:32632\n4 2022-06-18T10:36:34 EPSG:32632\n5 2022-06-15T10:26:36 EPSG:32632\n6 2022-05-19T10:36:29 EPSG:32632\n[ omitted 72 images ] \n\nBands:\n   name offset scale unit nodata image_count\n1   B01      0     1                      78\n2   B02      0     1                      78\n3   B03      0     1                      78\n4   B04      0     1                      78\n5   B05      0     1                      78\n6   B06      0     1                      78\n7   B07      0     1                      78\n8   B08      0     1                      78\n9   B09      0     1                      78\n10  B11      0     1                      78\n11  B8A      0     1                      78\n12  SCL      0     1                      78\n\n\n\nbbox_utm &lt;- sf::st_as_sfc(bbox) |&gt; sf::st_transform(\"EPSG:32632\") |&gt; sf::st_bbox()\nv &lt;- gdalcubes::cube_view(\n  srs = \"EPSG:32632\",\n  extent = list(\n    t0 = \"2018-06\", t1 = \"2022-09\",\n    left = bbox_utm[\"xmin\"] - 10, right = bbox_utm[\"xmax\"] + 10,\n    bottom = bbox_utm[\"ymin\"] - 10, top  = bbox_utm[\"ymax\"] + 10\n  ),\n  dx = 10, dy = 10, dt = \"P1M\",\n  aggregation = \"median\", resampling = \"bilinear\"\n)\nv\n\nA data cube view object\n\nDimensions:\n               low             high count pixel_size\nt       2018-06-01       2022-09-30    52        P1M\ny  5744956.9172092  5755796.9172092  1084         10\nx 583230.473549458 596220.473549458  1299         10\n\nSRS: \"EPSG:32632\"\nTemporal aggregation method: \"median\"\nSpatial resampling method: \"bilinear\"\n\n\n\n# Only when you actually want to materialize the cube\ns2_mask &lt;- gdalcubes::image_mask(\"SCL\", values = c(3,8,9))\ngdalcubes::gdalcubes_options(parallel = 16, ncdf_compression_level = 5)\ngdalcubes::raster_cube(s2_collection, v, mask = s2_mask) |&gt;\n  gdalcubes::write_ncdf(file.path(root_folder,\"data/harz_2018_2022_all.nc\"), overwrite = TRUE)\n\n\n# Requires the netCDF produced above\ngdalcubes::ncdf_cube(file.path(root_folder,\"data/harz_2018_2022_all.nc\")) |&gt;\n  gdalcubes::apply_pixel(\"tanh(((B08-B04)/(B08+B04))^2)\", \"kNDVI\") |&gt;\n  gdalcubes::reduce_time(\"mean(kNDVI)\") |&gt;\n  plot(key.pos = 1, col = ndvi.col(11), nbreaks = 12)\n\n\n\n\nAlternative 2: Copernicus Data Space Ecosystem (CDSE)\n\nHinweis: In CI werden diese Chunks ausgeschaltet (eval=eval_cdse), damit keine Secrets nötig sind.\n\n\nlibrary(CDSE); library(purrr); library(tibble)\n\nsafe_get &lt;- function() {\n  tryCatch(CDSE::GetCollections(as_data_frame = FALSE), error = function(e) NULL)\n}\n\ncollections_raw &lt;- safe_get()\nif (is.null(collections_raw)) {\n  message(\"CDSE GetCollections fehlgeschlagen – überspringe Anzeige.\")\n} else {\n  keys &lt;- c(\"id\",\"title\",\"description\",\"license\",\"stac_version\",\"sci:doi\",\"keywords\",\"type\")\n  collections &lt;- map(collections_raw, function(x) {\n    vals &lt;- lapply(keys, function(k) if (!is.null(x[[k]])) x[[k]] else NA)\n    names(vals) &lt;- keys\n    as_tibble(vals)\n  }) |&gt; list_rbind()\n  collections\n}\n\n# A tibble: 8 × 8\n  id             title description license stac_version `sci:doi` keywords type \n  &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;        &lt;lgl&gt;     &lt;lgl&gt;    &lt;chr&gt;\n1 sentinel-2-l1c Sent… Sentinel 2… propri… 1.0.0        NA        NA       Coll…\n2 sentinel-3-ol… Sent… Sentinel 3… propri… 1.0.0        NA        NA       Coll…\n3 sentinel-3-ol… Sent… Sentinel 3… propri… 1.0.0        NA        NA       Coll…\n4 sentinel-3-sl… Sent… Sentinel 3… propri… 1.0.0        NA        NA       Coll…\n5 sentinel-3-sy… Sent… Sentinel 3… propri… 1.0.0        NA        NA       Coll…\n6 sentinel-1-grd Sent… Sentinel 1… propri… 1.0.0        NA        NA       Coll…\n7 sentinel-2-l2a Sent… Sentinel 2… propri… 1.0.0        NA        NA       Coll…\n8 sentinel-5p-l2 Sent… Sentinel 5… propri… 1.0.0        NA        NA       Coll…\n\n\n\nimages &lt;- CDSE::SearchCatalog(\n  bbox = sf::st_bbox(train_areas_2019_2020),\n  from = \"2018-05-01\", to = \"2022-12-31\",\n  collection = \"sentinel-2-l2a\", with_geometry = TRUE,\n  client = OAuthClient\n)\nimages\n\nSimple feature collection with 683 features and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 8.999721 ymin: 51.35264 xmax: 10.61137 ymax: 52.35046\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   acquisitionDate tileCloudCover areaCoverage   satellite\n1       2022-12-30         100.00          100 sentinel-2a\n2       2022-12-27          79.03          100 sentinel-2a\n3       2022-12-25          99.98          100 sentinel-2b\n4       2022-12-22          99.41          100 sentinel-2b\n5       2022-12-20          97.58          100 sentinel-2a\n6       2022-12-17          29.46          100 sentinel-2a\n7       2022-12-15           3.70          100 sentinel-2b\n8       2022-12-10          95.68          100 sentinel-2a\n9       2022-12-07          96.03          100 sentinel-2a\n10      2022-12-05          99.97          100 sentinel-2b\n   acquisitionTimestampUTC acquisitionTimestampLocal\n1      2022-12-30 10:36:29       2022-12-30 11:36:29\n2      2022-12-27 10:26:31       2022-12-27 11:26:31\n3      2022-12-25 10:36:29       2022-12-25 11:36:29\n4      2022-12-22 10:26:31       2022-12-22 11:26:31\n5      2022-12-20 10:36:28       2022-12-20 11:36:28\n6      2022-12-17 10:26:31       2022-12-17 11:26:31\n7      2022-12-15 10:36:28       2022-12-15 11:36:28\n8      2022-12-10 10:36:30       2022-12-10 11:36:30\n9      2022-12-07 10:26:33       2022-12-07 11:26:33\n10     2022-12-05 10:36:27       2022-12-05 11:36:27\n                                                            sourceId long.min\n1  S2A_MSIL2A_20221230T103431_N0510_R108_T32UNC_20240803T051250.SAFE 8.999721\n2  S2A_MSIL2A_20221227T102431_N0510_R065_T32UNC_20240805T222708.SAFE 9.476927\n3  S2B_MSIL2A_20221225T103349_N0510_R108_T32UNC_20240806T140039.SAFE 8.999721\n4  S2B_MSIL2A_20221222T102339_N0510_R065_T32UNC_20240804T111825.SAFE 9.485113\n5  S2A_MSIL2A_20221220T103441_N0510_R108_T32UNC_20240803T223745.SAFE 8.999721\n6  S2A_MSIL2A_20221217T102431_N0510_R065_T32UNC_20240805T123818.SAFE 9.478651\n7  S2B_MSIL2A_20221215T103339_N0510_R108_T32UNC_20240805T070920.SAFE 8.999721\n8  S2A_MSIL2A_20221210T103431_N0510_R108_T32UNC_20240803T150543.SAFE 8.999721\n9  S2A_MSIL2A_20221207T102411_N0510_R065_T32UNC_20240806T043548.SAFE 9.476926\n10 S2B_MSIL2A_20221205T103319_N0510_R108_T32UNC_20240802T232027.SAFE 8.999721\n    lat.min long.max  lat.max                       geometry\n1  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n2  51.35264 10.61137 52.34711 POLYGON ((9.889693 52.34711...\n3  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n4  51.35264 10.61137 52.34705 POLYGON ((9.898648 52.34705...\n5  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n6  51.35264 10.61137 52.34710 POLYGON ((9.891455 52.3471,...\n7  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n8  51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n9  51.35264 10.61137 52.34711 POLYGON ((9.890282 52.34711...\n10 51.35264 10.61137 52.35046 POLYGON ((8.999721 52.35046...\n\nsummary(images$areaCoverage)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.48  100.00  100.00   99.83  100.00  100.00 \n\nday &lt;- images[order(images$tileCloudCover), ]$acquisitionDate[1:30]\n\n\nscript_file_raw   &lt;- system.file(\"scripts\", \"RawBands.js\", package = \"CDSE\")\nscript_file_kndvi &lt;- \"kndvi.js\"\nscript_file_savi  &lt;- \"savi.js\"\nscript_file_evi   &lt;- \"evi.js\"\n\nraw_2018_07_01 &lt;- CDSE::GetImage(\n  bbox = sf::st_bbox(train_areas_2019_2020), time_range = day[12],\n  script = script_file_raw, collection = \"sentinel-2-l2a\",\n  format = \"image/tiff\", mosaicking_order = \"leastCC\",\n  resolution = 10, mask = TRUE, buffer = 0.01, client = OAuthClient\n)\nnames(raw_2018_07_01) &lt;- c(\"B01\",\"B02\",\"B03\",\"B04\",\"B05\",\"B06\",\"B07\",\"B08\",\"B8A\",\"B09\",\"B11\",\"B12\")\n\nkndvi_2018_07_01 &lt;- CDSE::GetImage(\n  bbox = sf::st_bbox(train_areas_2019_2020), time_range = day[12],\n  script = script_file_kndvi, collection = \"sentinel-2-l2a\",\n  format = \"image/tiff\", mosaicking_order = \"leastCC\",\n  resolution = 10, mask = TRUE, buffer = 0.01, client = OAuthClient\n)\nnames(kndvi_2018_07_01)[1] &lt;- \"kNDVI\"\n\nsavi_2018_07_01 &lt;- CDSE::GetImage(\n  bbox = sf::st_bbox(train_areas_2019_2020), time_range = day[12],\n  script = script_file_savi, collection = \"sentinel-2-l2a\",\n  format = \"image/tiff\", mosaicking_order = \"leastCC\",\n  resolution = 10, mask = TRUE, buffer = 0.01, client = OAuthClient\n)\nnames(savi_2018_07_01)[1] &lt;- \"SAVI\"\n\nevi_2018_07_01 &lt;- CDSE::GetImage(\n  bbox = sf::st_bbox(train_areas_2019_2020), time_range = day[12],\n  script = script_file_evi, collection = \"sentinel-2-l2a\",\n  format = \"image/tiff\", mosaicking_order = \"leastCC\",\n  resolution = 10, mask = TRUE, buffer = 0.01, client = OAuthClient\n)\nnames(evi_2018_07_01)[1] &lt;- \"EVI\"\n\npred_stack_2018 &lt;- c(raw_2018_07_01, evi_2018_07_01[[1]], kndvi_2018_07_01[[1]], savi_2018_07_01[[1]])\n\n\nraw_2022_06_23 &lt;- CDSE::GetImage(\n  bbox = sf::st_bbox(train_areas_2019_2020), time_range = day[1],\n  script = script_file_raw, collection = \"sentinel-2-l2a\",\n  format = \"image/tiff\", mosaicking_order = \"leastCC\",\n  resolution = 10, mask = TRUE, buffer = 0.01, client = OAuthClient\n)\nnames(raw_2022_06_23) &lt;- c(\"B01\",\"B02\",\"B03\",\"B04\",\"B05\",\"B06\",\"B07\",\"B08\",\"B8A\",\"B09\",\"B11\",\"B12\")\n\nkndvi_2022_06_23 &lt;- CDSE::GetImage(\n  bbox = sf::st_bbox(train_areas_2019_2020), time_range = day[1],\n  script = script_file_kndvi, collection = \"sentinel-2-l2a\",\n  format = \"image/tiff\", mosaicking_order = \"leastCC\",\n  resolution = 10, mask = TRUE, buffer = 0.01, client = OAuthClient\n)\nnames(kndvi_2022_06_23)[1] &lt;- \"kNDVI\"\n\nsavi_2022_06_23 &lt;- CDSE::GetImage(\n  bbox = sf::st_bbox(train_areas_2019_2020), time_range = day[1],\n  script = script_file_savi, collection = \"sentinel-2-l2a\",\n  format = \"image/tiff\", mosaicking_order = \"leastCC\",\n  resolution = 10, mask = TRUE, buffer = 0.01, client = OAuthClient\n)\nnames(savi_2022_06_23)[1] &lt;- \"SAVI\"\n\nevi_2022_06_23 &lt;- CDSE::GetImage(\n  bbox = sf::st_bbox(train_areas_2019_2020), time_range = day[1],\n  script = script_file_evi, collection = \"sentinel-2-l2a\",\n  format = \"image/tiff\", mosaicking_order = \"leastCC\",\n  resolution = 10, mask = TRUE, buffer = 0.01, client = OAuthClient\n)\nnames(evi_2022_06_23)[1] &lt;- \"EVI\"\n\npred_stack_2022 &lt;- c(raw_2022_06_23, evi_2022_06_23[[1]], kndvi_2022_06_23[[1]], savi_2022_06_23[[1]])\n\nterra::writeRaster(pred_stack_2018, file.path(root_folder,\"data/pred_stack_2018.tif\"), overwrite=TRUE)\nterra::writeRaster(pred_stack_2022, file.path(root_folder,\"data/pred_stack_2022.tif\"), overwrite=TRUE)",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial data classification"
    ]
  },
  {
    "objectID": "reader/cd-1.html#step2-overview-unsupervised-classification",
    "href": "reader/cd-1.html#step2-overview-unsupervised-classification",
    "title": "Change Detection - Sentinel",
    "section": "Step2: Overview – Unsupervised classification",
    "text": "Step2: Overview – Unsupervised classification\n\nk-means clustering\n\n# Requires pred_stack_2018/2022 (CDSE) or create equivalent stacks otherwise\nif (!exists(\"pred_stack_2018\")) pred_stack_2018 &lt;- terra::rast(file.path(root_folder,\"data/pred_stack_2018.tif\"))\nif (!exists(\"pred_stack_2022\")) pred_stack_2022 &lt;- terra::rast(file.path(root_folder,\"data/pred_stack_2022.tif\"))\n\nprediction_kmeans_2018 &lt;- RStoolbox::unsuperClass(\n  pred_stack_2018, nClasses = 5, norm = TRUE, algorithm = \"MacQueen\"\n)\nplot(prediction_kmeans_2018$map, main = \"k-means 2018\")\n\n\n\n\n\n\n\nprediction_kmeans_2022 &lt;- RStoolbox::unsuperClass(\n  pred_stack_2022, nClasses = 5, norm = TRUE, algorithm = \"MacQueen\"\n)\nplot(prediction_kmeans_2022$map, main = \"k-means 2022\")\n\n\n\n\n\n\n\n\n\n\nbfast: Spatial identification of magnitudes and time periods of kNDVI changes\n\ngdalcubes::gdalcubes_options(parallel = 16)\n\ngdalcubes::ncdf_cube(file.path(root_folder,\"data/harz_2018_2022_all.nc\")) |&gt;\n  gdalcubes::reduce_time(names = c(\"change_date\",\"change_magnitude\"), FUN = function(x) {\n    knr   &lt;- exp(-((x[\"B08\",]/10000)-(x[\"B04\",]/10000))^2/(2))\n    kndvi &lt;- (1 - knr) / (1 + knr)\n    if (all(is.na(kndvi))) return(c(NA,NA))\n    kndvi_ts &lt;- ts(kndvi, start = c(2018,1), frequency = 12)\n    library(bfast)\n    tryCatch({\n      res &lt;- bfast::bfastmonitor(kndvi_ts, start = c(2019,1), level = 0.01)\n      c(res$breakpoint, res$magnitude)\n    }, error = function(e) c(NA,NA))\n  }) |&gt;\n  gdalcubes::write_ncdf(file.path(root_folder,\"data/bfast_results.nc\"), overwrite = TRUE)\n\n\ngdalcubes::ncdf_cube(file.path(root_folder,\"data/bfast_results.nc\")) |&gt;\n  plot(key.pos = 1, col = ndvi.col(11), nbreaks = 12)",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial data classification"
    ]
  },
  {
    "objectID": "reader/cd-1.html#step-3---generating-training-data",
    "href": "reader/cd-1.html#step-3---generating-training-data",
    "title": "Change Detection - Sentinel",
    "section": "Step 3 - Generating training data",
    "text": "Step 3 - Generating training data\n\npred_stack_2018 &lt;- terra::rast(file.path(root_folder,\"data/pred_stack_2018.tif\"))\npred_stack_2022 &lt;- terra::rast(file.path(root_folder,\"data/pred_stack_2022.tif\"))\n\ntDF_2019 &lt;- exactextractr::exact_extract(\n  pred_stack_2018, dplyr::filter(train_areas_2019_2020, year == 2019),\n  force_df = TRUE, include_cell = TRUE, include_xy = TRUE, full_colnames = TRUE, include_cols = \"class\"\n)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================| 100%\n\ntDF_2020 &lt;- exactextractr::exact_extract(\n  pred_stack_2022, dplyr::filter(train_areas_2019_2020, year == 2020),\n  force_df = TRUE, include_cell = TRUE, include_xy = TRUE, full_colnames = TRUE, include_cols = \"class\"\n)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n\ntDF_2019 &lt;- dplyr::bind_rows(tDF_2019); tDF_2019$year &lt;- 2019\ntDF_2020 &lt;- dplyr::bind_rows(tDF_2020); tDF_2020$year &lt;- 2020\n\ntDF_2019 &lt;- tDF_2019[complete.cases(tDF_2019), ]\ntDF_2020 &lt;- tDF_2020[complete.cases(tDF_2020), ]\n\ntDF &lt;- rbind(tDF_2019, tDF_2020)\nsaveRDS(tDF, file.path(root_folder,\"data/tDF_2018_2022.rds\"))",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial data classification"
    ]
  },
  {
    "objectID": "reader/cd-1.html#step-4---supervised-classification",
    "href": "reader/cd-1.html#step-4---supervised-classification",
    "title": "Change Detection - Sentinel",
    "section": "Step 4 - supervised classification",
    "text": "Step 4 - supervised classification\n\nMaximum Likelihood Classification\n\nset.seed(123)\ntDF &lt;- readRDS(file.path(root_folder,\"data/tDF_2018_2022.rds\"))\n\nidx      &lt;- caret::createDataPartition(tDF$class, list = FALSE, p = 0.05)\ntrainDat &lt;- tDF[idx,]\ntestDat  &lt;- tDF[-idx,]\n\ntrainDat$class &lt;- as.factor(trainDat$class)\ntestDat$class  &lt;- as.factor(testDat$class)\n\nsp_trainDat &lt;- trainDat\nsp_testDat  &lt;- testDat\nsp::coordinates(sp_trainDat) &lt;- ~x+y\nsp::coordinates(sp_testDat)  &lt;- ~x+y\nsp::proj4string(sp_trainDat) &lt;- terra::crs(pred_stack_2018)\nsp::proj4string(sp_testDat)  &lt;- terra::crs(pred_stack_2018)\n\nprediction_mlc_2018 &lt;- RStoolbox::superClass(\n  pred_stack_2018, trainData = sp_trainDat[,1:16], valData = sp_testDat[,1:16],\n  responseCol = \"class\", model = \"mlc\", tuneLength = 1,\n  trainPartition = 0.3, verbose = TRUE,\n  filename = file.path(root_folder,\"data/prediction_mlc_2018.tif\")\n)\n\nprediction_mlc_2022 &lt;- RStoolbox::superClass(\n  pred_stack_2022, trainData = sp_trainDat[,1:16], valData = sp_testDat[,1:16],\n  responseCol = \"class\", model = \"mlc\", tuneLength = 1,\n  trainPartition = 0.3, verbose = TRUE,\n  filename = file.path(root_folder,\"data/prediction_mlc_2022.tif\")\n)\n\nsaveRDS(prediction_mlc_2018, file.path(root_folder,\"data/prediction_mlc_2018.rds\"))\nsaveRDS(prediction_mlc_2022, file.path(root_folder,\"data/prediction_mlc_2022.rds\"))\n\n\n\nRandom forest\n\nctrlh &lt;- caret::trainControl(method = \"cv\", number = 10, savePredictions = TRUE)\n\nrf_model &lt;- caret::train(\n  x = trainDat[,2:16], y = trainDat[,1],\n  method = \"rf\", metric = \"Kappa\",\n  trControl = ctrlh, importance = TRUE\n)\nrf_model\nsaveRDS(rf_model, file.path(root_folder,\"data/rf_model.rds\"))\n\n\n\nPrediction on the original data\n\nrf_model &lt;- readRDS(file.path(root_folder,\"data/rf_model.rds\"))\n\nprediction_rf_2018 &lt;- terra::predict(pred_stack_2018, rf_model)\nprediction_rf_2022 &lt;- terra::predict(pred_stack_2022, rf_model)\n\nsaveRDS(prediction_rf_2018, file.path(root_folder,\"data/prediction_rf_2018.rds\"))\nsaveRDS(prediction_rf_2022, file.path(root_folder,\"data/prediction_rf_2022.rds\"))\n\n\nprediction_rf_2018  &lt;- readRDS(file.path(root_folder,\"data/prediction_rf_2018.rds\"))\nprediction_rf_2022  &lt;- readRDS(file.path(root_folder,\"data/prediction_rf_2022.rds\"))\nprediction_mlc_2018 &lt;- terra::rast(file.path(root_folder,\"data/prediction_mlc_2018.tif\"))\nprediction_mlc_2022 &lt;- terra::rast(file.path(root_folder,\"data/prediction_mlc_2022.tif\"))\n\nmask &lt;- terra::resample(harz_forest_mask, pred_stack_2022)\n\nplot(mask*prediction_rf_2022 - mask*prediction_rf_2018, main=\"RF Δ 2022–2018\") \nplot(mask*prediction_mlc_2022 - mask*prediction_mlc_2018, main=\"MLC Δ 2022–2018\")\n\n\n\nStep 5: Estimation model quality\n\ncm_rf &lt;- caret::confusionMatrix(\n  data = predict(rf_model, newdata = testDat),\n  reference = testDat$class\n)\ncm_rf",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial data classification"
    ]
  },
  {
    "objectID": "reader/cd-1.html#final-remarks",
    "href": "reader/cd-1.html#final-remarks",
    "title": "Change Detection - Sentinel",
    "section": "Final Remarks",
    "text": "Final Remarks\nYou may have noticed that we have mixed two different approaches …",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Spatial data classification"
    ]
  },
  {
    "objectID": "reader/mc_2025_tec.html",
    "href": "reader/mc_2025_tec.html",
    "title": "Microclimate Sensors & Power-Supply Units",
    "section": "",
    "text": "Froggit shop [DE]\n  \n  \n    \n     Ecowitt shop [US]\n  \n  \n    \n     Fine Offset\n  \n\n      \nThe sensors from Fine Offset are re-branded and partly modified by the resellers. This article deals with sensors from the german re-seller froggit and the US re-seller ecowitt. More precise the DP-/GW SmartHubs WiFi Gateway with temperature, humidity & Pressure which is developed by fine offset. The unique selling point of the LoRa-Wifi gateway is the extraordinarily extensive possibility of connecting radio-bound sensors.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Microclimate Sensors & Power-Supply Units"
    ]
  },
  {
    "objectID": "reader/mc_2025_tec.html#calibration-concept",
    "href": "reader/mc_2025_tec.html#calibration-concept",
    "title": "Microclimate Sensors & Power-Supply Units",
    "section": "Calibration Concept",
    "text": "Calibration Concept\nThe low budget sensors are usually lacking of a stable measurement quality. To obtain reliable micro climate data a two step calibration process is suggested.\n\nThe measurements of all sensors (preferably in a climate chamber) will be statistically analysed to identify sensor which produce systematic and significant outliers.\nThe sensors are calibrated against an operational running high price reference station in the field.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Microclimate Sensors & Power-Supply Units"
    ]
  },
  {
    "objectID": "reader/agisrs-1.html",
    "href": "reader/agisrs-1.html",
    "title": "aGIS-RS Ressources",
    "section": "",
    "text": "del Río, M., Pretzsch, H., Alberdi, I. et al. Characterization of the structure, dynamics, and productivity of mixed-species stands: review and perspectives. Eur J Forest Res 135, 23–49 (2016). https://doi.org/10.1007/s10342-015-0927-6. (VPN necessary)\nEstimating Forest Structure Indices for Evalution of Forest Bird Habitats by an Airborne Laser-Scanner H. Hashimoto, J. Imanishi, A. Hagiwara, Y. Morimoto, K. Kitada Conference Paper\nBarnes, C.; Balzter, H.; Barrett, K.; Eddy, J.; Milner, S.; Suárez, J.C. Individual Tree Crown Delineation from Airborne Laser Scanning for Diseased Larch Forest Stands. Remote Sens. 2017, 9, 231. https://doi.org/10.3390/rs9030231\nJakubowski, M.K.; Li, W.; Guo, Q.; Kelly, M. Delineating Individual Trees from Lidar Data: A Comparison of Vector- and Raster-based Segmentation Approaches. Remote Sens. 2013, 5, 4163-4186. https://doi.org/10.3390/rs5094163\nSpecial Issue Lidar Remote Sensing of Forest Structure, Biomass and Dynamics\n\n\n\n\n\nR — the interpreter can be installed on any operation system.\nRStudio — we recommend to use R Studio for (interactive) programming with R.\nGit environment for your operating system.\nFor Windows users with little experience on the command line we recommend GitHub Desktop.\nRQGIS installation guide.\n\n\n\n\n\nJean-Romain Roussel, David Auty, Nicholas C. Coops, Piotr Tompalski, Tristan R.H. Goodbody, Andrew Sánchez Meador, Jean-François Bourdon, Florian de Boissieu, Alexis Achim, lidR: An R package for analysis of Airborne Laser Scanning (ALS) data, Remote Sensing of Environment, Volume 251, 2020, 112061, ISSN 0034-4257, https://doi.org/10.1016/j.rse.2020.112061.\nThe lidR package book Perfect manual for most lidr related stuff\n\n\n\n\n\nr-bloggers best practices introduction\nUSGS best practices introduction\nworkflowR\nusethis\nMichael Dorman Introduction to Spatial Data Programming with R gives a lot of support for all kind of spatio temporal programming issues\nRobert Hijmans Spatial Data Science with R website provides a valuable insight in the raster and terra package with a lot of tutorials.\nColin Gillespie Efficient R programming If you want to know it really have allook at this book.\n\n\n\n\n\nGeocomputation with R HIGHLY Recommended From the authors trio Lovelace, Nowosad & Muenchow\n\n\n\n\n\nThe core of GIScience Download The editors Tolpekin & Stein 2012 are providing an excellent insight into GI concepts."
  },
  {
    "objectID": "reader/agisrs-1.html#data-readings-and-more",
    "href": "reader/agisrs-1.html#data-readings-and-more",
    "title": "aGIS-RS Ressources",
    "section": "",
    "text": "del Río, M., Pretzsch, H., Alberdi, I. et al. Characterization of the structure, dynamics, and productivity of mixed-species stands: review and perspectives. Eur J Forest Res 135, 23–49 (2016). https://doi.org/10.1007/s10342-015-0927-6. (VPN necessary)\nEstimating Forest Structure Indices for Evalution of Forest Bird Habitats by an Airborne Laser-Scanner H. Hashimoto, J. Imanishi, A. Hagiwara, Y. Morimoto, K. Kitada Conference Paper\nBarnes, C.; Balzter, H.; Barrett, K.; Eddy, J.; Milner, S.; Suárez, J.C. Individual Tree Crown Delineation from Airborne Laser Scanning for Diseased Larch Forest Stands. Remote Sens. 2017, 9, 231. https://doi.org/10.3390/rs9030231\nJakubowski, M.K.; Li, W.; Guo, Q.; Kelly, M. Delineating Individual Trees from Lidar Data: A Comparison of Vector- and Raster-based Segmentation Approaches. Remote Sens. 2013, 5, 4163-4186. https://doi.org/10.3390/rs5094163\nSpecial Issue Lidar Remote Sensing of Forest Structure, Biomass and Dynamics\n\n\n\n\n\nR — the interpreter can be installed on any operation system.\nRStudio — we recommend to use R Studio for (interactive) programming with R.\nGit environment for your operating system.\nFor Windows users with little experience on the command line we recommend GitHub Desktop.\nRQGIS installation guide.\n\n\n\n\n\nJean-Romain Roussel, David Auty, Nicholas C. Coops, Piotr Tompalski, Tristan R.H. Goodbody, Andrew Sánchez Meador, Jean-François Bourdon, Florian de Boissieu, Alexis Achim, lidR: An R package for analysis of Airborne Laser Scanning (ALS) data, Remote Sensing of Environment, Volume 251, 2020, 112061, ISSN 0034-4257, https://doi.org/10.1016/j.rse.2020.112061.\nThe lidR package book Perfect manual for most lidr related stuff\n\n\n\n\n\nr-bloggers best practices introduction\nUSGS best practices introduction\nworkflowR\nusethis\nMichael Dorman Introduction to Spatial Data Programming with R gives a lot of support for all kind of spatio temporal programming issues\nRobert Hijmans Spatial Data Science with R website provides a valuable insight in the raster and terra package with a lot of tutorials.\nColin Gillespie Efficient R programming If you want to know it really have allook at this book.\n\n\n\n\n\nGeocomputation with R HIGHLY Recommended From the authors trio Lovelace, Nowosad & Muenchow\n\n\n\n\n\nThe core of GIScience Download The editors Tolpekin & Stein 2012 are providing an excellent insight into GI concepts."
  },
  {
    "objectID": "reader/agisrs-1.html#data",
    "href": "reader/agisrs-1.html#data",
    "title": "aGIS-RS Ressources",
    "section": "Data",
    "text": "Data\n\nMarburg open Forest database\n\nThe MOF public data of Marburg Open Forest is a comprehensive data base for validation and testing purposes\nThe data server for MOF data. Please ignore insecure site warning."
  },
  {
    "objectID": "reader/mc3.html",
    "href": "reader/mc3.html",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "Let’s take a closer look at proximity, which is mentioned frequently. What exactly is it? How can proximity/neighborliness be expressed in such a way that the space becomes meaningful?\nIn general, spatial relationships are described in terms of neighborhoods (positional) and distances (metric). In spatial analysis or prediction, however, it is important to be able to name the spatial influence, i.e. the evaluation or weighting of this relationship, either qualitatively or quantitatively. Tobler did this for a specific objective by stating that “near” is more important than “far”. But what about in other cases? The challenge is that spatial influence can only be measured directly in exceptional cases. There are many ways to estimate it, however.\n\n\nNeighborhood is perhaps the most important concept. Higher dimensional geo-objects can be considered neighboring if they touch each other, e.g. neighboring countries. For zero-dimensional objects (points), the most common approach is to use distance in combination with a number of points to determine neighborhood.\n\n\n\nProximity or neighborhood analyses are often concerned with areas of influence or catchment areas, i.e. spatial patterns of effects or processes.\nThis section discusses some methods for calculating distances between spatial objects. Because of the different ways of discretizing space, we must make the – already familiar – distinction between vector and raster data models.\nInitially, it is often useful to work without spatially restrictive conditions in a first analysis, e.g. when this information is missing. The term “proximity” inherently implies a certain imprecision. Qualitative terms that can be used for this are: “near”, “far” or “in the neighborhood of”. Representation and data-driven analysis require these terms to be objectified and operationalized. So, this metric must be based on a distance concept, e.g. Euclidean distance or travel times. In a second interpretative step, we must decide which units define this type of proximity. In terms of the objective of a question, there are only suitable and less-suitable measures; there is no correct or incorrect. Therefore, it is critical to define a meaningful neighborhood relationship for the objects under investigation."
  },
  {
    "objectID": "reader/mc3.html#distance-and-data-representation",
    "href": "reader/mc3.html#distance-and-data-representation",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "Let’s take a closer look at proximity, which is mentioned frequently. What exactly is it? How can proximity/neighborliness be expressed in such a way that the space becomes meaningful?\nIn general, spatial relationships are described in terms of neighborhoods (positional) and distances (metric). In spatial analysis or prediction, however, it is important to be able to name the spatial influence, i.e. the evaluation or weighting of this relationship, either qualitatively or quantitatively. Tobler did this for a specific objective by stating that “near” is more important than “far”. But what about in other cases? The challenge is that spatial influence can only be measured directly in exceptional cases. There are many ways to estimate it, however.\n\n\nNeighborhood is perhaps the most important concept. Higher dimensional geo-objects can be considered neighboring if they touch each other, e.g. neighboring countries. For zero-dimensional objects (points), the most common approach is to use distance in combination with a number of points to determine neighborhood.\n\n\n\nProximity or neighborhood analyses are often concerned with areas of influence or catchment areas, i.e. spatial patterns of effects or processes.\nThis section discusses some methods for calculating distances between spatial objects. Because of the different ways of discretizing space, we must make the – already familiar – distinction between vector and raster data models.\nInitially, it is often useful to work without spatially restrictive conditions in a first analysis, e.g. when this information is missing. The term “proximity” inherently implies a certain imprecision. Qualitative terms that can be used for this are: “near”, “far” or “in the neighborhood of”. Representation and data-driven analysis require these terms to be objectified and operationalized. So, this metric must be based on a distance concept, e.g. Euclidean distance or travel times. In a second interpretative step, we must decide which units define this type of proximity. In terms of the objective of a question, there are only suitable and less-suitable measures; there is no correct or incorrect. Therefore, it is critical to define a meaningful neighborhood relationship for the objects under investigation."
  },
  {
    "objectID": "reader/mc3.html#filling-spatial-gaps",
    "href": "reader/mc3.html#filling-spatial-gaps",
    "title": "Spatial Interpolation",
    "section": "Filling spatial gaps",
    "text": "Filling spatial gaps\nNow that we have learned the basic concepts of distance, neighborhood and filling spatial gaps, let’s take a look at interpolating or predicting values in space.\nFor many decades, deterministic interpolation techniques (inverse distance weighting, nearest neighbor, kriging) have been the most popular spatial interpolation techniques. External drift kriging and regression kriging, in particular, are fundamental techniques that use spatial autocorrelation and covariate information, i.e. sophisticated regression statistics.\nMachine learning algorithms like random forest have become very popular for spatial environmental prediction. One major reason for this is that they are can take into account non-linear and complex relationships, i.e. compensate for certain disadvantages that are present in the usual regression methods."
  },
  {
    "objectID": "reader/mc3.html#proximity-concepts",
    "href": "reader/mc3.html#proximity-concepts",
    "title": "Spatial Interpolation",
    "section": "Proximity concepts",
    "text": "Proximity concepts\n\nVoronoi polygons – dividing space geometrically\nVoronoi polygons (aka Thiessen polygons) are an elementary method for geometrically determining proximity or neighborhoods. Voronoi polygons (see figure below) divide an area into regions that are closest to a given point in a set of irregularly distributed points. In two dimensions, a Voronoi polygon encompasses an area around a point, such that every spatial point within the Voronoi polygon is closer to this point than to any other point in the set. Such constructs can also be formed in higher dimensions, giving rise to Voronoi polyhedra.\n&lt;/frame&gt;\n&lt;iframe width=\"780\" height=\"1280\" src=\"https://geomoer.github.io/geoAI//assets/images/unit01/suisse6.html\" title=\"Interpol\"&gt;\n\n\nThe blue dots are a typical example of irregularly distributed points in space – in this case, rain gauges in Switzerland. The overlaid polygons are the corresponding Voronoi segments that define the corresponding closest geometrical areas (gisma 2021)“\n\n\nSince Voronoi polygons correspond to an organizational principle frequently observed in both nature (e.g. plant cells) and in the spatial sciences (e.g. central places , according to Christaller), there are manifold possible applications. Two things must be assumed, however: First, that nothing else is known about the space between the sampled locations and, second, that the boundary line between two samples is incomplete idea.\nVoronoi polygons can also be used to delineate catchment areas of shops, service facilities or wells, like in the example of the Soho cholera outbreak. Please note that within a polygon, one of the spatial features is isomorphic, i.e. the spatial features are identical.\nBut what if we know more about the spatial relationships of the features? Let’s have a look at some crucial concepts.\n\n\nSpatial interpolation of data\nSpatially interpolating data points provides us with a modeled quasi-continuous estimation of features under the corresponding assumptions. But what is spatial interpolation? Essentially, this means using known values to calculate neighboring values that are unknown. Most of these techniques are among the most complex methods of spatial analysis, so we will deliberately limit ourselves here to a basic overview of the methods. Some of the best-known and common interpolation methods found in spatial sciences are nearest neighbor inverse distance, spline interpolations, kriging, and regression methods.\n\n\nContinously filling the gaps by interpolation\nTo get started, take a look at the following figure, which shows six different interpolation methods to derive the spatial distribution of precipitation in Switzerland (in addition to the overlaid Voronoi tessellation).\n\n\n\nThe blue dots are a typical example of irregularly distributed points in space – in this case, rain gauges in Switzerland. The size of each dot corresponds to the amount of precipitation in mm. The overlaid polygons are the corresponding Voronoi segments that define the corresponding closest geometrical areas (gisma 2021)” top left: Nearest neighbor interpolation based on 3-5 nearest neighbors, top right: Inverse Distance weighting (IDW) interpolation method middle left: AutoKriging with no additional parameters, middle right: Thin plate spline regression interpolation method bottom left: Triangular irregular net (TIN) surface interpolation, bottom right: additive model (GAM) interpolation\n\n\nIn the example of precipitation in Switzerland, the positions of the weather stations are fixed and cannot be freely chosen.\nWhen choosing an appropriate interpolation method, we need to pay attention to several properties of the samples (distribution and properties of the measurement points):\n\nRepresentativeness of measurement points: The sample should represent the phenomenon being analyzed in all of its manifestations.\nHomogeneity of measurement points: The spatial interdependence of the data is a very important basic requirement for further meaningful analysis.\nSpatial distribution of measurement points: The spatial distribution is of great importance. It can be completely random, regular or clustered.\nNumber of measurement points: The number of measurement points depends on the phenomenon and the area. In most cases, the choice of sample size is subject to practical limitations.\n\nWhat makes things even more complex is that these four factors – representativeness, homogeneity, spatial distribution and size – are all interrelated. For example, a sample size of 5 measuring stations for estimating precipitation for all of Switzerland is hardly meaningful and therefore not representative. Equally unrepresentative would be selecting every measuring station in German-speaking Switzerland to estimate precipitation for the entire country. In this case, the number alone might be sufficient, but the spatial distribution would not be. If we select every station at an altitude below 750 m asl, the sample could be correct in terms of both size and spatial distribution, but the phenomenon is not homogeneously represented in the sample. An estimate based on this sample would be clearly distorted, especially in areas above 750 m asl. In practice, virtually every natural spatially-continuous phenomenon is governed by stochastic fluctuations, so, mathematically speaking, it can only be described in approximate terms.\n\n\nMachine learning\nMachine learning (ML) methods such as random forest can also produce spatial and temporal predictions (i.e. produce maps from point observations). These methods are particularly robust because they take spatial autocorrelation into account, which can improve predictions or interpolations by adding geographic distances. This ultimately leads to better maps with much more complex relationships and dependencies.\nIn the simplest case, the results are comparable to the well-known model-based geostatistics. The advantage of ML methods over model-based geostatistics, however, is that they make fewer assumptions, can take non-linearities into account and are easier to automate.\n\n\n\nThe original dataset (top left) is a terrain model reduced to 8 meters with 48384 single pixels. For interpolation, 1448 points were randomly drawn and interpolated with conventional kriging (top right), support vector machines (SVM) (middle left), neural networks (middle right), and two variants of random forest (bottom row). In each method, only the distance of the drawn points is used as a dependency.\n\n\nEach interpolation method was applied using the “default” settings. Tuning could possibly lead to significant changes in all of them. Fascinatingly, the error measures correlate to the visual results: Kriging and the neural network show the best performance, followed by the random forest models and the support-vector machine.\n\n\n\nmodel\ntotal_error\nmean_error\nsd_error\n\n\n\n\nKriging\n15797773.0\n54.2\n67.9\n\n\nNeural Network\n19772241.0\n67.8\n80.5\n\n\nRandom Forest\n20540628.1\n70.4\n82.5\n\n\nNormalized Random Forest\n20597969.8\n70.6\n82.7\n\n\nSupport Vector Machine\n21152987.7\n72.5\n68.3\n\n\n\n\n\nAdditional references\nGet the Most Out of AI, Machine Learning, and Deep Learning Part 1 (10:52) and Part 2 (13:18)\nWhy You Should NOT Learn Machine Learning! (6:17)\nGeoAI: Machine Learning meets ArcGIS (8:50)"
  },
  {
    "objectID": "reader/mc3.html#hands-on",
    "href": "reader/mc3.html#hands-on",
    "title": "Spatial Interpolation",
    "section": "Hands on",
    "text": "Hands on\nDownload the Harz exercises repository as a ZIP file from and unpack it on your computer. Then, you can open the .Rproj file and start working on the climate data interpolation exercise in RStudio."
  },
  {
    "objectID": "reader/mc3.html#further-more-complex-examples",
    "href": "reader/mc3.html#further-more-complex-examples",
    "title": "Spatial Interpolation",
    "section": "Further more complex examples",
    "text": "Further more complex examples\n\nSoil Humidity from the Forgenius Pinus Pinaster Project\nThe Forgenius Pinus Pinaster Project provides an fully integrated GIS source code and field data dealing with prediction classificaten of UAV and station realted data."
  },
  {
    "objectID": "reader/mc_2025_data.html",
    "href": "reader/mc_2025_data.html",
    "title": "Data Sources",
    "section": "",
    "text": "Download the exercises repository (as a ZIP), then unpack it locally. Open the .Rproj in RStudio and start with the provided exercises.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Filling the Gaps"
    ]
  },
  {
    "objectID": "reader/mc_2025_data.html#tutorial-data",
    "href": "reader/mc_2025_data.html#tutorial-data",
    "title": "Data Sources",
    "section": "",
    "text": "Download the exercises repository (as a ZIP), then unpack it locally. Open the .Rproj in RStudio and start with the provided exercises.",
    "crumbs": [
      "FAQ",
      "Lectures",
      "Filling the Gaps"
    ]
  },
  {
    "objectID": "worksheets/ws-01.html",
    "href": "worksheets/ws-01.html",
    "title": "Prerequisies",
    "section": "",
    "text": "Participants are expected to have a working recent version of R and RStudio installed, along with several R packages listed below.\n\nR: https://cloud.r-project.org/\nRStudio: https://posit.co/download/rstudio-desktop/#download\n\ninstall.packages(\"remotes\")\npkg_list = c(\"terra\", \"sf\", \"landscapemetrics\", \"motif\", \"tidyr\", \"dplyr\")\nremotes::install_cran(pkg_list)"
  },
  {
    "objectID": "worksheets/ws-01.html#prerequisites",
    "href": "worksheets/ws-01.html#prerequisites",
    "title": "Prerequisies",
    "section": "",
    "text": "Participants are expected to have a working recent version of R and RStudio installed, along with several R packages listed below.\n\nR: https://cloud.r-project.org/\nRStudio: https://posit.co/download/rstudio-desktop/#download\n\ninstall.packages(\"remotes\")\npkg_list = c(\"terra\", \"sf\", \"landscapemetrics\", \"motif\", \"tidyr\", \"dplyr\")\nremotes::install_cran(pkg_list)"
  },
  {
    "objectID": "worksheets/ws-01.html#exercises",
    "href": "worksheets/ws-01.html#exercises",
    "title": "Prerequisies",
    "section": "Exercises",
    "text": "Exercises\nThe slides are accompanied by practical exercises. The best way to get them is to download the exercises repository as a ZIP file from https://github.com/gisma-courses/LV-19-d19-006-24/blob/main/assets/exercises-session1.zip and unpack it on your computer. Then, you can open the .Rproj file and start working on the exercises in RStudio."
  },
  {
    "objectID": "worksheets/ws-06.html",
    "href": "worksheets/ws-06.html",
    "title": "Worksheet 5: Regionalisation",
    "section": "",
    "text": "Regionalize Germany’s climates.\n\nUse Worldclim versus CMIP data\nUse the upper helper functions and code for Great Britain\n\n\n\n\n\n\n\n\n\n\nMake sure you get an idea of a “scientific” climate zoning concept.\nThe following links provide an initial random selection of some thoughts on this concept.\n\nClimate Extremes are Becoming More Frequent, Co-occurring, and Persistent in Europe\nA systematic review of GIS-based local climate zone mapping studies\nEuropean summer weather regimes 1990-2019: Automatic classification and representation in a small - global climate model ensemble\n\nWhat are the main aims of the following articles in relation to climate zones?\nHow do you assess the proposed solution using supercells etc. in comparison?\nUse the LCZ-generator and compare it to your results."
  },
  {
    "objectID": "worksheets/ws-06.html#exercises-ws-5",
    "href": "worksheets/ws-06.html#exercises-ws-5",
    "title": "Worksheet 5: Regionalisation",
    "section": "",
    "text": "Regionalize Germany’s climates.\n\nUse Worldclim versus CMIP data\nUse the upper helper functions and code for Great Britain\n\n\n\n\n\n\n\n\n\n\nMake sure you get an idea of a “scientific” climate zoning concept.\nThe following links provide an initial random selection of some thoughts on this concept.\n\nClimate Extremes are Becoming More Frequent, Co-occurring, and Persistent in Europe\nA systematic review of GIS-based local climate zone mapping studies\nEuropean summer weather regimes 1990-2019: Automatic classification and representation in a small - global climate model ensemble\n\nWhat are the main aims of the following articles in relation to climate zones?\nHow do you assess the proposed solution using supercells etc. in comparison?\nUse the LCZ-generator and compare it to your results."
  },
  {
    "objectID": "worksheets/ws-02.html",
    "href": "worksheets/ws-02.html",
    "title": "Worksheet 1: Warm Up Exercises",
    "section": "",
    "text": "Read the data from exdata/lc_small.tif and visualize it. What is the location of the data? What are the extent of the data and its spatial resolution? How many categories it contains?\nCalculate Aggregation Index (AI) for the raster. Interpret the results.\nCalculate Total Edge (TE) for the raster. Interpret the results. Next, read the data from `exdata/lc_small2.tif, calculate AI and TE for this raster, and compare the results with the previous raster.\nCalculate Total Edge (TE) for the raster, but this time in a moving window of 9 by 9 cells. Visualize the results.\n(Extra) Using the read_sf() function from the sf package, read the exdata/points.gpkg file. Next, calculate SHDI and AI of an area of 3000 meters from each sampling point (see the sample_lsm() function)."
  },
  {
    "objectID": "worksheets/ws-02.html#exercises-ws-1",
    "href": "worksheets/ws-02.html#exercises-ws-1",
    "title": "Worksheet 1: Warm Up Exercises",
    "section": "",
    "text": "Read the data from exdata/lc_small.tif and visualize it. What is the location of the data? What are the extent of the data and its spatial resolution? How many categories it contains?\nCalculate Aggregation Index (AI) for the raster. Interpret the results.\nCalculate Total Edge (TE) for the raster. Interpret the results. Next, read the data from `exdata/lc_small2.tif, calculate AI and TE for this raster, and compare the results with the previous raster.\nCalculate Total Edge (TE) for the raster, but this time in a moving window of 9 by 9 cells. Visualize the results.\n(Extra) Using the read_sf() function from the sf package, read the exdata/points.gpkg file. Next, calculate SHDI and AI of an area of 3000 meters from each sampling point (see the sample_lsm() function)."
  },
  {
    "objectID": "slides/slide0.html#scientific-process-overview",
    "href": "slides/slide0.html#scientific-process-overview",
    "title": "From Foundations to Results: A Scientific Workflow",
    "section": "Scientific Process – Overview",
    "text": "Scientific Process – Overview\n\n\n\n&lt;strong&gt;Foundations / Background&lt;/strong&gt;&lt;br&gt;\n&lt;span class=\"small\"&gt;domain knowledge, theory, context&lt;/span&gt;\n\n\n&lt;strong&gt;Research Question&lt;/strong&gt;&lt;br&gt;\n&lt;span class=\"small\"&gt;precise, testable, scoped&lt;/span&gt;\n\n\n&lt;strong&gt;State of the Art (Literature)&lt;/strong&gt;&lt;br&gt;\n&lt;span class=\"small\"&gt;what is known, gaps, competing hypotheses&lt;/span&gt;\n\n\n&lt;strong&gt;Necessary Methods & Workflow&lt;/strong&gt;&lt;br&gt;\n&lt;span class=\"small\"&gt;data, preprocessing, validation design, metrics&lt;/span&gt;\n\n\n&lt;strong&gt;Advanced Methods&lt;/strong&gt;&lt;br&gt;\n&lt;span class=\"small\"&gt;models, algorithms, tuning, ablations&lt;/span&gt;\n\n\n&lt;strong&gt;Applied Solution Path&lt;/strong&gt;&lt;br&gt;\n&lt;span class=\"small\"&gt;implementation, assumptions, parameters&lt;/span&gt;\n\n\n&lt;strong&gt;Results&lt;/strong&gt;&lt;br&gt;\n&lt;span class=\"small\"&gt;findings, uncertainty, limitations&lt;/span&gt;\n\n\n&lt;strong&gt;Loopback → Evaluation & Improvement&lt;/strong&gt;&lt;br&gt;\n&lt;span class=\"small\"&gt;error analysis, robustness checks, next iterations&lt;/span&gt;"
  },
  {
    "objectID": "slides/slide0.html#process-as-diagram",
    "href": "slides/slide0.html#process-as-diagram",
    "title": "From Foundations to Results: A Scientific Workflow",
    "section": "Process as Diagram",
    "text": "Process as Diagram\n\n\n\n\n\nflowchart LR\n  A[Foundations / Background] --&gt; B[Research Question]\n  B --&gt; C[State of the Art (Literature)]\n  C --&gt; D[Necessary Methods & Workflow]\n  D --&gt; E[Advanced Methods]\n  E --&gt; F[Applied Solution Path]\n  F --&gt; G[Results]\n  G -.-&gt;|Loopback: evaluation, error analysis, ablations| D\n  G -.-&gt;|Refine question| B"
  }
]